[
["semi-structured-data.html", "Chapter 6 Semi-structured data 6.1 Objectives 6.2 What is semi-structured data? 6.3 Workflow 6.4 HTML/CSS: web scraping 6.5 XML/JSON: social media scraping", " Chapter 6 Semi-structured data 6.1 Objectives Automating the process of turning semi-structured data (input) into structured data (output) 6.2 What is semi-structured data? Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. - Wikipedia Examples: HTML (Hypertext Markup Language) files (e.g., websites) and JSON (JavaScript Object Notation) files (e.g., tweets) Why should we care semi-structured data? Because this is what the data frontier looks like: # of unstructured data &gt; # of semi-structured data &gt; # of structured data There are easy and fast ways to turn semi-structured data into structured data (ideally in a tidy format) using R, Python, and command-line tools. See my own examples (tidyethnicnews and tidytweetjson). 6.3 Workflow Import/connect to a semi-structured file using rvest, jsonlite, xml2, pdftools, tidyjson, etc. Define target elements in the single file and extract them readr package providers parse_ functions that are useful for vector parsing. stringr package for string manipulations (e.g., using regular expressions in a tidy way). Quite useful for parsing PDF files (see this example). rvest package for parsing HTML (R equivalent to beautiful soup in Python) tidyjson package for parsing JSON data Create a list of files (in this case URLs) to parse Write a parsing function Automate parsing process 6.4 HTML/CSS: web scraping 6.5 XML/JSON: social media scraping 6.5.1 API 6.5.1.1 Objectives Learning what kind of social media data are accessible through application programming interfaces (APIs) Review question In the previous session, we learned the difference between semi-structured data and structured data. Can anyone tell us the difference between them? 6.5.1.2 The big picture for digital data collection Input: semi-structured data Output: structured data Process: Getting target data from a remote server The target data is usually huge (&gt;10GB) by the traditional social science standard. Parsing the target data your laptop/database Laptop (sample-parse): Downsamle the large target data and parse it on your laptop. This is just one option to deal with big data in R. It’s a simple strategy as it doesn’t require storing target data in your own database. Database (push-parse): Push the large target data to a database, then explore, select, and filter it. If you were interested in using this option, then check out my SQL for R Users workshop. Sample-Parse. From RStudio. Push-Parse. From RStudio. But what exactly is this target data? When you scrape websites, you mostly deal with HTML (defines a structure of a website), CSS (its style), and JavaScript (its dynamic interactions). When you access social media data through API, you deal with either XML or JSON (major formats for storing and transporting data; they are light and flexible). XML and JSON have tree-like (nested; a root and branches) structures and keys and values (or elements and attributes). If HTML, CSS, and JavaScript are storefronts, then XML and JSON are warehouses. By Andreas Praefcke (Own work), via Wikimedia Commons 6.5.1.3 Opportunities and challenges for parsing social media data This explanation draws on Pablo Barbara’s LSE social media workshop slides. Basic information What is an API?: An interface (you can think of it as something akin to restaurant menu. API parameters are menu items.) REST (Representational state transfer) API: static information (e.g., user profiles, list of followers and friends) R packages: tweetscores, twitteR, rtweet Streaming API: dynamic information (e..g, new tweets) This streaming data is filtered by (1) keywords, (2) location, and (3) sample (1% of the total tweets) R packages: streamR Status Twitter API is still widely accessible (v2 recently released; new fields available such as conversation threads). Twitter data is unique from data shared by most other social platforms because it reflects information that users choose to share publicly. Our API platform provides broad access to public Twitter data that users have chosen to share with the world. - Twitter Help Center What does this policy mean? If Twitter users don’t share the locations of their tweets (e.g., GPS), you can’t collect them. Facebook API access has become much constrained with the exception of Social Science One since the 2016 U.S. election. YouTube API access is somewhat limited (but you need to check as I’m not updated on this). Upside Legal and well-documented. Web scraping (Wild Wild West) &lt;&gt; API (Big Gated Garden) You have legal but limited access to (growing) big data that can be divided into text, image, and video and transformed into cross-sectional (geocodes), longitudinal (timestamps), and event historical data (hashtags). For more information, see Zachary C. Steinert-Threlkeld’s 2020 APSA Short Course Generating Event Data From Social Media. Social media data are also well-organized, managed, and curated data. It’s easy to navigate because XML and JSON have keys and values. If you find keys, you will find observations you look for. Downside Rate-limited. If you want to access to more and various data than those available, you need to pay for premium access. 6.5.1.4 Next steps If you want to know how to sign up a new Twitter developer account and access Twitter API, then see Steinert-Threlkeld’s APSA workshop slides. If you want to know about how to use tweetscore package, then see Pablo Barbara’s R markdown file for scraping data from Twitter’s REST API 6.5.2 Hydrating 6.5.2.1 Objectives Learning how hydrating works Learning how to use Twarc to communicate with Twitter’s API Review question What are the main two types of Twitter’s API? 6.5.2.2 Hydrating: An Alternative Way to Collect Historical Twitter Data You can collect Twitter data using Twitter’s API or you can hydrate Tweet IDs collected by other researchers. This is a good resource to collect historical Twitter data. Covid-19 Twitter chatter dataset for scientic use by Panacealab Women’s March Dataset by Littman and Park Harvard Dataverse has a number of dehydrated Tweet IDs that could be of interest to social scientists. Dehydrated Tweet IDs 6.5.2.3 Twarc: one solution to (almost) all Twitter’s API problems Why Twarc? A command-line tool and Python library that works for almost every Twitter’s API related problem. It’s really well-documented, tested, and maintained. Twarc documentation covers basic commands. Tward-cloud documentation explains how to collect data from Twitter’s API using Twarc running in Amazon Web Services (AWS). Twarc was developed as part of the Documenting the Now project which was funded by the Mellon Foundation. One ring that rules them all. There’s no reason to be afraid of using a command-line tool and Python library, even though you primarily use R. It’s easy to embed Python code and shell scripts in R Markdown. Even though you don’t know how to write Python code or shell scripts, it’s really useful to know how to integrate them in your R workflow. I assume that you have already installed Python 3. pip3 install twarc 6.5.2.3.1 Applications The following examples are created by the University of Virginia library. 6.5.2.3.1.1 Search Download pre-existing tweets (7-day window) matching certain conditions In command-line, &gt; = Create a file I recommend running the following commands in the terminal because it’s more stable than doing so in R Markdown. You can type commands in the Terminal in R Studio. # Key word twarc search blacklivesmatter &gt; blm_tweets.jsonl # Hashtag twarc search &#39;#blacklivesmatter&#39; &gt; blm_tweets_hash.jsonl # Hashtag + Language twarc search &#39;#blacklivesmatter&#39; --lang en &gt; blm_tweets_hash.jsonl It is really important to save these tweets into a jsonl format; jsonl extension refers to JSON Lines files. This structure is useful for splitting JSON data into smaller chunks, if it is too large. 6.5.2.3.1.2 Filter Download tweets meeting certain conditions as they happen. # Key word twarc filter blacklivesmatter &gt; blm_tweets.jsonl 6.5.2.3.1.3 Sample Use Twitter’s random sample of recent tweets. twarc sample &gt; tweets.jsonl 6.5.2.3.1.4 Hydrate Tweet IDs -&gt; Tweets twarc hydrate tweet_ids.txt &gt; tweets.jsonl 6.5.2.3.1.5 Dehydrate Hydrate &lt;&gt; Dehydrate Tweets -&gt; Tweet IDs twarc dehydrate tweets.jsonl &gt; tweet_ids.txt Challenge Collect tweets contain some key words of your choice using twarc search and save them as tweets.jsonl. Using less command in the terminal, inspect twarc.log. Using less command in the terminal, inspect tweets.json. 6.5.3 Parsing JSON # Install packages if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load(tidyverse, # tidyverse pkgs including purrr furrr, # parallel processing tictoc, # performance test tcltk, # GUI for choosing a dir path tidyjson) # tidying JSON files ## Install the current development version from GitHub devtools::install_github(&quot;jaeyk/tidytweetjson&quot;, dependencies = TRUE) ## Skipping install of &#39;tidytweetjson&#39; from a github remote, the SHA1 (3ab642b2) has not changed since last install. ## Use `force = TRUE` to force installation library(tidytweetjson) ## Warning: replacing previous import &#39;maps::map&#39; by &#39;purrr::map&#39; when loading ## &#39;tidytweetjson&#39; 6.5.3.1 Objectives Learning chunk and pull strategy Learning how tidyjson works Learning how to apply tidyjson to tweets 6.5.3.2 Chunk and Pull 6.5.3.2.1 Problem What if the size of the Twitter data you downloaded is too big (e.g., &gt;10GB) to do complex wrangling in R? 6.5.3.2.2 Solution Chunk and Pull. From Studio. Step1: Split the large JSON file in small chunks. #Divide the JSON file by 100 lines (tweets) # Linux and Windows (in Bash) $ split -100 search.jsonl # macOS $ gsplit -100 search.jsonl After that, you will see several files appeared in the directory. Each of these files should have 100 tweets or fewer. All of these file names should start with “x”, as in “xaa”. Step 2: Apply the parsing function to each chunk and pull all of these chunks together. # You need to choose a Tweet JSON file filepath &lt;- file.choose() # Assign the parsed result to the `df` object # 11.28 sec elapsed to parse 17,928 tweets tic() df &lt;- jsonl_to_df(filepath) toc() # Setup n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers # You need to designate a directory path where you saved the list of JSON files. # 9.385 sec elapsed to parse 17,928 tweets dirpath &lt;- tcltk::tk_choose.dir() tic() df_all &lt;- tidytweetjson::jsonl_to_df_all(dirpath) toc() 6.5.3.2.3 tidyjson The tidyjson package helps to use tidyverse framework to JSON data. toy example # JSON collection; nested structure + keys and values worldbank[1] ## [1] &quot;{\\&quot;_id\\&quot;:{\\&quot;$oid\\&quot;:\\&quot;52b213b38594d8a2be17c780\\&quot;},\\&quot;boardapprovaldate\\&quot;:\\&quot;2013-11-12T00:00:00Z\\&quot;,\\&quot;closingdate\\&quot;:\\&quot;2018-07-07T00:00:00Z\\&quot;,\\&quot;countryshortname\\&quot;:\\&quot;Ethiopia\\&quot;,\\&quot;majorsector_percent\\&quot;:[{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:46},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:26},{\\&quot;Name\\&quot;:\\&quot;Public Administration, Law, and Justice\\&quot;,\\&quot;Percent\\&quot;:16},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:12}],\\&quot;project_name\\&quot;:\\&quot;Ethiopia General Education Quality Improvement Project II\\&quot;,\\&quot;regionname\\&quot;:\\&quot;Africa\\&quot;,\\&quot;totalamt\\&quot;:130000000}&quot; # Check out keys (objects) worldbank %&gt;% as.tbl_json() %&gt;% gather_object() %&gt;% filter(document.id == 1) ## # A tbl_json: 8 x 3 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id name ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 &quot;{\\&quot;$oid\\&quot;:\\&quot;52b213...&quot; 1 _id ## 2 &quot;\\&quot;2013-11-12T00:...&quot; 1 boardapprovaldate ## 3 &quot;\\&quot;2018-07-07T00:...&quot; 1 closingdate ## 4 &quot;\\&quot;Ethiopia\\&quot;&quot; 1 countryshortname ## 5 &quot;[{\\&quot;Name\\&quot;:\\&quot;Educa...&quot; 1 majorsector_percent ## 6 &quot;\\&quot;Ethiopia Gener...&quot; 1 project_name ## 7 &quot;\\&quot;Africa\\&quot;&quot; 1 regionname ## 8 &quot;130000000&quot; 1 totalamt # Get the values associated with the keys worldbank %&gt;% as.tbl_json() %&gt;% # Turn JSON into tbl_json object enter_object(&quot;project_name&quot;) %&gt;% # Enter the objects append_values_string() %&gt;% # Append the values as_tibble() # To reduce the size of the file ## # A tibble: 500 x 2 ## document.id string ## &lt;int&gt; &lt;chr&gt; ## 1 1 Ethiopia General Education Quality Improvement Project II ## 2 2 TN: DTF Social Protection Reforms Support ## 3 3 Tuvalu Aviation Investment Project - Additional Financing ## 4 4 Gov&#39;t and Civil Society Organization Partnership ## 5 5 Second Private Sector Competitiveness and Economic Diversificati… ## 6 6 Additional Financing for Cash Transfers for Orphans and Vulnerab… ## 7 7 National Highways Interconnectivity Improvement Project ## 8 8 China Renewable Energy Scale-Up Program Phase II ## 9 9 Rajasthan Road Sector Modernization Project ## 10 10 MA Accountability and Transparency DPL ## # … with 490 more rows The following example draws on my tidytweetjson R package. The package applies tidyjson to Tweets. 6.5.3.2.3.1 Individual file jsonl_to_df &lt;- function(file_path){ # Save file name file_name &lt;- strsplit(x = file_path, split = &quot;[/]&quot;) file_name &lt;- file_name[[1]][length(file_name[[1]])] # Import a Tweet JSON file listed &lt;- read_json(file_path, format = c(&quot;jsonl&quot;)) # IDs of the tweets with country codes ccodes &lt;- listed %&gt;% enter_object(&quot;place&quot;) %&gt;% enter_object(&quot;country_code&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(&quot;country_code&quot; = &quot;string&quot;) # IDs of the tweets with location locations &lt;- listed %&gt;% enter_object(&quot;user&quot;) %&gt;% enter_object(&quot;location&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(location = &quot;string&quot;) # Extract other key elements from the JSON file df &lt;- listed %&gt;% spread_values( id = jnumber(&quot;id&quot;), created_at = jstring(&quot;created_at&quot;), full_text = jstring(&quot;full_text&quot;), retweet_count = jnumber(&quot;retweet_count&quot;), favorite_count = jnumber(&quot;favorite_count&quot;), user.followers_count = jnumber(&quot;user.followers_count&quot;), user.friends_count = jnumber(&quot;user.friends_count&quot;) ) %&gt;% as_tibble message(paste(&quot;Parsing&quot;, file_name, &quot;done.&quot;)) # Full join outcome &lt;- full_join(ccodes, df) %&gt;% full_join(locations) # Or you can write this way: outcome &lt;- reduce(list(df, ccodes, locations), full_join) # Select outcome %&gt;% select(-c(&quot;document.id&quot;))} 6.5.3.2.3.2 Many files Set up parallel processing. n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. ## system ## 7 plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers ## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled ## in future (&gt;= 1.13.0) when running R from RStudio, because it is ## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall ## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to ## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more details, ## how to control forked processing or not, and how to silence this warning in ## future R sessions, see ?future::supportsMulticore Parsing in parallel. Review There are at least three ways you can use function + purrr::map(). squared &lt;- function(x){ x*2 } # Named function map(1:3, squared) # Anonymous function map(1:3, function(x){ x *2 }) # Using formula; ~ = formula, .x = input map(1:3,~.x*2) # Create a list of file paths filename &lt;- list.files(dir_path, pattern = &#39;^x&#39;, full.names = TRUE) df &lt;- filename %&gt;% # Apply jsonl_to_df function to items on the list future_map(~jsonl_to_df(.)) %&gt;% # Full join the list of dataframes reduce(full_join, by = c(&quot;id&quot;, &quot;location&quot;, &quot;country_code&quot;, &quot;created_at&quot;, &quot;full_text&quot;, &quot;retweet_count&quot;, &quot;favorite_count&quot;, &quot;user.followers_count&quot;, &quot;user.friends_count&quot;)) # Output df "]
]
