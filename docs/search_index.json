[
["index.html", "Computational Thinking for Social Scientists Chapter 1 PS239T 1.1 Textbook 1.2 Objectives 1.3 Logistics 1.4 Course schedule 1.5 Questions, comments, or suggestions 1.6 Special thanks 1.7 License", " Computational Thinking for Social Scientists Jae Yeon Kim 2020-09-29 Chapter 1 PS239T Welcome to PS239T This course will help social science graduate students to think computationally and develop proficiency with computational tools and techniques, necessary to conduct research in computational social science. Mastering these tools and techniques not only enables students to collect, wrangle, analyze, and interpret data with less pain and more fun, but it also let students to work on research projects that would previously seem impossible. 1.1 Textbook The online book version of the course materials is currently work in progress. 1.2 Objectives The course is currently divided into two main subjects (fundamentals and applications) and six main sessions. 1.2.1 Part I Fundamentals In the first section, students learn best practices in data and code management using Git and Bash. In the second, students learn how to wrangle, model, and visualize data easier and faster. In the third, students learn how to use functions to automate repeated things and develop their own data tools (e.g., packages). 1.2.2 Part II Applications In the fourth, students learn how to collect and parse semi-structured data at scale (e.g., using APIs and webscraping). In the fifth, students learn how to analyze high-dimensional data (e.g., text) using machine learning. In the final, students learn how to access, query, and manage big data using SQL. We will learn how to do all of the above mostly in R, and sometimes in bash and Python. 1.3 Logistics 1.3.1 Instructor Jae Yeon Kim: jaeyeonkim@berkeley.edu 1.3.2 Time and location Lecture: TBD (Zoom) Section: TBD (Zoom) 1.3.3 Office hours By appointment with … 1.3.4 Slack &amp; GitHub Slack for communication (announcements and questions). You should ask questions about class material and assignments through the Slack channels so that everyone can benefit from the discussion. We encourage you to respond to each other’s questions as well. GitHub for everything else, including turning in assignments (except final project proposals, which will be submitted to Slack). Students are required to use GitHub for their final projects, which will be publicly available, unless they have special considerations (e.g. proprietary data). All course materials will be posted on GitHub at https://github.com/jaeyk/PS239T, including class notes, code demonstrations, sample data, and assignments. 1.3.5 Accessibility This class is committed to creating an environment in which everyone can participate, regardless of background, discipline, or disability. If you have a particular concern, please come to me as soon as possible so that we can make special arrangements. 1.3.6 Code for conduct TBD 1.3.7 Course requirements and grades This is a graded class based on the following: Completion of assigned homework (50%) Participation (25%) Final project (25%) 1.3.7.1 Assignments Assignments will be assigned at the end of every session. They will be due at the start of the following class unless otherwise noted. The assignments will be frequent but each of them should be fairly short. You are encouraged to work in groups, but the work you turn in must be your own. Group submission of homework, or turning in copies of the same code or output, is not acceptable. Remember, the only way you actually learn how to write code is to write code. Unless otherwise specified, assignments should be turned in as pdf documents via the bCourses site. 1.3.7.2 Class participation The class participation portion of the grade can be satisfied in one or more of the following ways: attending the lecture and section (note that section is non-optional) asking and answering questions in class contributing to class discussion through the bCourse site, and/or collaborating with the campus computing community, either by attending a D-Lab or BIDS workshop, submitting a pull request to a campus github repository (including the class repository), answering a question on StackExchange, or other involvement in the social computing / digital humanities community. Because we will be using laptops every class, the temptation to attend to other things during slow moments will be high. While you may choose to do so, I do request that you think of your laptop screen as in the public domain for the duration of classtime. Please do not load anything that will distract your classmates or is otherwise inappropriate to a classroom setting. 1.3.7.3 Final project The final project consists of using the tools we learned in class on your own data of interest. First- and second-year students in the political science department are encouraged to use this as an opportunity to gather data to be used for other courses or the second-year thesis. Students are required to write a short proposal by March (no more than 2 paragraphs) in order to get approval and feedback from the instructor. During sections in April we will have lightning talk sessions where students present their projects in a maximum 5 minute talk, with 5 minutes for class Q&amp;A. Since there is no expectation of a formal paper, you should select a project that is completable by the end of the term. In other words, submitting a research design for your future dissertation that will use skills from the class but collects no data is not acceptable, but completing a viably small portion of a study or thesis is. Final project rubric Final project template Final project examples 1.3.8 Class activities and materials 1.3.8.1 Lecture Classes will follow a “workshop” style, combining lecture and lab formats. The class is interactive, with students programming every session. During the “skills” parts of the class, we will be learning how to program in Unix, HTML, and R by following course notes and tutorials. During the “applications” sections, we will follow a similar structure, with occasional guest speakers. 1.3.8.2 Section The “lab” section will generally be a less formal session dedicated to helping students with materials from lecture and homework. It will be mostly student led, so come with questions. If there are no questions, the lab turns into a “hackathon” where groups can work on the assignments together. Section is required unless prior permission to miss it is obtained from both the instructor and one’s groupmates. Attending office hours is not a substitute for attending section. 1.3.8.3 Computer requirements The software needed for the course is as follows: Access to the UNIX command line (e.g., a Mac laptop, a Bash wrapper on Windows) Git R and RStudio (latest versions) Anaconda and Python 3 (latest versions) Pandoc and LaTeX This requires a computer that can handle all this software. Almost any Mac will do the job. Most Windows machines are fine too if they have enough space and memory. You must have all the software downloaded and installed PRIOR to the first day of class. If there are issues with installation on your machine, please contact the section assistant, Julia Christensen, for assistance. See B_Install.md for more information. 1.4 Course schedule 1.4.1 Part I Fundamentals Week 1 Computational thinking and setup Week 2 Managing data and code Week 3 Tidy data and why it matters Week 4 Wrangling data Week 5 Wrangling data at scale Week 6 Modeling and visualizing tidy data Week 7 From for loop to functional programming Week 8 Developing your own data tools 1.4.2 Part II Applications Week 9 HTML/CSS: web scraping Week 10 XML/JSON: social media scraping Week 11 Supervised machine learning Week 12 Unsupervised machine learning Week 13 Database and SQL Week 14 Wrap-up Week 15 Final presentation 1.5 Questions, comments, or suggestions Please create issues, if you have questions, comments, or suggestions. 1.6 Special thanks This course is a remix version of the course originally developed by Rochelle Terman then revised by Rachel Bernhard. Other teaching materials draw from the workshops I created for D-Lab and Data Science Discovery Program at UC Berkeley. 1.7 License This work is licensed under a Creative Commons Attribution 4.0 International License. "],
["motivation.html", "Chapter 2 Computational thinking 2.1 Why computational thinking 2.2 Computational way of thinking about data 2.3 Computational way of thinking about research process", " Chapter 2 Computational thinking 2.1 Why computational thinking If social scientists want to know how to work smart and not just hard, they need to take full advantage of the power of modern programming languages, and that power is automation. Let’s think about the following two cases. Case 1: Suppose a social scientist needs to collect data on civic organizations in the United States from websites, Internal Revenue Service reports, and social media posts. As the number of these organizations is large, the researcher could not collect a large volume of data from diverse sources, so they would hire undergraduates and distribute tasks among them. This is a typical data collection plan in social science research, and it is labor-intensive. Automation is not part of the game plan. Yet, it is critical for so many reasons. Because the process is costly, no one is likely to either replicate or update the data collection effort. Put differently, without making the process efficient, it is difficult for it to be reproducible and scalable. Case 2: An alternative is to write computer programs that collect such data automatically, parse them, and store them in interconnected databases. Additionally, someone may need to maintain and validate the quality of the data infrastructure. Nevertheless, this approach lowers the cost of the data collection process, thereby substantially increasing the reproducibility and scalability of the process. Furthermore, the researcher can document their code and publicly share it using their GitHub repository or even gather some of the functions they used and distribute them as open-source libraries. Programming is as valuable a skill as writing in social science research. The extent to which a researcher can automate the research process can determine its efficiency, reproducibility, and scalability. Every modern statistical and data analysis problem needs code to solve it. You shouldn’t learn just the basics of programming, spend some time gaining mastery. Improving your programming skills pays off because code is a force multiplier: once you’ve solved a problem once, code allows you to solve it much faster in the future. As your programming skill increases, the generality of your solutions improves: you solve not just the precise problem you encountered, but a wider class of related problems (in this way programming skill is very much like mathematical skill). Finally, sharing your code with others allows them to benefit from your experience. - Hadley Wickham How can we automate our research process? How can we talk to and teach a machine so that it could become the most powerful and reliable research assistant ever? From BBC Bitesize 2.2 Computational way of thinking about data 2.2.1 Structure Structured data (Excel spreadsheets, CSVs) Tidy data Semi-structured data HTML/CSS: Websites JSON/XML: APIs 2.2.2 Dimension Low-dimensional data (n &gt; p) Survey, experimental, and administrative data High-dimensional data (n &lt; p) Text, speech, image, video, etc. 2.2.3 Size Data fit in your laptop’s memory Data don’t fit in your laptop’s memory (=big data) 2.3 Computational way of thinking about research process Computational tools and techniques make … Doing traditional research easier, faster, scalable, and more reproducible Data wrangling Modeling Visualization Documentation and collaboration easier, faster, scalable, safer, and more experimental Dynamic reporting (markdown) Version control system (Git and GitHub) Collecting and analyzing large and complex data possible Digital data collection (API and web scraping) Building a data infrastructure (SQL) Machine learning "],
["intro.html", "Chapter 3 Managing data and code 3.1 Project-oriented research 3.2 Writing code: How to code like a professional 3.3 Asking questions: Minimal reproducible example 3.4 References", " Chapter 3 Managing data and code 3.1 Project-oriented research 3.1.1 Computational reproducibility 3.1.1.1 Setup pacman::p_load( tidyverse, # tidyverse here # computational reproducibility ) 3.1.1.2 Motivation Why do you need to make your research project computationally reproducible? For your self-interest and public benefits. 3.1.1.3 How to organize files in a project You won’t be able to reproduce your project unless it is efficiently organized. Step 1. Environment is part of your project. If someone can’t reproduce your environment, they won’t be able to run your code. Launch R Studio. Choose Tools &gt; Global Options. You should not check Restor .RData into workspace at startup and set saving workspace option to NEVER. Step 2. For each project, create a project directory named after the project. # Don&#39;t name it a project. Use a name that&#39;s more informative. For instance, us_election not my_project. dir.create(&quot;../us_election&quot;) Step 3. Launch R Studio. Choose File &gt; New project &gt; Browse existing directories &gt; Create project This allows each project has its own workspace. Step 4. Organize files by putting them in separate subdirectories and naming them in a sensible way. Treat raw data as read only (raw data should be RAW!) and put in the data subdirectory. Note that version control does not need replace backup. You still need to backup your raw data. dir.create(here::here(&quot;us_election&quot;, &quot;data&quot;)) Separate read-only data from processed data and put in the processed_data subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;processed_data&quot;)) Put your code in the src subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Put generated outputs (e.g., tables, figures) in the outputs subdirectory and treat them as disposable. dir.create(here::here(&quot;us_election&quot;, &quot;outputs&quot;)) Put your custom functions in the functions subdirectory. You can gather some of these functions and distribute them as an open-source library. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Challenge 2 Set a project structure for a project named “starwars”. 3.1.1.4 How to organize code in a R markdown file In addition to environment, workflow is an important component of project efficiency and reproducibility. What is R markdown? An R package, developed by Yihui Xie, that provides an authoring framework for data science. Xie is also a developer of many widely popular R packages such as knitr, xaringan (cool kids use xaringan not Beamer these days), blogdown (used to create my personal website), and bookdown (used to create this book) among many others. Many applications: reports, presentations, dashboards, websites Check out Communicating with R markdown workshop by Alison Hill (RStudio) Alison Hill is a co-author of blogdown: Creating Websites with R Markdown. Key strengths: dynamic reporting + reproducible science + easy deployment R Markdown The bigger picture - Garrett Grolemund R-Ladies Oslo (English) - Reports to impress your boss! Rmarkdown magic - Athanasia Mowinckel R Markdown basic syntax # Header 1 ## Header 2 ### Header 3 Use these section headers to indicate workflow. # Import packages and data # Tidy data # Wrangle data # Model data # Visualize data Press ctrl + shift + o. You can see a document outline based on these headers. This is a nice feature for finding code you need to focus. If your project’s scale is large, then divide these sections into files, number, and save them in code subdirectory. 01_wrangling.Rmd 02_modeling.Rmd … 3.1.1.5 Making a project computationally reproducible setwd(): set a working directory. Note that using setwd() is not a reproducible way to set up your project. For instance, none will be able to run the following code except me. # Set a working directory setwd(&quot;/home/jae/starwars&quot;) # Do something ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() # Export the object. # dot means the working directory set by setwd() ggsave(&quot;./outputs/example.png&quot;) # This is called relative path Instead, learn how to use here()’. Key idea: separate workflow (e.g., workspace information) from products (code and data). For more information, read Jenny Bryan’s wonderful piece on project-oriented workflow. Example # New: Reproducible ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() ggsave(here(&quot;project&quot;, &quot;outputs&quot;, &quot;example.png&quot;)) How here works here() function shows what’s the top-level project directory. here::here() Build a path including subdirectories here::here(&quot;project&quot;, &quot;outputs&quot;) #depth 1 #depth 2 How here defines the top-level project directory. The following list came from the here package vignette). Is a file named .here present? Is this an RStudio Project? (Note that we already set up an RStudio Project! So, if you use RStudio’s project feature, then you are ready to use here.) Is this an R package? Does it have a DESCRIPTION file? Is this a remake project? Does it have a file named remake.yml? Is this a projectile project? Does it have a file named .projectile? Is this a checkout from a version control system? Does it have a directory named .git or .svn? Currently, only Git and Subversion are supported. If there’s no match then use set_here() to create an empty .here file. Challenge 1 Can you define computational reproducibility? Can you explain why sharing code and data is not enough for computational reproducibility? 3.1.2 Version control (Git and Bash) 3.1.2.1 What Is Bash? 3.1.2.1.1 Writing your first shell script Write a shell script that creates a directory called /pdfs under /Download directory, then find PDF files in /Download and copy those files to pdfs. This shell script creates a backup. #!/bin/sh mkdir /home/jae/Downloads/pdfs cd Download cp *.pdf pdfs/ echo &quot;Copied pdfs&quot; 3.1.2.2 What Are Git and GitHub? Figure 2.1. A schematic git workflow from Healy’s “The Plain Person’s Guide to Plain Text Social Science” 3.1.2.2.1 Basics: git push and git pull 3.1.2.2.2 Time machine: git revert 3.1.2.2.3 Parallel universe: git branch 3.1.2.2.4 User-manual: readme README.md In this simple markdown file, note some basic information about the project including the project structure. This is how I used the README.md file for this course. Check out my GitHub account to see how I manage my projects. 3.1.2.3 Deployment: GitHub Pages 3.1.2.4 Tracking progress: GitHub Issues 3.1.2.5 Project management: GitHub Dashboards 3.2 Writing code: How to code like a professional 3.2.1 Write readable code What is code style? Every major open-source project has its own style guide: a set of conventions (sometimes arbitrary) about how to write code for that project. It is much easier to understand a large codebase when all the code in it is in a consistent style. - Google Style Guides 10 Tips For Clean Code - Michael Toppa How to avoid smelly code? Check out the code-smells Git repository by Jenny Bryan. Code smells and feels - Jenny Bryan \"Code smell\" is an evocative term for that vague feeling of unease we get when reading certain bits of code. It's not necessarily wrong, but neither is it obviously correct. We may be reluctant to work on such code, because past experience suggests it's going to be fiddly and bug-prone. In contrast, there's another type of code that just feels good to read and work on. What's the difference? If we can be more precise about code smells and feels, we can be intentional about writing code that is easier and more pleasant to work on. I've been fortunate to spend the last couple years embedded in a group of developers working on the tidyverse and r-lib packages. Based on this experience, I'll talk about specific code smells and deodorizing strategies for R. - Jenny Bryan Naming matters When naming files: Don’t use special characters. (Spaces make filenames awkward in the console/command-line.) Don’t capitalize. (UNIX is case sensitive.) Numbering them if files should be run in an order. # Good fit_models.R # Bad fit models.R When naming objects: Don’t use special characters. Don’t capitalize. # Good day_one # Bad DayOne When naming functions: Don’t use special characters. Don’t capitalize. Use verbs instead of nouns. (Functions do something!) # Good run_rdd # Bad rdd Spacing # Good x[, 1] mean(x, na.rm = TRUE) # Bad x[,1] mean (x, na.rm = TRUE) Indenting # Good if (y &lt; 0) { message(&quot;y is negative&quot;) } # Bad if (y &lt; 0) { message(&quot;Y is negative&quot;)} Long lines # Good do_something_very_complicated( something = &quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) # Bad do_something_very_complicated(&quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) Comments Use comments to explain your decisions. But, show your code; Do not try to explain your code by comments. Also, try to comment out rather than delete the code that you experiment with. # Average sleep hours of Jae jae %&gt;% # By week group_by(week) %&gt;% # Mean sleep hours summarise(week_sleep = mean(sleep, na.rm = TRUE)) Pipes (chaining commands) # Good iris %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, mean) %&gt;% ungroup() %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) # Bad iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% ungroup %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) Additional tips Use lintr to check whether your code complies with a recommended style guideline (e.g., tidyverse) and styler package to format your code according to the style guideline. how lintr works 3.2.2 Write reusable code Pasting Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia It’s okay for pasting for the first attempt to solve a problem. But if you copy and paste three times (a.k.a. Rule of Three in programming), something’s wrong. You’re working too hard. You need to be lazy. What do I mean and how can you do that? Example Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble(&quot;a&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE)) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Using a function function: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. # Create a custom function fix_missing &lt;- function(x) { # INPUT x[x == -99] &lt;- NA # COMPUTATION x # OUTPUT } # Apply the function to each column (vector) # This iterated part can and should be automated. df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df Automation Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) # What is this magic? We will unpack the blackbox (`map_df()`) later. df Takeaways Your code becomes more reusable, when it’s easier to change, debug, and scale up. Don’t repeat yourself and embrace the power of lazy programming. Lazy, because only lazy programmers will want to write the kind of tools that might replace them in the end. Lazy, because only a lazy programmer will avoid writing monotonous, repetitive code—thus avoiding redundancy, the enemy of software maintenance and flexible refactoring. Mostly, the tools and processes that come out of this endeavor fired by laziness will speed up the production. - Philipp Lenssen Only when your code becomes reusable, you would become efficient in your data work. Otherwise, you need to start from scratch or copy and paste, when you work on a new project. Code reuse aims to save time and resources and reduce redundancy by taking advantage of assets that have already been created in some form within the software product development process.[2] The key idea in reuse is that parts of a computer program written at one time can be or should be used in the construction of other programs written at a later time. - Wikipedia 3.2.3 Test your code systematically 3.3 Asking questions: Minimal reproducible example 3.3.1 How to create a minimal reproducible example 3.4 References Project-oriented research Computational reproducibility “Good Enough Practices in Scientific Computing” by PLOS Project Management with RStudio by Software Carpentry Initial steps toward reproducible research by Karl Broman Version control Version Control with Git by Software Carpentry The Plain Person’s Guide to Plain Text Social Science by Kieran Healy Writing code Style guides R Google’s R style guide R code style guide by Hadley Wickham The tidyverse style guide by Hadley Wickham Python Google Python Style Guide Code Style by the Hitchhiker’s Guide to Python Asking questions "],
["tidy-data.html", "Chapter 4 Tidy data and its friends 4.1 Tidy data and why it matters 4.2 Wrangling data 4.3 Wrangling data at scale 4.4 Modeling and visualizing tidy data", " Chapter 4 Tidy data and its friends 4.1 Tidy data and why it matters 4.2 Wrangling data 4.3 Wrangling data at scale 4.4 Modeling and visualizing tidy data "],
["functional-programming.html", "Chapter 5 Automating repeated things 5.1 Why Functional Programming 5.2 Automote 2 or 2+ Tasks 5.3 Automate plotting 5.4 Automate joining 5.5 Make automation slower or faster 5.6 Make error handling easier 5.7 Developing your own data tools", " Chapter 5 Automating repeated things 5.1 Why Functional Programming Setup # Install packages if (!require(&quot;pacman&quot;)) { install.packages(&quot;pacman&quot;) } ## Loading required package: pacman pacman::p_load( tidyverse, # tidyverse pkgs including purrr tictoc, # performance test broom, # tidy modeling glue, # paste string and objects furrr, # parallel processing rvest ) # web scraping 5.1.1 Why map() 5.1.1.1 Objectives How to use purrr to automate workflow in a cleaner, faster, and more extendable way 5.1.1.2 Copy-and-paste programming Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia Example Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble( &quot;a&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE) ) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge. Explain why this solution is not very efficient (Hint: If df$a[df$a == -99] &lt;- NA has an error, how are you going to fix it? A solution is not scalable if it’s not automatable. 5.1.1.3 Using a function Let’s recall what’s function in R: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. ` function(input){ computation return(output) } ` # Function fix_missing &lt;- function(x) { x[x == -99] &lt;- NA x } # Apply function to each column (vector) df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge Why using function is more efficient than 100% copying and pasting? Can you think about a way we can automate the process? Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 map() is a higher-order function that applies a given function to each element of a list/vector. This is how map() works. It’s easier to understand with a picture. - Input: Takes a vector/list. - Computation: Calls the function once for each element of the vector - Output: Returns in a list or whatever data format you prefer (e.g., `_df helper: dataframe`) Challenge If you run the code below, what’s going to be the data type of the output? map(df, fix_missing) ## $a ## [1] 3 3 1 1 NA ## ## $b ## [1] 3 2 NA NA 1 ## ## $c ## [1] 3 3 1 2 1 ## ## $d ## [1] 1 1 2 1 3 Why map() is a good alternative to for loop. The Joy of Functional Programming (for Data Science) - Hadley Wickham # Built-in data data(&quot;airquality&quot;) tic() # Placeholder out1 &lt;- vector(&quot;double&quot;, ncol(airquality)) # Sequence variable for (i in seq_along(airquality)) { # # Assign a computation result to each element out1[[i]] &lt;- mean(airquality[[i]], na.rm = TRUE) } toc() ## 0.007 sec elapsed tic() out1 &lt;- airquality %&gt;% map_dbl(mean, na.rm = TRUE) toc() ## 0.002 sec elapsed In short, map() is more readable, faster, and easily extendable with other data science tasks (e.g., wrangling, modeling, and visualization) using %&gt;%. Final point: Why not base R apply family? Short answer: purrr::map() is simpler to write. For instance, map_dbl(x, mean, na.rm = TRUE) = vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1)) 5.1.1.4 Application (many models) One popular application of map() is to run regression models (or whatever model you want to run) on list-columns. No more copying and pasting for running many regression models on subgroups! # Have you ever tried this? lm_A &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_A&quot;)) lm_B &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_B&quot;)) lm_C &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_C&quot;)) lm_D &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_D&quot;)) lm_E &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_E&quot;)) For more information on this technique, read the Many Models subchapter of the R for Data Science. # Function lm_model &lt;- function(df) { lm(Temp ~ Ozone, data = df) } # Map models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, lm_model)) # Map models$ols[1] ## [[1]] ## ## Call: ## lm(formula = Temp ~ Ozone, data = df) ## ## Coefficients: ## (Intercept) Ozone ## 62.8842 0.1629 # Add tidying tidy_lm_model &lt;- purrr::compose( # compose multiple functions broom::tidy, # convert lm objects into tidy tibbles lm_model ) tidied_models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, tidy_lm_model)) tidied_models$ols[1] ## [[1]] ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 62.9 1.61 39.2 2.88e-23 ## 2 Ozone 0.163 0.0500 3.26 3.31e- 3 5.2 Automote 2 or 2+ Tasks 5.2.1 Objectives Learning how to use map2() and pmap() to avoid writing nested loops. 5.2.2 Problem Problem: How can you create something like below? [1] “University = Berkeley | Department = waterbenders” [1] “University = Berkeley | Department = earthbenders” [1] “University = Berkeley | Department = firebenders” [1] “University = Berkeley | Department = airbenders” [1] “University = Stanford | Department = waterbenders” [1] “University = Stanford | Department = earthbenders” [1] “University = Stanford | Department = firebenders” [1] “University = Stanford | Department = airbenders” The most manual way: You can copy and paste eight times. paste(&quot;University = Berkeley | Department = CS&quot;) ## [1] &quot;University = Berkeley | Department = CS&quot; 5.2.3 For loop A slightly more efficient way: using a for loop. Think about which part of the statement is constant and which part varies ( = parameters). Do we need a placeholder? No. We don’t need a placeholder because we don’t store the result of iterations. Challenge: How many parameters do you need to solve the problem below? # Outer loop for (univ in c(&quot;Berkeley&quot;, &quot;Stanford&quot;)) { # Inner loop for (dept in c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;)) { print(paste(&quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept)) } } ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Berkeley | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Berkeley | Department = airbenders&quot; ## [1] &quot;University = Stanford | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Stanford | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; This is not bad, but … n arguments -&gt; n nested for loops. As a scale of your problem grows, your code gets really complicated. To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs. — Bjarne Stroustrup 5.2.4 map2 &amp; pmap Step 1: Define inputs and a function. Challenge Why are we using rep() to create input vectors? For instance, for univ_list why not just use c(\"Berkeley\", \"Stanford\")? # Inputs (remember the length of these inputs should be identical) univ_list &lt;- rep(c(&quot;Berkeley&quot;, &quot;Stanford&quot;), 4) dept_list &lt;- rep(c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;), 2) # Function print_lists &lt;- function(univ, dept) { print(paste( &quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept )) } # Test print_lists(univ_list[1], dept_list[1]) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; Step2: Using map2() or pmap() # 2 arguments map2_output &lt;- map2(univ_list, dept_list, print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; # 3+ arguments pmap_output &lt;- pmap(list(univ_list, dept_list), print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; Challenge Have you noticed that we used a slightly different input for pmap() compared to map() or map2()? What is the difference? 5.3 Automate plotting 5.3.1 Objective Learning how to use map() and glue() to automate creating multiple plots 5.3.2 Problem Making the following data visualization process more efficient. data(&quot;airquality&quot;) airquality %&gt;% ggplot(aes(x = Ozone, y = Solar.R)) + geom_point() + labs( title = &quot;Relationship between Ozone and Solar.R&quot;, y = &quot;Solar.R&quot; ) ## Warning: Removed 42 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Wind)) + geom_point() + labs( title = &quot;Relationship between Ozone and Wind&quot;, y = &quot;Wind&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Temp)) + geom_point() + labs( title = &quot;Relationship between Ozone and Temp&quot;, y = &quot;Temp&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). 5.3.3 Solution Learn how glue() works. glue() combines strings and objects and it works simpler and faster than paste() or sprintif(). names &lt;- c(&quot;Jae&quot;, &quot;Aniket&quot;, &quot;Avery&quot;) fields &lt;- c(&quot;Political Science&quot;, &quot;Law&quot;, &quot;Public Health&quot;) glue(&quot;{names} studies {fields}.&quot;) ## Jae studies Political Science. ## Aniket studies Law. ## Avery studies Public Health. So, our next step is to combine glue() and map(). Let’s first think about writing a function that includes glue(). Challenge How can you create the character vector of column names? Challenge How can make ggplot2() take strings as x and y variable names? (Hint: Type ?aes_string()) airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[2])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[2]}&quot;), y = glue(&quot;{names(airquality)[2]}&quot;) ) ## Warning: Removed 42 rows containing missing values (geom_point). The next step is to write an automatic plotting function. Note that in the function i (abstract argument) replaced 2 (specific number). create_point_plot &lt;- function(i) { airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[i])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[i]}&quot;), y = glue(&quot;{names(airquality)[i]}&quot;) ) } The final step is to put the function in map(). map(2:ncol(airquality), create_point_plot) ## [[1]] ## Warning: Removed 42 rows containing missing values (geom_point). ## ## [[2]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[3]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[4]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[5]] ## Warning: Removed 37 rows containing missing values (geom_point). 5.4 Automate joining 5.4.1 Objective Learning how to use reduce() to automate joining multiple dataframes 5.4.2 Problem How can you make joining multiple dataframes more efficient? Note that we will use dplyr::left_join() = merge(x, y, all.x = TRUE). df1 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df2 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df3 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) 5.4.3 Copy and paste first_join &lt;- left_join(df1, df2) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) second_join &lt;- left_join(first_join, df3) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) second_join ## # A tibble: 3 x 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 5 8 ## 2 4 8 3 ## 3 4 4 4 Challenge Why the above solution is not efficient? 5.4.4 reduce How reduce() works. - Input: Takes a vector of length n - Computation: Calls a function with a pair of values at a time - Output: Returns a vector of length 1 reduced &lt;- reduce(list(df1, df2, df3), left_join) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) reduced ## # A tibble: 3 x 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 5 8 ## 2 4 8 3 ## 3 4 4 4 5.5 Make automation slower or faster 5.5.1 Objectives Learning how to use slowly() and future_ to make automation process either slower or faster 5.5.2 How to Make Automation Slower Scraping 50 pages from a website and you don’t want to overload the server. How can you do that? 5.5.3 For loop 5.5.4 Map walk() works same as map() but doesn’t store its output. If you’re web scraping, one problem with this approach is it’s too fast by human standards. If you want to make the function run slowly … slowly() takes a function and modifies it to wait a given amount of time between each call. - purrr package vignette - If a function is a verb, then a helper function is an adverb (modifying the behavior of the verb). 5.5.5 How to Make Automation Faster In a different situation, you want to make your function run faster. This is a common situation when you collect and analyze data at large-scale. You can solve this problem using parallel processing. For more on the parallel processing in R, read this review. Parallel processing setup Step1: Determine the number of max workers (availableCores()) Step2: Determine the parallel processing mode (plan()) # Setup n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. ## system ## 7 plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores ) # the maximum number of workers ## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled ## in future (&gt;= 1.13.0) when running R from RStudio, because it is ## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall ## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to ## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more details, ## how to control forked processing or not, and how to silence this warning in ## future R sessions, see ?future::supportsMulticore tic() mean100 &lt;- map(1:1000000, mean) toc() ## 3.459 sec elapsed tic() mean100 &lt;- future_map(1:1000000, mean) toc() ## 2.602 sec elapsed 5.6 Make error handling easier 5.6.1 Learning objective Learning how to use safely() and possibly() to make error handling easier ### Problem Challenge Explain why we can’t run map(url_lists, read_html) url_lists &lt;- c( &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot;, &quot;https://en.wikipedia.org/wiki/Stanford_University&quot;, &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot;, &quot;https://DLAB&quot; ) map(url_lists, read_html) This is a very simple problem so it’s easy to tell where the problem is. How can you make your error more informative? 5.6.2 Solution 5.6.2.1 Try-catch There are three kinds of messages you will run into, if your code has an error based on the following functions. stop(): errors; Functions must stop. warning(): warnings; Functions may still work. Nonetheless, something is possibly messed up. message(): messages; Some actions happened. The basic logic of try-catch, R’s basic error handling function, works like the following. tryCatch( { map(url_lists, read_html) }, warning = function(w) { &quot;Warning&quot; }, error = function(e) { &quot;Error&quot; }, finally = { &quot;Message&quot; } ) ## [1] &quot;Error&quot; Here’s purrr version of the try-catch mechanism (evaluates code and assigns exception handlers). 5.6.2.2 safely and possibly Outputs result: result or NULL error: NULL or error map(url_lists, safely(read_html)) ## [[1]] ## [[1]]$result ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[1]]$error ## NULL ## ## ## [[2]] ## [[2]]$result ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[2]]$error ## NULL ## ## ## [[3]] ## [[3]]$result ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[3]]$error ## NULL ## ## ## [[4]] ## [[4]]$result ## NULL ## ## [[4]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Could not resolve host: DLAB&gt; The easier way to solve this problem is just avoiding the error. map(url_lists, safely(read_html)) %&gt;% map(&quot;result&quot;) %&gt;% # = map(function(x) x[[&quot;result&quot;]]) = map(~.x[[&quot;name&quot;]]) purrr::compact() # Remove empty elements ## [[1]] ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[2]] ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... ## ## [[3]] ## {html_document} ## &lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ... 5.6.2.3 possibly What if the best way to solve the problem is not ignoring the error … # If error occurred, &quot;The URL is broken.&quot; will be stored in that element(s). out &lt;- map( url_lists, possibly(read_html, otherwise = &quot;The URL is broken.&quot; ) ) # Let&#39;s find the broken URL. url_lists[out[seq(out)] == &quot;The URL is broken.&quot;] ## [1] &quot;https://DLAB&quot; 5.7 Developing your own data tools "],
["semi-structured-data.html", "Chapter 6 Semi-structured data 6.1 HTML/CSS: web scraping 6.2 XML/JSON: social media scraping", " Chapter 6 Semi-structured data 6.1 HTML/CSS: web scraping 6.2 XML/JSON: social media scraping "],
["machine-learning.html", "Chapter 7 High-dimensional data 7.1 Overview 7.2 Dataset 7.3 Workflow 7.4 Supervised learning 7.5 Unsupervised learning 7.6 Bias and fairness in machine learning 7.7 Resources", " Chapter 7 High-dimensional data 7.1 Overview The rise of high-dimensional data. The new data frontiers in social sciences—text (Gentzkow et al. 2019; Grimmer and Stewart 2013) and and image (Joo and Steinert-Threlkeld 2018)—are all high-dimensional data. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. “High-dimensional methods and inference on structural and treatment effects.” Journal of Economic Perspectives 28, no. 2 (2014): 29-50. The rise of new approach: statistics + computer science = machine learning Statistical inference \\(y\\) &lt;- some probability models (e.g., linear regression, logistic regression) &lt;- \\(x\\) \\(y\\) = \\(X\\beta\\) + \\(\\epsilon\\) The goal is to estimate \\(\\beta\\) Machine learning \\(y\\) &lt;- unknown &lt;- \\(x\\) \\(y\\) &lt;-&gt; decision trees, neutral nets &lt;-&gt; \\(x\\) For the main idea behind prediction modeling, see Breiman, Leo (Berkeley stat faculty who passed away in 2005). “Statistical modeling: The two cultures (with comments and a rejoinder by the author).” Statistical science 16, no. 3 (2001): 199-231. “The problem is to find an algorithm \\(f(x)\\) such that for future \\(x\\) in a test set, \\(f(x)\\) will be a good predictor of \\(y\\).” “There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.” Algorithmic models, both in theory and practice, has developed rapidly in fields of outside statistics. It can be used on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. - Leo Breiman How ML differs from econometrics? A review by Athey, Susan, and Guido W. Imbens. “Machine learning methods that economists should know about.” Annual Review of Economics 11 (2019): 685-725. Stat: Specifying a target (i.e., an estimand) Fitting a model to data using an objective function (e.g., the sum of squared errors) Reporting point estimates (effect size) and standard errors (uncertainty) Validation by yes-no using goodness-of-fit tests and residual examination ML: Developing algorithms (estimating f(x)) Prediction power not structural/causal parameters Basically, high-dimensional data statistics (N &lt; P) The major problem is to avoid “the curse of dimensionality” (too many features - &gt; overfitting) Validation: out-of-sample comparisons (cross-validation) not in-sample goodness-of-fit measures So, it’s curve-fitting but the primary focus is unseen (test data) not seen data (training data) A quick review on ML lingos for those trained in econometrics Sample to estimate parameters = Training sample Estimating the model = Being trained Regressors, covariates, or predictors = Features Regression parameters = weights Prediction problems = Supervised (some \\(y\\) are known) + Unsupervised (\\(y\\) unknown) How to teach machines. Based on vas3k blog. Many images in this chapter come from vas3k blog. The main types of machine learning. Based on vas3k blog The map of the machine learning universe. Based on vas3k blog Classical machine learning. Based on vas3k blog 7.2 Dataset pacman::p_load(tidymodels) # Ames, Iowa housing data data(ames) # Glimpse ames %&gt;% glimpse() ## Rows: 2,930 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_194… ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density, Residential_High_Density,… ## $ Lot_Frontage &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63… ## $ Lot_Area &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 500… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Alley &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access,… ## $ Lot_Shape &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, R… ## $ Land_Contour &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, … ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, Al… ## $ Lot_Config &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, In… ## $ Land_Slope &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, … ## $ Neighborhood &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gi… ## $ Condition_1 &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, N… ## $ Condition_2 &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, No… ## $ Bldg_Type &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Tw… ## $ House_Style &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_St… ## $ Overall_Cond &lt;fct&gt; Average, Above_Average, Above_Average, Average, Av… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 19… ## $ Year_Remod_Add &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 19… ## $ Roof_Style &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, … ## $ Roof_Matl &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompS… ## $ Exterior_1st &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, Vinyl… ## $ Exterior_2nd &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, Vinyl… ## $ Mas_Vnr_Type &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, N… ## $ Mas_Vnr_Area &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Exter_Cond &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typic… ## $ Foundation &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PCon… ## $ Bsmt_Cond &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical,… ## $ Bsmt_Exposure &lt;fct&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No… ## $ BsmtFin_Type_1 &lt;fct&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, … ## $ BsmtFin_SF_1 &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3,… ## $ BsmtFin_Type_2 &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, … ## $ BsmtFin_SF_2 &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, … ## $ Bsmt_Unf_SF &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994… ## $ Total_Bsmt_SF &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595,… ## $ Heating &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Ga… ## $ Heating_QC &lt;fct&gt; Fair, Typical, Typical, Excellent, Good, Excellent… ## $ Central_Air &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,… ## $ Electrical &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, S… ## $ First_Flr_SF &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616,… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0… ## $ Gr_Liv_Area &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 161… ## $ Bsmt_Full_Bath &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,… ## $ Bsmt_Half_Bath &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Full_Bath &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2,… ## $ Half_Bath &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,… ## $ Bedroom_AbvGr &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4,… ## $ Kitchen_AbvGr &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ TotRms_AbvGrd &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8… ## $ Functional &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, … ## $ Fireplaces &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,… ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, At… ## $ Garage_Finish &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, … ## $ Garage_Cars &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2,… ## $ Garage_Area &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, … ## $ Garage_Cond &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typic… ## $ Paved_Drive &lt;fct&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Pave… ## $ Wood_Deck_SF &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 4… ## $ Open_Porch_SF &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, … ## $ Enclosed_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Screen_Porch &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140,… ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Pool_QC &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Po… ## $ Fence &lt;fct&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Min… ## $ Misc_Feature &lt;fct&gt; None, None, Gar2, None, None, None, None, None, No… ## $ Misc_Val &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0,… ## $ Mo_Sold &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6,… ## $ Year_Sold &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20… ## $ Sale_Type &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , … ## $ Sale_Condition &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, No… ## $ Sale_Price &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 21… ## $ Longitude &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.63… ## $ Latitude &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, … For more information on the Iowa housing data, read Cook (2011). This is one of the famous datastets used in many prediction modeling competitions. 7.3 Workflow Data splitting Preprocessing Model building Model fitting Model evaluation Model tuning Prediction 7.3.1 Tidymodels Like tidyverse, tidymodels is a collection of packages. Tidymodels. From RStudio. tidymodels are an integrated, modular, extensible set of packages that implement a framework that facilitates creating predicative stochastic models. - Joseph Rickert@RStudio Currently, 238 models are available 7.3.2 Data split rsample: for data splitting # data split set.seed(1234) df_split &lt;- rsample::initial_split(df, prop = 0.7) train &lt;- rsample::training(df_split) test &lt;- rsample::testing(df_split) 7.3.2.1 Cross-validation 7.3.3 Pre-process recipes: for pre-processing textrecipes for text pre-processing # preprocess df_recipe &lt;- df %&gt;% recipe(resonse ~.) %&gt;% # Centering: x - mean(x) step_center(all_predictors(), -all_outcomes()) %&gt;% # Scaling: x * k step_scale(all_predictors(), -all_outcomes()) %&gt;% prep() # preprocessed training and testsets processed_train &lt;- df_recipe %&gt;% bake(train) processed_test &lt;- df_recipe %&gt;% bake(test) 7.3.4 Model building parsnip: for model building # Fit model ## Choose model df_ranger &lt;- rand_forest(trees = 1000, ## Declare mode mode = &quot;classification&quot;) %&gt;% ## Choose engine set_engine(&quot;ranger&quot;) %&gt;% ## Fit fit(response ~ ., data = processed_train) # Make predictions df_pred &lt;- predict(df_ranger, processed_test) 7.3.5 Model evaluation yardstick: for model evaluations # validate df_pred %&gt;% bind_cols(processed_test) %&gt;% # You can also easily change metrics you want to use metrics(truth = response, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.9 ## 2 kap multiclass 0.8 7.3.6 Tuning tune: parameter tuning 7.4 Supervised learning x -&gt; f - &gt; y (defined) 7.4.1 Regularization 7.4.1.1 Regression (OLS) # Build a linear regression model out &lt;- lm(mpg ~ cyl, data = mtcars) # Predict the first five rows predict(out)[1:5] ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 20.62984 20.62984 26.38142 20.62984 ## Hornet Sportabout ## 14.87826 Based on vas3k blog 7.4.1.2 Lasso, ridge, and elastic net Tibshirani, Robert. “The lasso method for variable selection in the Cox model.” Statistics in medicine 16, no. 4 (1997): 385-395. R glmnet package 7.4.2 Decision tree and ensemble models 7.4.2.1 Decision tree Partitioning feature space sequentially. 7.4.2.1.1 Bandit algorithm (optimizing an experiment) 7.4.2.2 Random forest 7.4.2.2.1 Causal forest (estimating heterogeneous treatment effect) 7.4.2.3 XGboost Repeatedly using weak learners. 7.4.2.4 SuperLearners 7.4.2.5 Neural networks / Deep learning 7.5 Unsupervised learning x -&gt; f - &gt; y (not defined) 7.5.1 Dimension reduction Projecting 2D-data to a line (PCA). From vas3k.com 7.5.2 Clustering Agglomerative Hierarchical Clustering. From George Seif’s medium post. 7.5.2.1 Topic modeling 7.6 Bias and fairness in machine learning 7.7 Resources 7.7.1 Books An Introduction to Statistical Learning - with Applications in R (2013) by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. Amazon or free PDF. Hands-On Machine Learning with R (2020) by Bradley Boehmke &amp; Brandon Greenwell. CRC Press or Amazon Applied Predictive Modeling (2013) by Max Kuhn and Kjell Johnson. Springer: New York. Amazon Feature Engineering and Selection: A Practical Approach for Predictive Models (2019) by Kjell Johnson and Max Kuhn. Taylor &amp; Francis. Amazon or free HTML. Tidy Modeling with R (2020) by Max Kuhn and Julia Silge (work-in-progress) 7.7.2 Lecture slides An introduction to supervised and unsupervised learning (2015) by Susan Athey and Guido Imbens "],
["big-data.html", "Chapter 8 Big data 8.1 Database and SQL", " Chapter 8 Big data 8.1 Database and SQL "]
]
