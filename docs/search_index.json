[
["index.html", "Computational Thinking for Social Scientists Chapter 1 PS239T 1.1 Textbook 1.2 Objectives 1.3 Logistics 1.4 Course schedule 1.5 Questions, comments, or suggestions 1.6 Special thanks 1.7 License", " Computational Thinking for Social Scientists Jae Yeon Kim 2020-09-29 Chapter 1 PS239T Welcome to PS239T This course will help social science graduate students to think computationally and develop proficiency with computational tools and techniques, necessary to conduct research in computational social science. Mastering these tools and techniques not only enables students to collect, wrangle, analyze, and interpret data with less pain and more fun, but it also let students to work on research projects that would previously seem impossible. 1.1 Textbook The online book version of the course materials is currently work in progress. I am loosely aiming the completion of this book in 2020. 1.2 Objectives The goal of this course is to help students become a more efficient and innovative researcher by leveraging the power of automation. The course is currently divided into two main subjects (fundamentals and applications) and six main sessions. 1.2.1 Part I Fundamentals In the first section, students learn best practices in data and code management using Git and Bash. In the second, students learn how to wrangle, model, and visualize data easier and faster. In the third, students learn how to use functions to automate repeated things and develop their own data tools (e.g., packages). 1.2.2 Part II Applications In the fourth, students learn how to collect and parse semi-structured data at scale (e.g., using APIs and webscraping). In the fifth, students learn how to analyze high-dimensional data (e.g., text) using machine learning. In the final, students learn how to access, query, and manage big data using SQL. We will learn how to do all of the above mostly in R, and sometimes in bash and Python. R is free, easy to learn (thanks to tidyverse and RStudio), fast (thanks to rcpp), runs everywhere, open (16,000+ packages; counting only ones available at CRAN), and has a growing massive and inclusive community (#rstats). 1.3 Logistics 1.3.1 Contributors Instructor and content developer: Jae Yeon Kim: jaeyeonkim@berkeley.edu 1.3.2 Time and location Lecture: TBD (Zoom) Section: TBD (Zoom) 1.3.3 Office hours By appointment with … 1.3.4 Slack &amp; GitHub Slack for communication (announcements and questions). You should ask questions about class material and assignments through the Slack channels so that everyone can benefit from the discussion. We encourage you to respond to each other’s questions as well. GitHub for everything else, including turning in assignments (except final project proposals, which will be submitted to Slack). Students are required to use GitHub for their final projects, which will be publicly available, unless they have special considerations (e.g. proprietary data). All course materials will be posted on GitHub at https://github.com/jaeyk/PS239T, including class notes, code demonstrations, sample data, and assignments. 1.3.5 Accessibility This class is committed to creating an environment in which everyone can participate, regardless of background, discipline, or disability. If you have a particular concern, please come to me as soon as possible so that we can make special arrangements. 1.3.6 Code for conduct TBD 1.3.7 Course requirements and grades This is a graded class based on the following: Completion of assigned homework (50%) Participation (25%) Final project (25%) 1.3.7.1 Assignments Assignments will be assigned at the end of every session. They will be due at the start of the following class unless otherwise noted. The assignments will be frequent but each of them should be fairly short. You are encouraged to work in groups, but the work you turn in must be your own. Group submission of homework, or turning in copies of the same code or output, is not acceptable. Remember, the only way you actually learn how to write code is to write code. Unless otherwise specified, assignments should be turned in as pdf documents via the bCourses site. 1.3.7.2 Class participation The class participation portion of the grade can be satisfied in one or more of the following ways: attending the lecture and section (note that section is non-optional) asking and answering questions in class contributing to class discussion through the bCourse site, and/or collaborating with the campus computing community, either by attending a D-Lab or BIDS workshop, submitting a pull request to a campus github repository (including the class repository), answering a question on StackExchange, or other involvement in the social computing / digital humanities community. Because we will be using laptops every class, the temptation to attend to other things during slow moments will be high. While you may choose to do so, I do request that you think of your laptop screen as in the public domain for the duration of classtime. Please do not load anything that will distract your classmates or is otherwise inappropriate to a classroom setting. 1.3.7.3 Final project The final project consists of using the tools we learned in class on your own data of interest. First- and second-year students in the political science department are encouraged to use this as an opportunity to gather data to be used for other courses or the second-year thesis. Students are required to write a short proposal by March (no more than 2 paragraphs) in order to get approval and feedback from the instructor. During sections in April we will have lightning talk sessions where students present their projects in a maximum 5 minute talk, with 5 minutes for class Q&amp;A. Since there is no expectation of a formal paper, you should select a project that is completable by the end of the term. In other words, submitting a research design for your future dissertation that will use skills from the class but collects no data is not acceptable, but completing a viably small portion of a study or thesis is. Final project rubric Final project template Final project examples 1.3.8 Class activities and materials 1.3.8.1 Lecture Classes will follow a “workshop” style, combining lecture and lab formats. The class is interactive, with students programming every session. During the “skills” parts of the class, we will be learning how to program in R, UNIX (bash), and Python by following course notes and tutorials. During the “applications” sections, we will follow a similar structure, with occasional guest speakers. 1.3.8.2 Section The “lab” section will generally be a less formal session dedicated to helping students with materials from lecture and homework. It will be mostly student led, so come with questions. If there are no questions, the lab turns into a “hackathon” where groups can work on the assignments together. Section is required unless prior permission to miss it is obtained from both the instructor and one’s groupmates. Attending office hours is not a substitute for attending section. 1.3.8.3 Computer requirements The software needed for the course is as follows: Access to the UNIX command line (e.g., a Mac laptop, a Bash wrapper on Windows) Git R and RStudio (latest versions) Anaconda and Python 3 (latest versions) Pandoc and LaTeX This requires a computer that can handle all this software. Almost any Mac will do the job. Most Windows machines are fine too if they have enough space and memory. You must have all the software downloaded and installed PRIOR to the first day of class. If there are issues with installation on your machine, please contact the section assistant, Julia Christensen, for assistance. See B_Install.md for more information. 1.4 Course schedule 1.4.1 Part I Fundamentals Week 1 Computational thinking and setup Week 2 Managing data and code Week 3 Tidy data and why it matters Week 4 Wrangling data Week 5 Wrangling data at scale Week 6 Modeling and visualizing tidy data Week 7 From for loop to functional programming Week 8 Developing your own data tools 1.4.2 Part II Applications Week 9 HTML/CSS: web scraping Week 10 XML/JSON: social media scraping Week 11 Supervised machine learning Week 12 Unsupervised machine learning Week 13 Database, SQL, MongoDB, and Spark Week 14 Wrap-up Week 15 Final presentation 1.5 Questions, comments, or suggestions Please create issues [TBD: issue template], if you have questions, comments, or suggestions. 1.6 Special thanks This course is a remix version of the course originally developed by Rochelle Terman then revised by Rachel Bernhard. Other teaching materials draw from the workshops I created for D-Lab and Data Science Discovery Program at UC Berkeley. 1.7 License This work is licensed under a Creative Commons Attribution 4.0 International License. "],
["motivation.html", "Chapter 2 Computational thinking 2.1 Why computational thinking 2.2 Computational way of thinking about data 2.3 Computational way of thinking about research process", " Chapter 2 Computational thinking 2.1 Why computational thinking If social scientists want to know how to work smart and not just hard, they need to take full advantage of the power of modern programming languages, and that power is automation. Let’s think about the following two cases. Case 1: Suppose a social scientist needs to collect data on civic organizations in the United States from websites, Internal Revenue Service reports, and social media posts. As the number of these organizations is large, the researcher could not collect a large volume of data from diverse sources, so they would hire undergraduates and distribute tasks among them. This is a typical data collection plan in social science research, and it is labor-intensive. Automation is not part of the game plan. Yet, it is critical for so many reasons. Because the process is costly, no one is likely to either replicate or update the data collection effort. Put differently, without making the process efficient, it is difficult for it to be reproducible and scalable. Case 2: An alternative is to write computer programs that collect such data automatically, parse them, and store them in interconnected databases. Additionally, someone may need to maintain and validate the quality of the data infrastructure. Nevertheless, this approach lowers the cost of the data collection process, thereby substantially increasing the reproducibility and scalability of the process. Furthermore, the researcher can document their code and publicly share it using their GitHub repository or even gather some of the functions they used and distribute them as open-source libraries. Programming is as valuable a skill as writing in social science research. The extent to which a researcher can automate the research process can determine its efficiency, reproducibility, and scalability. Every modern statistical and data analysis problem needs code to solve it. You shouldn’t learn just the basics of programming, spend some time gaining mastery. Improving your programming skills pays off because code is a force multiplier: once you’ve solved a problem once, code allows you to solve it much faster in the future. As your programming skill increases, the generality of your solutions improves: you solve not just the precise problem you encountered, but a wider class of related problems (in this way programming skill is very much like mathematical skill). Finally, sharing your code with others allows them to benefit from your experience. - Hadley Wickham How can we automate our research process? How can we talk to and teach a machine so that it could become (hopefully) the most competent and reliable research assistant ever? From BBC Bitesize 2.2 Computational way of thinking about data 2.2.1 Structure Structured data (Excel spreadsheets, CSVs) Tidy data Semi-structured data HTML/CSS: Websites JSON/XML: APIs 2.2.2 Dimension Low-dimensional data (n &gt; p) Survey, experimental, and administrative data High-dimensional data (n &lt; p) Text, speech, image, video, etc. 2.2.3 Size Data fit in your laptop’s memory Data don’t fit in your laptop’s memory (=big data) 2.3 Computational way of thinking about research process Computational tools and techniques make … Doing traditional research easier, faster, scalable, and more reproducible Data wrangling Modeling Visualization Documentation and collaboration easier, faster, scalable, safer, and more experimental Dynamic reporting (markdown) Version control system (Git and GitHub) Collecting and analyzing large and complex data possible Digital data collection (API and web scraping) Building a data infrastructure (SQL) Machine learning "],
["intro.html", "Chapter 3 Managing data and code 3.1 Project-oriented research 3.2 Writing code: How to code like a professional 3.3 Asking questions: Minimal reproducible example 3.4 References", " Chapter 3 Managing data and code 3.1 Project-oriented research 3.1.1 Computational reproducibility 3.1.1.1 Setup pacman::p_load( tidyverse, # tidyverse here # computational reproducibility ) 3.1.1.2 Motivation Why do you need to make your research project computationally reproducible? For your self-interest and public benefits. 3.1.1.3 How to organize files in a project You won’t be able to reproduce your project unless it is efficiently organized. Step 1. Environment is part of your project. If someone can’t reproduce your environment, they won’t be able to run your code. Launch R Studio. Choose Tools &gt; Global Options. You should not check Restor .RData into workspace at startup and set saving workspace option to NEVER. Step 2. For each project, create a project directory named after the project. # Don&#39;t name it a project. Use a name that&#39;s more informative. For instance, us_election not my_project. dir.create(&quot;../us_election&quot;) Step 3. Launch R Studio. Choose File &gt; New project &gt; Browse existing directories &gt; Create project This allows each project has its own workspace. Step 4. Organize files by putting them in separate subdirectories and naming them in a sensible way. Treat raw data as read only (raw data should be RAW!) and put in the data subdirectory. Note that version control does not need replace backup. You still need to backup your raw data. dir.create(here::here(&quot;us_election&quot;, &quot;data&quot;)) Separate read-only data from processed data and put in the processed_data subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;processed_data&quot;)) Put your code in the src subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Put generated outputs (e.g., tables, figures) in the outputs subdirectory and treat them as disposable. dir.create(here::here(&quot;us_election&quot;, &quot;outputs&quot;)) Put your custom functions in the functions subdirectory. You can gather some of these functions and distribute them as an open-source library. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Challenge 2 Set a project structure for a project named “starwars”. 3.1.1.4 How to organize code in a R markdown file In addition to environment, workflow is an important component of project efficiency and reproducibility. What is R markdown? An R package, developed by Yihui Xie, that provides an authoring framework for data science. Xie is also a developer of many widely popular R packages such as knitr, xaringan (cool kids use xaringan not Beamer these days), blogdown (used to create my personal website), and bookdown (used to create this book) among many others. Many applications: reports, presentations, dashboards, websites Check out Communicating with R markdown workshop by Alison Hill (RStudio) Alison Hill is a co-author of blogdown: Creating Websites with R Markdown. Key strengths: dynamic reporting + reproducible science + easy deployment R Markdown The bigger picture - Garrett Grolemund R-Ladies Oslo (English) - Reports to impress your boss! Rmarkdown magic - Athanasia Mowinckel R Markdown basic syntax # Header 1 ## Header 2 ### Header 3 Use these section headers to indicate workflow. # Import packages and data # Tidy data # Wrangle data # Model data # Visualize data Press ctrl + shift + o. You can see a document outline based on these headers. This is a nice feature for finding code you need to focus. If your project’s scale is large, then divide these sections into files, number, and save them in code subdirectory. 01_wrangling.Rmd 02_modeling.Rmd … 3.1.1.5 Making a project computationally reproducible setwd(): set a working directory. Note that using setwd() is not a reproducible way to set up your project. For instance, none will be able to run the following code except me. # Set a working directory setwd(&quot;/home/jae/starwars&quot;) # Do something ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() # Export the object. # dot means the working directory set by setwd() ggsave(&quot;./outputs/example.png&quot;) # This is called relative path Instead, learn how to use here()’. Key idea: separate workflow (e.g., workspace information) from products (code and data). For more information, read Jenny Bryan’s wonderful piece on project-oriented workflow. Example # New: Reproducible ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() ggsave(here(&quot;project&quot;, &quot;outputs&quot;, &quot;example.png&quot;)) How here works here() function shows what’s the top-level project directory. here::here() Build a path including subdirectories here::here(&quot;project&quot;, &quot;outputs&quot;) #depth 1 #depth 2 How here defines the top-level project directory. The following list came from the here package vignette). Is a file named .here present? Is this an RStudio Project? (Note that we already set up an RStudio Project! So, if you use RStudio’s project feature, then you are ready to use here.) Is this an R package? Does it have a DESCRIPTION file? Is this a remake project? Does it have a file named remake.yml? Is this a projectile project? Does it have a file named .projectile? Is this a checkout from a version control system? Does it have a directory named .git or .svn? Currently, only Git and Subversion are supported. If there’s no match then use set_here() to create an empty .here file. Challenge 1 Can you define computational reproducibility? Can you explain why sharing code and data is not enough for computational reproducibility? 3.1.2 Version control (Git and Bash) 3.1.2.1 What Is Bash? 3.1.2.1.1 Writing your first shell script Write a shell script that creates a directory called /pdfs under /Download directory, then find PDF files in /Download and copy those files to pdfs. This shell script creates a backup. #!/bin/sh mkdir /home/jae/Downloads/pdfs cd Download cp *.pdf pdfs/ echo &quot;Copied pdfs&quot; 3.1.2.2 What Are Git and GitHub? Figure 2.1. A schematic git workflow from Healy’s “The Plain Person’s Guide to Plain Text Social Science” 3.1.2.2.1 Basics: git push and git pull 3.1.2.2.2 Time machine: git revert 3.1.2.2.3 Parallel universe: git branch 3.1.2.2.4 User-manual: readme README.md In this simple markdown file, note some basic information about the project including the project structure. This is how I used the README.md file for this course. Check out my GitHub account to see how I manage my projects. 3.1.2.3 Deployment: GitHub Pages 3.1.2.4 Tracking progress: GitHub Issues 3.1.2.5 Project management: GitHub Dashboards 3.2 Writing code: How to code like a professional 3.2.1 Write readable code What is code style? Every major open-source project has its own style guide: a set of conventions (sometimes arbitrary) about how to write code for that project. It is much easier to understand a large codebase when all the code in it is in a consistent style. - Google Style Guides 10 Tips For Clean Code - Michael Toppa How to avoid smelly code? Check out the code-smells Git repository by Jenny Bryan. Code smells and feels - Jenny Bryan \"Code smell\" is an evocative term for that vague feeling of unease we get when reading certain bits of code. It's not necessarily wrong, but neither is it obviously correct. We may be reluctant to work on such code, because past experience suggests it's going to be fiddly and bug-prone. In contrast, there's another type of code that just feels good to read and work on. What's the difference? If we can be more precise about code smells and feels, we can be intentional about writing code that is easier and more pleasant to work on. I've been fortunate to spend the last couple years embedded in a group of developers working on the tidyverse and r-lib packages. Based on this experience, I'll talk about specific code smells and deodorizing strategies for R. - Jenny Bryan Naming matters When naming files: Don’t use special characters. (Spaces make filenames awkward in the console/command-line.) Don’t capitalize. (UNIX is case sensitive.) Numbering them if files should be run in an order. # Good fit_models.R # Bad fit models.R When naming objects: Don’t use special characters. Don’t capitalize. # Good day_one # Bad DayOne When naming functions: Don’t use special characters. Don’t capitalize. Use verbs instead of nouns. (Functions do something!) # Good run_rdd # Bad rdd Spacing # Good x[, 1] mean(x, na.rm = TRUE) # Bad x[,1] mean (x, na.rm = TRUE) Indenting # Good if (y &lt; 0) { message(&quot;y is negative&quot;) } # Bad if (y &lt; 0) { message(&quot;Y is negative&quot;)} Long lines # Good do_something_very_complicated( something = &quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) # Bad do_something_very_complicated(&quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) Comments Use comments to explain your decisions. But, show your code; Do not try to explain your code by comments. Also, try to comment out rather than delete the code that you experiment with. # Average sleep hours of Jae jae %&gt;% # By week group_by(week) %&gt;% # Mean sleep hours summarise(week_sleep = mean(sleep, na.rm = TRUE)) Pipes (chaining commands) # Good iris %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, mean) %&gt;% ungroup() %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) # Bad iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% ungroup %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) Additional tips Use lintr to check whether your code complies with a recommended style guideline (e.g., tidyverse) and styler package to format your code according to the style guideline. how lintr works 3.2.2 Write reusable code Pasting Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia It’s okay for pasting for the first attempt to solve a problem. But if you copy and paste three times (a.k.a. Rule of Three in programming), something’s wrong. You’re working too hard. You need to be lazy. What do I mean and how can you do that? Example Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble(&quot;a&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE)) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Using a function function: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. # Create a custom function fix_missing &lt;- function(x) { # INPUT x[x == -99] &lt;- NA # COMPUTATION x # OUTPUT } # Apply the function to each column (vector) # This iterated part can and should be automated. df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df Automation Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) # What is this magic? We will unpack the blackbox (`map_df()`) later. df Takeaways Your code becomes more reusable, when it’s easier to change, debug, and scale up. Don’t repeat yourself and embrace the power of lazy programming. Lazy, because only lazy programmers will want to write the kind of tools that might replace them in the end. Lazy, because only a lazy programmer will avoid writing monotonous, repetitive code—thus avoiding redundancy, the enemy of software maintenance and flexible refactoring. Mostly, the tools and processes that come out of this endeavor fired by laziness will speed up the production. - Philipp Lenssen Only when your code becomes reusable, you would become efficient in your data work. Otherwise, you need to start from scratch or copy and paste, when you work on a new project. Code reuse aims to save time and resources and reduce redundancy by taking advantage of assets that have already been created in some form within the software product development process.[2] The key idea in reuse is that parts of a computer program written at one time can be or should be used in the construction of other programs written at a later time. - Wikipedia 3.2.3 Test your code systematically 3.3 Asking questions: Minimal reproducible example 3.3.1 How to create a minimal reproducible example 3.4 References Project-oriented research Computational reproducibility “Good Enough Practices in Scientific Computing” by PLOS Project Management with RStudio by Software Carpentry Initial steps toward reproducible research by Karl Broman Version control Version Control with Git by Software Carpentry The Plain Person’s Guide to Plain Text Social Science by Kieran Healy Writing code Style guides R Google’s R style guide R code style guide by Hadley Wickham The tidyverse style guide by Hadley Wickham Python Google Python Style Guide Code Style by the Hitchhiker’s Guide to Python Asking questions "],
["tidy-data.html", "Chapter 4 Tidy data and its friends 4.1 Setup 4.2 Tidyverse way of thinking data science workflow 4.3 Tidy data and why it matters 4.4 Wrangling data 4.5 Wrangling data at scale 4.6 Modeling and visualizing tidy data", " Chapter 4 Tidy data and its friends 4.1 Setup Check your dplyr package is up-to-date by typing packageVersion(\"dplyr\"). If the current installed version is less than 1.0, then update by typing update.packages(\"dplyr\"). You may need to restart R to make it work. ifelse(packageVersion(&quot;dplyr&quot;) &gt; 1, &quot;The installed version of dplyr package is greater than or equal to 1.0.0&quot;, update.packages(&quot;dplyr&quot;)) ## [1] &quot;The installed version of dplyr package is greater than or equal to 1.0.0&quot; if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load( tidyverse, # for the tidyverse framework here # for computational reproducibility ) The rest of the chapter follows the basic structure in the Data Wrangling Cheat Sheet created by RStudio. 4.2 Tidyverse way of thinking data science workflow Tidyverse design guide Human centered Consistent Composable (modualized) Inclusive Influenced by the Basics of the Unix Philosophy, The Zen of Python, and the Design Principles Behind Smalltalk 4.3 Tidy data and why it matters “Tidy data sets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” - Hadley Wickham Variables -&gt; Columns Observations -&gt; Rows Values -&gt; Cells Tidy Data Example (Source: R for Data Science) If dataframes are tidy, it’s easy to transform, visualize, model, and program them using tidyverse packages (a whole workflow). Tidyverse: an opinionated collection of R packages Nevertheless, don’t be religious. In summary, tidy data is a useful conceptual idea and is often the right way to go for general, small data sets, but may not be appropriate for all problems. - Jeff Leek For instance, in many data science applications, linear algebra-based computations are essential (e.g., Principal Component Analysis). These computations are optimized to work on matrices, not tidy data frames (for more information, read Jeff Leek’s blog post). This is what a tidy data looks like. library(tidyverse) table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 4.4 Wrangling data 4.5 Wrangling data at scale 4.6 Modeling and visualizing tidy data "],
["functional-programming.html", "Chapter 5 Automating repeated things 5.1 Why functional programming 5.2 Automote 2 or 2+ tasks 5.3 Automate plotting 5.4 Automate joining 5.5 Make automation slower or faster 5.6 Make error handling easier 5.7 Developing your own data tools", " Chapter 5 Automating repeated things 5.1 Why functional programming Setup # Install packages if (!require(&quot;pacman&quot;)) { install.packages(&quot;pacman&quot;) } ## Loading required package: pacman pacman::p_load( tidyverse, # tidyverse pkgs including purrr tictoc, # performance test broom, # tidy modeling glue, # paste string and objects furrr, # parallel processing rvest ) # web scraping 5.1.1 Why map? 5.1.1.1 Objectives How to use purrr to automate workflow in a cleaner, faster, and more extendable way 5.1.1.2 Copy-and-paste programming Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia Example Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble( &quot;a&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE) ) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge. Explain why this solution is not very efficient (Hint: If df$a[df$a == -99] &lt;- NA has an error, how are you going to fix it? A solution is not scalable if it’s not automatable. 5.1.1.3 Using a function Let’s recall what’s function in R: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. ` function(input){ computation return(output) } ` # Function fix_missing &lt;- function(x) { x[x == -99] &lt;- NA x } # Apply function to each column (vector) df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge Why using function is more efficient than 100% copying and pasting? Can you think about a way we can automate the process? Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 map() is a higher-order function that applies a given function to each element of a list/vector. This is how map() works. It’s easier to understand with a picture. - Input: Takes a vector/list. - Computation: Calls the function once for each element of the vector - Output: Returns in a list or whatever data format you prefer (e.g., `_df helper: dataframe`) Challenge If you run the code below, what’s going to be the data type of the output? map(df, fix_missing) ## $a ## [1] 3 3 1 1 NA ## ## $b ## [1] 3 2 NA NA 1 ## ## $c ## [1] 3 3 1 2 1 ## ## $d ## [1] 1 1 2 1 3 Why map() is a good alternative to for loop. The Joy of Functional Programming (for Data Science) - Hadley Wickham # Built-in data data(&quot;airquality&quot;) tic() # Placeholder out1 &lt;- vector(&quot;double&quot;, ncol(airquality)) # Sequence variable for (i in seq_along(airquality)) { # # Assign a computation result to each element out1[[i]] &lt;- mean(airquality[[i]], na.rm = TRUE) } toc() ## 0.007 sec elapsed tic() out1 &lt;- airquality %&gt;% map_dbl(mean, na.rm = TRUE) toc() ## 0.002 sec elapsed In short, map() is more readable, faster, and easily extendable with other data science tasks (e.g., wrangling, modeling, and visualization) using %&gt;%. Final point: Why not base R apply family? Short answer: purrr::map() is simpler to write. For instance, map_dbl(x, mean, na.rm = TRUE) = vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1)) 5.1.1.4 Application (many models) One popular application of map() is to run regression models (or whatever model you want to run) on list-columns. No more copying and pasting for running many regression models on subgroups! # Have you ever tried this? lm_A &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_A&quot;)) lm_B &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_B&quot;)) lm_C &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_C&quot;)) lm_D &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_D&quot;)) lm_E &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_E&quot;)) For more information on this technique, read the Many Models subchapter of the R for Data Science. # Function lm_model &lt;- function(df) { lm(Temp ~ Ozone, data = df) } # Map models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, lm_model)) # Map models$ols[1] ## [[1]] ## ## Call: ## lm(formula = Temp ~ Ozone, data = df) ## ## Coefficients: ## (Intercept) Ozone ## 62.8842 0.1629 # Add tidying tidy_lm_model &lt;- purrr::compose( # compose multiple functions broom::tidy, # convert lm objects into tidy tibbles lm_model ) tidied_models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, tidy_lm_model)) tidied_models$ols[1] ## [[1]] ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 62.9 1.61 39.2 2.88e-23 ## 2 Ozone 0.163 0.0500 3.26 3.31e- 3 5.2 Automote 2 or 2+ tasks 5.2.1 Objectives Learning how to use map2() and pmap() to avoid writing nested loops. 5.2.2 Problem Problem: How can you create something like below? [1] “University = Berkeley | Department = waterbenders” [1] “University = Berkeley | Department = earthbenders” [1] “University = Berkeley | Department = firebenders” [1] “University = Berkeley | Department = airbenders” [1] “University = Stanford | Department = waterbenders” [1] “University = Stanford | Department = earthbenders” [1] “University = Stanford | Department = firebenders” [1] “University = Stanford | Department = airbenders” The most manual way: You can copy and paste eight times. paste(&quot;University = Berkeley | Department = CS&quot;) ## [1] &quot;University = Berkeley | Department = CS&quot; 5.2.3 For loop A slightly more efficient way: using a for loop. Think about which part of the statement is constant and which part varies ( = parameters). Do we need a placeholder? No. We don’t need a placeholder because we don’t store the result of iterations. Challenge: How many parameters do you need to solve the problem below? # Outer loop for (univ in c(&quot;Berkeley&quot;, &quot;Stanford&quot;)) { # Inner loop for (dept in c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;)) { print(paste(&quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept)) } } ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Berkeley | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Berkeley | Department = airbenders&quot; ## [1] &quot;University = Stanford | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Stanford | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; This is not bad, but … n arguments -&gt; n-nested for loops. As a scale of your problem grows, your code gets really complicated. To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs. — Bjarne Stroustrup 5.2.4 map2 &amp; pmap Step 1: Define inputs and a function. Challenge Why are we using rep() to create input vectors? For instance, for univ_list why not just use c(\"Berkeley\", \"Stanford\")? # Inputs (remember the length of these inputs should be identical) univ_list &lt;- rep(c(&quot;Berkeley&quot;, &quot;Stanford&quot;), 4) dept_list &lt;- rep(c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;), 2) # Function print_lists &lt;- function(univ, dept) { print(paste( &quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept )) } # Test print_lists(univ_list[1], dept_list[1]) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; Step2: Using map2() or pmap() # 2 arguments map2_output &lt;- map2(univ_list, dept_list, print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; # 3+ arguments pmap_output &lt;- pmap(list(univ_list, dept_list), print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; Challenge Have you noticed that we used a slightly different input for pmap() compared to map() or map2()? What is the difference? 5.3 Automate plotting 5.3.1 Objective Learning how to use map() and glue() to automate creating multiple plots 5.3.2 Problem Making the following data visualization process more efficient. data(&quot;airquality&quot;) airquality %&gt;% ggplot(aes(x = Ozone, y = Solar.R)) + geom_point() + labs( title = &quot;Relationship between Ozone and Solar.R&quot;, y = &quot;Solar.R&quot; ) ## Warning: Removed 42 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Wind)) + geom_point() + labs( title = &quot;Relationship between Ozone and Wind&quot;, y = &quot;Wind&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Temp)) + geom_point() + labs( title = &quot;Relationship between Ozone and Temp&quot;, y = &quot;Temp&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). 5.3.3 Solution Learn how glue() works. glue() combines strings and objects and it works simpler and faster than paste() or sprintif(). names &lt;- c(&quot;Jae&quot;, &quot;Aniket&quot;, &quot;Avery&quot;) fields &lt;- c(&quot;Political Science&quot;, &quot;Law&quot;, &quot;Public Health&quot;) glue(&quot;{names} studies {fields}.&quot;) ## Jae studies Political Science. ## Aniket studies Law. ## Avery studies Public Health. So, our next step is to combine glue() and map(). Let’s first think about writing a function that includes glue(). Challenge How can you create the character vector of column names? Challenge How can you make ggplot2() take strings as x and y variable names? (Hint: Type ?aes_string()) airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[2])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[2]}&quot;), y = glue(&quot;{names(airquality)[2]}&quot;) ) ## Warning: Removed 42 rows containing missing values (geom_point). The next step is to write an automatic plotting function. Note that in the function argument i (abstract) replaced 2 (specific): abstraction create_point_plot &lt;- function(i) { airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[i])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[i]}&quot;), y = glue(&quot;{names(airquality)[i]}&quot;) ) } The final step is to put the function in map(). map(2:ncol(airquality), create_point_plot) ## [[1]] ## Warning: Removed 42 rows containing missing values (geom_point). ## ## [[2]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[3]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[4]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[5]] ## Warning: Removed 37 rows containing missing values (geom_point). 5.4 Automate joining 5.4.1 Objective Learning how to use reduce() to automate joining multiple dataframes 5.4.2 Problem How can you make joining multiple dataframes more efficient? Note that we will use dplyr::left_join() = merge(x, y, all.x = TRUE). df1 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df2 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df3 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) 5.4.3 Copy and paste first_join &lt;- left_join(df1, df2) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) second_join &lt;- left_join(first_join, df3) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) second_join ## # A tibble: 3 x 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 5 8 ## 2 4 8 3 ## 3 4 4 4 Challenge Why the above solution is not efficient? 5.4.4 reduce How reduce() works. - Input: Takes a vector of length n - Computation: Calls a function with a pair of values at a time - Output: Returns a vector of length 1 reduced &lt;- reduce(list(df1, df2, df3), left_join) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) reduced ## # A tibble: 3 x 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 5 8 ## 2 4 8 3 ## 3 4 4 4 5.5 Make automation slower or faster 5.5.1 Objectives Learning how to use slowly() and future_ to make automation process either slower or faster 5.5.2 How to make automation slower Scraping 50 pages from a website and you don’t want to overload the server. How can you do that? 5.5.3 For loop 5.5.4 Map walk() works same as map() but doesn’t store its output. If you’re web scraping, one problem with this approach is it’s too fast by human standards. If you want to make the function run slowly … slowly() takes a function and modifies it to wait a given amount of time between each call. - purrr package vignette - If a function is a verb, then a helper function is an adverb (modifying the behavior of the verb). 5.5.5 How to make automation Faster In a different situation, you want to make your function run faster. This is a common situation when you collect and analyze data at large-scale. You can solve this problem using parallel processing. For more on the parallel processing in R, read this review. Parallel processing setup Step1: Determine the number of max workers (availableCores()) Step2: Determine the parallel processing mode (plan()) 5.6 Make error handling easier 5.6.1 Learning objective Learning how to use safely() and possibly() to make error handling easier ### Problem Challenge Explain why we can’t run map(url_lists, read_html) url_lists &lt;- c( &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot;, &quot;https://en.wikipedia.org/wiki/Stanford_University&quot;, &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot;, &quot;https://DLAB&quot; ) map(url_lists, read_html) This is a very simple problem so it’s easy to tell where the problem is. How can you make your error more informative? 5.6.2 Solution 5.6.2.1 Try-catch There are three kinds of messages you will run into, if your code has an error based on the following functions. stop(): errors; Functions must stop. warning(): warnings; Functions may still work. Nonetheless, something is possibly messed up. message(): messages; Some actions happened. The basic logic of try-catch, R’s basic error handling function, works like the following. tryCatch( { map(url_lists, read_html) }, warning = function(w) { &quot;Warning&quot; }, error = function(e) { &quot;Error&quot; }, finally = { &quot;Message&quot; } ) ## [1] &quot;Error&quot; Here’s purrr version of the try-catch mechanism (evaluates code and assigns exception handlers). 5.6.2.2 safely Outputs result: result or NULL error: NULL or error map(url_lists, safely(read_html)) ## [[1]] ## [[1]]$result ## NULL ## ## [[1]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Timeout was reached: [en.wikipedia.org] Connection timed out after 10000 milliseconds&gt; ## ## ## [[2]] ## [[2]]$result ## NULL ## ## [[2]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Timeout was reached: [en.wikipedia.org] Connection timed out after 10001 milliseconds&gt; ## ## ## [[3]] ## [[3]]$result ## NULL ## ## [[3]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Timeout was reached: [en.wikipedia.org] Connection timed out after 10001 milliseconds&gt; ## ## ## [[4]] ## [[4]]$result ## NULL ## ## [[4]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Could not resolve host: DLAB&gt; The easier way to solve this problem is just avoiding the error. map(url_lists, safely(read_html)) %&gt;% map(&quot;result&quot;) %&gt;% # = map(function(x) x[[&quot;result&quot;]]) = map(~.x[[&quot;name&quot;]]) purrr::compact() # Remove empty elements ## list() 5.6.2.3 possibly What if the best way to solve the problem is not ignoring the error … # If error occurred, &quot;The URL is broken.&quot; will be stored in that element(s). out &lt;- map( url_lists, possibly(read_html, otherwise = &quot;The URL is broken.&quot; ) ) # Let&#39;s find the broken URL. url_lists[out[seq(out)] == &quot;The URL is broken.&quot;] ## [1] &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot; ## [2] &quot;https://en.wikipedia.org/wiki/Stanford_University&quot; ## [3] &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot; ## [4] &quot;https://DLAB&quot; 5.7 Developing your own data tools "],
["semi-structured-data.html", "Chapter 6 Semi-structured data 6.1 Objectives 6.2 What is semi-structured data? 6.3 Workflow 6.4 HTML/CSS: web scraping 6.5 XML/JSON: social media scraping", " Chapter 6 Semi-structured data 6.1 Objectives Automating the process of turning semi-structured data (input) into structured data (output) 6.2 What is semi-structured data? Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. - Wikipedia Examples: HTML (Hypertext Markup Language) files (e.g., websites) and JSON (JavaScript Object Notation) files (e.g., tweets) Why should we care semi-structured data? Because this is what the data frontier looks like: # of unstructured data &gt; # of semi-structured data &gt; # of structured data There are easy and fast ways to turn semi-structured data into structured data (ideally in a tidy format) using R, Python, and command-line tools. See my own examples (tidyethnicnews and tidytweetjson). 6.3 Workflow Import/connect to a semi-structured file using rvest, jsonlite, xml2, pdftools, tidyjson, etc. Define target elements in the single file and extract them readr package providers parse_ functions that are useful for vector parsing. stringr package for string manipulations (e.g., using regular expressions in a tidy way). Quite useful for parsing PDF files (see this example). rvest package for parsing HTML (R equivalent to beautiful soup in Python) tidyjson package for parsing JSON data Create a list of files (in this case URLs) to parse Write a parsing function Automate parsing process 6.4 HTML/CSS: web scraping 6.5 XML/JSON: social media scraping 6.5.1 API 6.5.1.1 Objectives Learning what kind of social media data are accessible through application programming interfaces (APIs) Review question In the previous session, we learned the difference between semi-structured data and structured data. Can anyone tell us the difference between them? 6.5.1.2 The big picture for digital data collection Input: semi-structured data Output: structured data Process: Getting target data from a remote server The target data is usually huge (&gt;10GB) by the traditional social science standard. Parsing the target data your laptop/database Laptop (sample-parse): Downsamle the large target data and parse it on your laptop. This is just one option to deal with big data in R. It’s a simple strategy as it doesn’t require storing target data in your own database. Database (push-parse): Push the large target data to a database, then explore, select, and filter it. If you were interested in using this option, then check out my SQL for R Users workshop. Sample-Parse. From RStudio. Push-Parse. From RStudio. But what exactly is this target data? When you scrape websites, you mostly deal with HTML (defines a structure of a website), CSS (its style), and JavaScript (its dynamic interactions). When you access social media data through API, you deal with either XML or JSON (major formats for storing and transporting data; they are light and flexible). XML and JSON have tree-like (nested; a root and branches) structures and keys and values (or elements and attributes). If HTML, CSS, and JavaScript are storefronts, then XML and JSON are warehouses. By Andreas Praefcke (Own work), via Wikimedia Commons 6.5.1.3 Opportunities and challenges for parsing social media data This explanation draws on Pablo Barbara’s LSE social media workshop slides. Basic information What is an API?: An interface (you can think of it as something akin to restaurant menu. API parameters are menu items.) REST (Representational state transfer) API: static information (e.g., user profiles, list of followers and friends) R packages: tweetscores, twitteR, rtweet Streaming API: dynamic information (e..g, new tweets) This streaming data is filtered by (1) keywords, (2) location, and (3) sample (1% of the total tweets) R packages: streamR Status Twitter API is still widely accessible (v2 recently released; new fields available such as conversation threads). Twitter data is unique from data shared by most other social platforms because it reflects information that users choose to share publicly. Our API platform provides broad access to public Twitter data that users have chosen to share with the world. - Twitter Help Center - What does this policy mean? If Twitter users don&#39;t share the locations of their tweets (e.g., GPS), you can&#39;t get collect them. Facebook API access has become much constrained with the exception of Social Science One since the 2016 U.S. election. YouTube API access is somewhat limited (but you need to check as I’m not updated on this). Upside Legal and well-documented. Web scraping (Wild Wild West) &lt;&gt; API (Big Gated Garden) You have legal but limited access to (growing) big data that can be divided into text, image, and video and transformed into cross-sectional (geocodes), longitudinal (timestamps), and event historical data (hashtags). For more information, see Zachary C. Steinert-Threlkeld’s 2020 APSA Short Course Generating Event Data From Social Media. Social media data are also well-organized, managed, and curated data. It’s easy to navigate because XML and JSON have keys and values. If you find keys, you will find observations you look for. Downside Rate-limited. If you want to access to more and various data than those available, you need to pay for premium access. 6.5.1.4 Next steps If you want to know how to sign up a new Twitter developer account and access Twitter API, then see Steinert-Threlkeld’s APSA workshop slides. If you want to know about how to use tweetscore package, then see Pablo Barbara’s R markdown file for scraping data from Twitter’s REST API 6.5.2 Hydrating 6.5.2.1 Objectives Learning how hydrating works Learning how to use Twarc to communicate with Twitter’s API Review question What are the main two types of Twitter’s API? 6.5.2.2 Hydrating: An Alternative Way to Collect Historical Twitter Data You can collect Twitter data using Twitter’s API or you can hydrate Tweet IDs collected by other researchers. This is a good resource to collect historical Twitter data. Covid-19 Twitter chatter dataset for scientic use by Panacealab Women’s March Dataset by Littman and Park Harvard Dataverse has a number of dehydrated Tweet IDs that could be of interest to social scientists. Dehydrated Tweet IDs 6.5.2.3 Twarc: one solution to (almost) all Twitter’s API problems Why Twarc? A command-line tool and Python library that works for almost every Twitter’s API related problem. It’s really well-documented, tested, and maintained. Twarc documentation covers basic commands. Tward-cloud documentation explains how to collect data from Twitter’s API using Twarc running in Amazon Web Services (AWS). Twarc was developed as part of the Documenting the Now project which was funded by the Mellon Foundation. One ring that rules them all. There’s no reason to be afraid of using a command-line tool and Python library, even though you primarily use R. It’s easy to embed Python code and shell scripts in R Markdown. Even though you don’t know how to write Python code or shell scripts, it’s really useful to know how to integrate them in your R workflow. I assume that you have already installed Python 3. pip3 install twarc 6.5.2.3.1 Applications The following examples are created by the University of Virginia library. 6.5.2.3.1.1 Search Download pre-existing tweets (7-day window) matching certain conditions In command-line, &gt; = Create a file I recommend running the following commands in the terminal because it’s more stable than doing so in R Markdown. You can type commands in the Terminal in R Studio. # Key word twarc search blacklivesmatter &gt; blm_tweets.jsonl # Hashtag twarc search &#39;#blacklivesmatter&#39; &gt; blm_tweets_hash.jsonl # Hashtag + Language twarc search &#39;#blacklivesmatter&#39; --lang en &gt; blm_tweets_hash.jsonl It is really important to save these tweets into a jsonl format; jsonl extension refers to JSON Lines files. This structure is useful for splitting JSON data into smaller chunks, if it is too large. 6.5.2.3.1.2 Filter Download tweets meeting certain conditions as they happen. # Key word twarc filter blacklivesmatter &gt; blm_tweets.jsonl 6.5.2.3.1.3 Sample Use Twitter’s random sample of recent tweets. twarc sample &gt; tweets.jsonl 6.5.2.3.1.4 Hydrate Tweet IDs -&gt; Tweets twarc hydrate tweet_ids.txt &gt; tweets.jsonl 6.5.2.3.1.5 Dehydrate Hydrate &lt;&gt; Dehydrate Tweets -&gt; Tweet IDs twarc dehydrate tweets.jsonl &gt; tweet_ids.txt Challenge Collect tweets contain some key words of your choice using twarc search and save them as tweets.jsonl. Using less command in the terminal, inspect twarc.log. Using less command in the terminal, inspect tweets.json. 6.5.3 Parsing JSON # Install packages if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load(tidyverse, # tidyverse pkgs including purrr furrr, # parallel processing tictoc, # performance test tcltk, # GUI for choosing a dir path tidyjson) # tidying JSON files ## Install the current development version from GitHub devtools::install_github(&quot;jaeyk/tidytweetjson&quot;, dependencies = TRUE) ## Skipping install of &#39;tidytweetjson&#39; from a github remote, the SHA1 (3ab642b2) has not changed since last install. ## Use `force = TRUE` to force installation library(tidytweetjson) ## Warning: replacing previous import &#39;maps::map&#39; by &#39;purrr::map&#39; when loading ## &#39;tidytweetjson&#39; 6.5.3.1 Objectives Learning chunk and pull strategy Learning how tidyjson works Learning how to apply tidyjson to tweets 6.5.3.2 Chunk and Pull 6.5.3.2.1 Problem What if the size of the Twitter data you downloaded is too big (e.g., &gt;10GB) to do complex wrangling in R? 6.5.3.2.2 Solution Chunk and Pull. From Studio. Step1: Split the large JSON file in small chunks. #Divide the JSON file by 100 lines (tweets) # Linux and Windows (in Bash) $ split -100 search.jsonl # macOS $ gsplit -100 search.jsonl After that, you will see several files appeared in the directory. Each of these files should have 100 tweets or fewer. All of these file names should start with “x”, as in “xaa”. Step 2: Apply the parsing function to each chunk and pull all of these chunks together. # You need to choose a Tweet JSON file filepath &lt;- file.choose() # Assign the parsed result to the `df` object # 11.28 sec elapsed to parse 17,928 tweets tic() df &lt;- jsonl_to_df(filepath) toc() # Setup n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers # You need to designate a directory path where you saved the list of JSON files. # 9.385 sec elapsed to parse 17,928 tweets dirpath &lt;- tcltk::tk_choose.dir() tic() df_all &lt;- tidytweetjson::jsonl_to_df_all(dirpath) toc() 6.5.3.2.3 tidyjson The tidyjson package helps to use tidyverse framework to JSON data. toy example # JSON collection; nested structure + keys and values worldbank[1] ## [1] &quot;{\\&quot;_id\\&quot;:{\\&quot;$oid\\&quot;:\\&quot;52b213b38594d8a2be17c780\\&quot;},\\&quot;boardapprovaldate\\&quot;:\\&quot;2013-11-12T00:00:00Z\\&quot;,\\&quot;closingdate\\&quot;:\\&quot;2018-07-07T00:00:00Z\\&quot;,\\&quot;countryshortname\\&quot;:\\&quot;Ethiopia\\&quot;,\\&quot;majorsector_percent\\&quot;:[{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:46},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:26},{\\&quot;Name\\&quot;:\\&quot;Public Administration, Law, and Justice\\&quot;,\\&quot;Percent\\&quot;:16},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:12}],\\&quot;project_name\\&quot;:\\&quot;Ethiopia General Education Quality Improvement Project II\\&quot;,\\&quot;regionname\\&quot;:\\&quot;Africa\\&quot;,\\&quot;totalamt\\&quot;:130000000}&quot; # Check out keys (objects) worldbank %&gt;% as.tbl_json() %&gt;% gather_object() %&gt;% filter(document.id == 1) ## # A tbl_json: 8 x 3 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id name ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 &quot;{\\&quot;$oid\\&quot;:\\&quot;52b213...&quot; 1 _id ## 2 &quot;\\&quot;2013-11-12T00:...&quot; 1 boardapprovaldate ## 3 &quot;\\&quot;2018-07-07T00:...&quot; 1 closingdate ## 4 &quot;\\&quot;Ethiopia\\&quot;&quot; 1 countryshortname ## 5 &quot;[{\\&quot;Name\\&quot;:\\&quot;Educa...&quot; 1 majorsector_percent ## 6 &quot;\\&quot;Ethiopia Gener...&quot; 1 project_name ## 7 &quot;\\&quot;Africa\\&quot;&quot; 1 regionname ## 8 &quot;130000000&quot; 1 totalamt # Get the values associated with the keys worldbank %&gt;% as.tbl_json() %&gt;% # Turn JSON into tbl_json object enter_object(&quot;project_name&quot;) %&gt;% # Enter the objects append_values_string() %&gt;% # Append the values as_tibble() # To reduce the size of the file ## # A tibble: 500 x 2 ## document.id string ## &lt;int&gt; &lt;chr&gt; ## 1 1 Ethiopia General Education Quality Improvement Project II ## 2 2 TN: DTF Social Protection Reforms Support ## 3 3 Tuvalu Aviation Investment Project - Additional Financing ## 4 4 Gov&#39;t and Civil Society Organization Partnership ## 5 5 Second Private Sector Competitiveness and Economic Diversificati… ## 6 6 Additional Financing for Cash Transfers for Orphans and Vulnerab… ## 7 7 National Highways Interconnectivity Improvement Project ## 8 8 China Renewable Energy Scale-Up Program Phase II ## 9 9 Rajasthan Road Sector Modernization Project ## 10 10 MA Accountability and Transparency DPL ## # … with 490 more rows The following example draws on my tidytweetjson R package. The package applies tidyjson to Tweets. 6.5.3.2.3.1 Individual file jsonl_to_df &lt;- function(file_path){ # Save file name file_name &lt;- strsplit(x = file_path, split = &quot;[/]&quot;) file_name &lt;- file_name[[1]][length(file_name[[1]])] # Import a Tweet JSON file listed &lt;- read_json(file_path, format = c(&quot;jsonl&quot;)) # IDs of the tweets with country codes ccodes &lt;- listed %&gt;% enter_object(&quot;place&quot;) %&gt;% enter_object(&quot;country_code&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(&quot;country_code&quot; = &quot;string&quot;) # IDs of the tweets with location locations &lt;- listed %&gt;% enter_object(&quot;user&quot;) %&gt;% enter_object(&quot;location&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(location = &quot;string&quot;) # Extract other key elements from the JSON file df &lt;- listed %&gt;% spread_values( id = jnumber(&quot;id&quot;), created_at = jstring(&quot;created_at&quot;), full_text = jstring(&quot;full_text&quot;), retweet_count = jnumber(&quot;retweet_count&quot;), favorite_count = jnumber(&quot;favorite_count&quot;), user.followers_count = jnumber(&quot;user.followers_count&quot;), user.friends_count = jnumber(&quot;user.friends_count&quot;) ) %&gt;% as_tibble message(paste(&quot;Parsing&quot;, file_name, &quot;done.&quot;)) # Full join outcome &lt;- full_join(ccodes, df) %&gt;% full_join(locations) # Or you can write this way: outcome &lt;- reduce(list(df, ccodes, locations), full_join) # Select outcome %&gt;% select(-c(&quot;document.id&quot;))} 6.5.3.2.3.2 Many files Set up parallel processing. n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. ## system ## 7 plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers ## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled ## in future (&gt;= 1.13.0) when running R from RStudio, because it is ## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall ## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to ## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more details, ## how to control forked processing or not, and how to silence this warning in ## future R sessions, see ?future::supportsMulticore Parsing in parallel. Review There are at least three ways you can use function + purrr::map(). squared &lt;- function(x){ x*2 } # Named function map(1:3, squared) # Anonymous function map(1:3, function(x){ x *2 }) # Using formula; ~ = formula, .x = input map(1:3,~.x*2) # Create a list of file paths filename &lt;- list.files(dir_path, pattern = &#39;^x&#39;, full.names = TRUE) df &lt;- filename %&gt;% # Apply jsonl_to_df function to items on the list future_map(~jsonl_to_df(.)) %&gt;% # Full join the list of dataframes reduce(full_join, by = c(&quot;id&quot;, &quot;location&quot;, &quot;country_code&quot;, &quot;created_at&quot;, &quot;full_text&quot;, &quot;retweet_count&quot;, &quot;favorite_count&quot;, &quot;user.followers_count&quot;, &quot;user.friends_count&quot;)) # Output df "],
["machine-learning.html", "Chapter 7 High-dimensional data 7.1 Overview 7.2 Dataset 7.3 Workflow 7.4 Pre-processing 7.5 Supervised learning 7.6 Unsupervised learning 7.7 Bias and fairness in machine learning 7.8 Resources", " Chapter 7 High-dimensional data 7.1 Overview The rise of high-dimensional data. The new data frontiers in social sciences—text (Gentzkow et al. 2019; Grimmer and Stewart 2013) and and image (Joo and Steinert-Threlkeld 2018)—are all high-dimensional data. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. “High-dimensional methods and inference on structural and treatment effects.” Journal of Economic Perspectives 28, no. 2 (2014): 29-50. The rise of new approach: statistics + computer science = machine learning Statistical inference \\(y\\) &lt;- some probability models (e.g., linear regression, logistic regression) &lt;- \\(x\\) \\(y\\) = \\(X\\beta\\) + \\(\\epsilon\\) The goal is to estimate \\(\\beta\\) Machine learning \\(y\\) &lt;- unknown &lt;- \\(x\\) \\(y\\) &lt;-&gt; decision trees, neutral nets &lt;-&gt; \\(x\\) For the main idea behind prediction modeling, see Breiman, Leo (Berkeley stat faculty who passed away in 2005). “Statistical modeling: The two cultures (with comments and a rejoinder by the author).” Statistical science 16, no. 3 (2001): 199-231. “The problem is to find an algorithm \\(f(x)\\) such that for future \\(x\\) in a test set, \\(f(x)\\) will be a good predictor of \\(y\\).” “There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.” Algorithmic models, both in theory and practice, has developed rapidly in fields of outside statistics. It can be used on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. - Leo Breiman How ML differs from econometrics? A review by Athey, Susan, and Guido W. Imbens. “Machine learning methods that economists should know about.” Annual Review of Economics 11 (2019): 685-725. Stat: Specifying a target (i.e., an estimand) Fitting a model to data using an objective function (e.g., the sum of squared errors) Reporting point estimates (effect size) and standard errors (uncertainty) Validation by yes-no using goodness-of-fit tests and residual examination ML: Developing algorithms (estimating f(x)) Prediction power not structural/causal parameters Basically, high-dimensional data statistics (N &lt; P) The major problem is to avoid “the curse of dimensionality” (too many features - &gt; overfitting) Validation: out-of-sample comparisons (cross-validation) not in-sample goodness-of-fit measures So, it’s curve-fitting but the primary focus is unseen (test data) not seen data (training data) A quick review on ML lingos for those trained in econometrics Sample to estimate parameters = Training sample Estimating the model = Being trained Regressors, covariates, or predictors = Features Regression parameters = weights Prediction problems = Supervised (some \\(y\\) are known) + Unsupervised (\\(y\\) unknown) How to teach machines. Based on vas3k blog. Many images in this chapter come from vas3k blog. The main types of machine learning. Based on vas3k blog The map of the machine learning universe. Based on vas3k blog Classical machine learning. Based on vas3k blog 7.2 Dataset Heart disease data from UCI One of the popular datasets used in machine learning competitions # Load packages ## CRAN packages pacman::p_load(here, tidyverse, tidymodels, doParallel, # parallel processing patchwork) # arranging ggplots ## Jae&#39;s custom functions source(here(&quot;functions&quot;, &quot;ml_utils.r&quot;)) # Import the dataset data_original &lt;- read_csv(here(&quot;data&quot;, &quot;heart.csv&quot;)) ## Parsed with column specification: ## cols( ## age = col_double(), ## sex = col_double(), ## cp = col_double(), ## trestbps = col_double(), ## chol = col_double(), ## fbs = col_double(), ## restecg = col_double(), ## thalach = col_double(), ## exang = col_double(), ## oldpeak = col_double(), ## slope = col_double(), ## ca = col_double(), ## thal = col_double(), ## target = col_double() ## ) glimpse(data_original) ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, … ## $ sex &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, … ## $ cp &lt;dbl&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, … ## $ trestbps &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, … ## $ chol &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, … ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ restecg &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ thalach &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, … ## $ exang &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ oldpeak &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, … ## $ slope &lt;dbl&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, … ## $ ca &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, … ## $ thal &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ target &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … # Createa a copy data &lt;- data_original theme_set(theme_minimal()) For more information on the Iowa housing data, read Cook (2011). This is one of the famous datastets used in many prediction modeling competitions. 7.3 Workflow Preprocessing Model building Model fitting Model evaluation Model tuning Prediction ## Tidymodels Like tidyverse, tidymodels is a collection of packages. rsample: for data splitting recipes: for pre-processing parsnip: for model building tune: parameter tuning yardstick: for model evaluations workflows: for bundling a pieplne that bundles together pre-processing, modeling, and post-processing requests Why taking a tidyverse approach to machine learning? Benefits Readable code Reusable data structures Extendable code Tidymodels. From RStudio. tidymodels are an integrated, modular, extensible set of packages that implement a framework that facilitates creating predicative stochastic models. - Joseph Rickert@RStudio Currently, 238 models are available The following materials are based on the machine learning with tidymodels workshop I developed for D-Lab. The original workshop was designed by Chris Kennedy and [Evan Muzzall](https://dlab.berkeley.edu/people/evan-muzzall. 7.4 Pre-processing recipes: for pre-processing textrecipes for text pre-processing Step 1: recipe() defines target and predictor variables (ingredients). Step 2: step_*() defines preprocessing steps to be taken (recipe). The list of the preprocessing steps draws on the vignette of the parsnip package. dummy: Also called one-hot encoding zero variance: Removing columns (or features) with a single unique value impute: Imputing missing values decorrelate: Mitigating correlated predictors (e.g., principal component analysis) normalize: Centering and/or scaling predictors (e.g., log scaling) transform: Making predictors symmetric Step 3: prep() prepares a dataset to base each step on. Step 4: bake() applies the pre-processing steps to your datasets. In this course, we focus on two preprocessing tasks. One-hot encoding (creating dummy/indicator variables) # Turn selected numeric variables into factor variables data &lt;- data %&gt;% dplyr::mutate(across(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;), as.factor)) glimpse(data) ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, … ## $ sex &lt;fct&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, … ## $ cp &lt;fct&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, … ## $ trestbps &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, … ## $ chol &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, … ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ restecg &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ thalach &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, … ## $ exang &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ oldpeak &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, … ## $ slope &lt;fct&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, … ## $ ca &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, … ## $ thal &lt;fct&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ target &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Imputation # Check missing values map_df(data, ~ is.na(.) %&gt;% sum()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 0 0 0 0 ## # … with 3 more variables: ca &lt;int&gt;, thal &lt;int&gt;, target &lt;int&gt; # Add missing values data$oldpeak[sample(seq(data), size = 10)] &lt;- NA # Check missing values # Check the number of missing values data %&gt;% map_df(~is.na(.) %&gt;% sum()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 0 0 10 0 ## # … with 3 more variables: ca &lt;int&gt;, thal &lt;int&gt;, target &lt;int&gt; # Check the rate of missing values data %&gt;% map_df(~is.na(.) %&gt;% mean()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 0 0 0.0330 0 ## # … with 3 more variables: ca &lt;dbl&gt;, thal &lt;dbl&gt;, target &lt;dbl&gt; 7.4.1 Regression setup 7.4.1.1 Outcome variable # Continuous variable data$age %&gt;% class() ## [1] &quot;numeric&quot; 7.4.1.2 Data splitting using random sampling # for reproducibility set.seed(1234) # split split_reg &lt;- initial_split(data, prop = 0.7) # training set raw_train_x_reg &lt;- training(split_reg) # test set raw_test_x_reg &lt;- testing(split_reg) 7.4.1.3 recipe # Regression recipe rec_reg &lt;- raw_train_x_reg %&gt;% # Define the outcome variable recipe(age ~ .) %&gt;% # Median impute oldpeak column step_medianimpute(oldpeak) %&gt;% # Expand &quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, and &quot;thal&quot; features out into dummy variables (indicators). step_dummy(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;)) # Prepare a dataset to base each step on prep_reg &lt;- rec_reg %&gt;% prep(retain = TRUE) # x features train_x_reg &lt;- juice(prep_reg, all_predictors()) test_x_reg &lt;- bake(object = prep_reg, new_data = raw_test_x_reg, all_predictors()) # y variables train_y_reg &lt;- juice(prep_reg, all_outcomes())$age %&gt;% as.numeric() test_y_reg &lt;- bake(prep_reg, raw_test_x_reg, all_outcomes())$age %&gt;% as.numeric() # Checks names(train_x_reg) # Make sure there&#39;s no age variable! ## [1] &quot;trestbps&quot; &quot;chol&quot; &quot;fbs&quot; &quot;restecg&quot; &quot;thalach&quot; &quot;exang&quot; ## [7] &quot;oldpeak&quot; &quot;target&quot; &quot;sex_X1&quot; &quot;ca_X1&quot; &quot;ca_X2&quot; &quot;ca_X3&quot; ## [13] &quot;ca_X4&quot; &quot;cp_X1&quot; &quot;cp_X2&quot; &quot;cp_X3&quot; &quot;slope_X1&quot; &quot;slope_X2&quot; ## [19] &quot;thal_X1&quot; &quot;thal_X2&quot; &quot;thal_X3&quot; class(train_y_reg) # Make sure this is a continuous variable! ## [1] &quot;numeric&quot; Note that other imputation methods are also available. grep(&quot;impute&quot;, ls(&quot;package:recipes&quot;), value = TRUE) ## [1] &quot;step_bagimpute&quot; &quot;step_knnimpute&quot; ## [3] &quot;step_lowerimpute&quot; &quot;step_meanimpute&quot; ## [5] &quot;step_medianimpute&quot; &quot;step_modeimpute&quot; ## [7] &quot;step_rollimpute&quot; &quot;tunable.step_bagimpute&quot; ## [9] &quot;tunable.step_knnimpute&quot; &quot;tunable.step_meanimpute&quot; ## [11] &quot;tunable.step_rollimpute&quot; You can also create your own step_ functions. For more information, see tidymodels.org. 7.4.2 Classification setup 7.4.2.1 Outcome variable data$target %&gt;% class() ## [1] &quot;numeric&quot; data$target &lt;- as.factor(data$target) data$target %&gt;% class() ## [1] &quot;factor&quot; 7.4.2.2 Data splitting using stratified random sampling # split split_class &lt;- initial_split(data %&gt;% mutate(target = as.factor(target)), prop = 0.7, strata = target) # training set raw_train_x_class &lt;- training(split_class) # testing set raw_test_x_class &lt;- testing(split_class) 7.4.2.3 recipe # Classification recipe rec_class &lt;- raw_train_x_class %&gt;% # Define the outcome variable recipe(target ~ .) %&gt;% # Median impute oldpeak column step_medianimpute(oldpeak) %&gt;% # Expand &quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, and &quot;thal&quot; features out into dummy variables (indicators). step_normalize(age) %&gt;% step_dummy(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;)) # Prepare a dataset to base each step on prep_class &lt;- rec_class %&gt;%prep(retain = TRUE) # x features train_x_class &lt;- juice(prep_class, all_predictors()) test_x_class &lt;- bake(prep_class, raw_test_x_class, all_predictors()) # y variables train_y_class &lt;- juice(prep_class, all_outcomes())$target %&gt;% as.factor() test_y_class &lt;- bake(prep_class, raw_test_x_class, all_outcomes())$target %&gt;% as.factor() # Checks names(train_x_class) # Make sure there&#39;s no target variable! ## [1] &quot;age&quot; &quot;trestbps&quot; &quot;chol&quot; &quot;fbs&quot; &quot;restecg&quot; &quot;thalach&quot; ## [7] &quot;exang&quot; &quot;oldpeak&quot; &quot;sex_X1&quot; &quot;ca_X1&quot; &quot;ca_X2&quot; &quot;ca_X3&quot; ## [13] &quot;ca_X4&quot; &quot;cp_X1&quot; &quot;cp_X2&quot; &quot;cp_X3&quot; &quot;slope_X1&quot; &quot;slope_X2&quot; ## [19] &quot;thal_X1&quot; &quot;thal_X2&quot; &quot;thal_X3&quot; class(train_y_class) # Make sure this is a factor variable! ## [1] &quot;factor&quot; 7.5 Supervised learning x -&gt; f - &gt; y (defined) 7.5.1 OLS and Lasso 7.5.1.1 parsnip Build models (parsnip) Specify a model Specify an engine Specify a mode # OLS spec ols_spec &lt;- linear_reg() %&gt;% # Specify a model set_engine(&quot;lm&quot;) %&gt;% # Specify an engine: lm, glmnet, stan, keras, spark set_mode(&quot;regression&quot;) # Declare a mode: regression or classification # Lasso spec lasso_spec &lt;- linear_reg(penalty = 0.1, # tuning parameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) # If you don&#39;t understand parsnip arguments lasso_spec %&gt;% translate() # See the documentation ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 1 ## ## Computational engine: glmnet ## ## Model fit template: ## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), ## alpha = 1, family = &quot;gaussian&quot;) Fit models ols_fit &lt;- ols_spec %&gt;% fit_xy(x = train_x_reg, y= train_y_reg) # fit(train_y_reg ~ ., train_x_reg) # When you data are not preprocessed lasso_fit &lt;- lasso_spec %&gt;% fit_xy(x = train_x_reg, y= train_y_reg) 7.5.1.2 yardstick Visualize model fits map2(list(ols_fit, lasso_fit), c(&quot;OLS&quot;, &quot;Lasso&quot;), visualize_fit) ## [[1]] ## ## [[2]] # Define performance metrics metrics &lt;- yardstick::metric_set(rmse, mae, rsq) # Evaluate many models evals &lt;- purrr::map(list(ols_fit, lasso_fit), evaluate_reg) %&gt;% reduce(bind_rows) %&gt;% mutate(type = rep(c(&quot;OLS&quot;, &quot;Lasso&quot;), each = 3)) # Visualize the test results evals %&gt;% ggplot(aes(x = fct_reorder(type, .estimate), y = .estimate)) + geom_point() + labs(x = &quot;Model&quot;, y = &quot;Estimate&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free_y&quot;) - For more information, read Tidy Modeling with R by Max Kuhn and Julia Silge. 7.5.1.3 tune 7.5.1.3.1 tune ingredients # tune() = placeholder tune_spec &lt;- linear_reg(penalty = tune(), # tuning parameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) tune_spec ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet # penalty() searches 50 possible combinations lambda_grid &lt;- grid_regular(penalty(), levels = 50) # 10-fold cross-validation set.seed(1234) # for reproducibility rec_folds &lt;- vfold_cv(train_x_reg %&gt;% bind_cols(tibble(age = train_y_reg))) 7.5.1.3.2 Add these elements to a workflow # Workflow rec_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_formula(age~.) # Tuning results rec_res &lt;- rec_wf %&gt;% tune_grid( resamples = rec_folds, grid = lambda_grid ) 7.5.1.3.3 Visualize # Visualize rec_res %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, col = .metric)) + geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err ), alpha = 0.3 ) + geom_line(size = 2) + scale_x_log10() + labs(x = &quot;log(lambda)&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free&quot;, nrow = 2) + theme(legend.position = &quot;none&quot;) 7.5.1.3.4 Select top_rmse &lt;- show_best(rec_res, metric = &quot;rmse&quot;) best_rmse &lt;- select_best(rec_res, metric = &quot;rmse&quot;) best_rmse ## # A tibble: 1 x 2 ## penalty .config ## &lt;dbl&gt; &lt;chr&gt; ## 1 0.244 Model47 glue(&#39;The RMSE of the intiail model is {evals %&gt;% filter(type == &quot;Lasso&quot;, .metric == &quot;rmse&quot;) %&gt;% select(.estimate) %&gt;% round(2)}&#39;) ## The RMSE of the intiail model is ## 7.86 glue(&#39;The RMSE of the tuned model is {rec_res %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% arrange(mean) %&gt;% dplyr::slice(1) %&gt;% select(mean) %&gt;% round(2)}&#39;) ## The RMSE of the tuned model is 7.71 Finalize your workflow and visualize variable importance finalize_lasso &lt;- rec_wf %&gt;% finalize_workflow(best_rmse) finalize_lasso %&gt;% fit(train_x_reg %&gt;% bind_cols(tibble(age = train_y_reg))) %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.5.1.3.5 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_lasso %&gt;% fit(test_x_reg %&gt;% bind_cols(tibble(age = test_y_reg))) evaluate_reg(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 7.17 ## 2 mae standard 5.93 ## 3 rsq standard 0.405 7.5.2 Decision tree 7.5.2.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow tree_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec tree_spec &lt;- decision_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters cost_complexity = NULL, tree_depth = NULL) %&gt;% set_engine(&quot;rpart&quot;) # rpart, c5.0, spark tree_wf &lt;- tree_wf %&gt;% add_model(tree_spec) Fit a model tree_fit &lt;- tree_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 7.5.2.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) From wikipedia To learn more about other metrics, check out the yardstick package references. # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) # Visualize tree_fit_viz_metr &lt;- visualize_class_eval(tree_fit) tree_fit_viz_metr tree_fit_viz_mat &lt;- visualize_class_conf(tree_fit) tree_fit_viz_mat 7.5.2.3 tune 7.5.2.3.1 tune ingredients complexity parameter: a high CP means a simple decision tree with few splits. tree_depth tune_spec &lt;- decision_tree( cost_complexity = tune(), tree_depth = tune(), mode = &quot;classification&quot; ) %&gt;% set_engine(&quot;rpart&quot;) tree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = 5) # 2 parameters -&gt; 5*5 = 25 combinations tree_grid %&gt;% count(tree_depth) ## # A tibble: 5 x 2 ## tree_depth n ## &lt;int&gt; &lt;int&gt; ## 1 1 5 ## 2 4 5 ## 3 8 5 ## 4 11 5 ## 5 15 5 # 10-fold cross-validation set.seed(1234) # for reproducibility tree_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.5.2.3.2 Add these elements to a workflow # Update workflow tree_wf &lt;- tree_wf %&gt;% update_model(tune_spec) # Determine the number of cores no_cores &lt;- detectCores() - 1 # Initiate cl &lt;- makeCluster(no_cores) registerDoParallel(cl) # Tuning results tree_res &lt;- tree_wf %&gt;% tune_grid( resamples = tree_folds, grid = tree_grid, metrics = metrics ) 7.5.2.3.3 Visualize The following plot draws on the vignette of the tidymodels package. tree_res %&gt;% collect_metrics() %&gt;% mutate(tree_depth = factor(tree_depth)) %&gt;% ggplot(aes(cost_complexity, mean, col = .metric)) + geom_point(size = 3) + # Subplots facet_wrap(~ tree_depth, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;Cost complexity&quot;, col = &quot;Tree depth&quot;, y = NULL) + coord_flip() ##### Select # Optimal parameter best_tree &lt;- select_best(tree_res, &quot;recall&quot;) # Add the parameter to the workflow finalize_tree &lt;- tree_wf %&gt;% finalize_workflow(best_tree) tree_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (tree_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (tree_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance tree_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.5.2.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.744 ## 2 precision binary 0.705 ## 3 recall binary 0.756 7.5.3 Random forest 7.5.3.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow rand_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec rand_spec &lt;- rand_forest( # Mode mode = &quot;classification&quot;, # Tuning parameters mtry = NULL, # The number of predictors to available for splitting at each node min_n = NULL, # The minimum number of data points needed to keep splitting nodes trees = 500) %&gt;% # The number of trees set_engine(&quot;ranger&quot;, # We want the importance of predictors to be assessed. seed = 1234, importance = &quot;permutation&quot;) rand_wf &lt;- rand_wf %&gt;% add_model(rand_spec) Fit a model rand_fit &lt;- rand_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 7.5.3.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) rand_fit_viz_metr &lt;- visualize_class_eval(rand_fit) rand_fit_viz_metr Visualize the confusion matrix. rand_fit_viz_mat &lt;- visualize_class_conf(rand_fit) rand_fit_viz_mat 7.5.3.3 tune 7.5.3.3.1 tune ingredients We focus on the following two parameters: mtry: The number of predictors to available for splitting at each node. min_n: The minimum number of data points needed to keep splitting nodes. tune_spec &lt;- rand_forest( mode = &quot;classification&quot;, # Tuning parameters mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, seed = 1234, importance = &quot;permutation&quot;) rand_grid &lt;- grid_regular(mtry(range = c(1, 10)), min_n(range = c(2, 10)), levels = 5) rand_grid %&gt;% count(min_n) ## # A tibble: 5 x 2 ## min_n n ## &lt;int&gt; &lt;int&gt; ## 1 2 5 ## 2 4 5 ## 3 6 5 ## 4 8 5 ## 5 10 5 # 10-fold cross-validation set.seed(1234) # for reproducibility rand_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.5.3.3.2 Add these elements to a workflow # Update workflow rand_wf &lt;- rand_wf %&gt;% update_model(tune_spec) # Tuning results rand_res &lt;- rand_wf %&gt;% tune_grid( resamples = rand_folds, grid = rand_grid, metrics = metrics ) 7.5.3.3.3 Visualize rand_res %&gt;% collect_metrics() %&gt;% mutate(min_n = factor(min_n)) %&gt;% ggplot(aes(mtry, mean, color = min_n)) + # Line + Point plot geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + # Subplots facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;The number of predictors to be sampled&quot;, col = &quot;The minimum number of data points needed for splitting&quot;, y = NULL) + theme(legend.position=&quot;bottom&quot;) # Optimal parameter best_tree &lt;- select_best(rand_res, &quot;accuracy&quot;) best_tree ## # A tibble: 1 x 3 ## mtry min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 2 Model01 # Add the parameter to the workflow finalize_tree &lt;- rand_wf %&gt;% finalize_workflow(best_tree) rand_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (rand_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(rand_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (rand_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(rand_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance rand_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.5.3.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.933 ## 2 precision binary 0.973 ## 3 recall binary 0.878 7.5.4 XGboost 7.5.4.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow xg_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec xg_spec &lt;- boost_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters # The number of trees to fit, aka boosting iterations trees = c(100, 300, 500, 700, 900), # The depth of the decision tree (how many levels of splits). tree_depth = c(1, 6), # Learning rate: lower means the ensemble will adapt more slowly. learn_rate = c(0.0001, 0.01, 0.2), # Stop splitting a tree if we only have this many obs in a tree node. min_n = 10L ) %&gt;% set_engine(&quot;xgboost&quot;) xg_wf &lt;- xg_wf %&gt;% add_model(xg_spec) Fit a model xg_fit &lt;- xg_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) ## Warning in begin_iteration:end_iteration: numerical expression has 5 elements: ## only the first used 7.5.4.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) metrics &lt;- metric_set(yardstick::accuracy, yardstick::precision, yardstick::recall) evaluate_class(xg_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.733 ## 2 precision binary 0.730 ## 3 recall binary 0.659 xg_fit_viz_metr &lt;- visualize_class_eval(xg_fit) xg_fit_viz_metr Visualize the confusion matrix. xg_fit_viz_mat &lt;- visualize_class_conf(xg_fit) xg_fit_viz_mat 7.5.4.3 tune 7.5.4.3.1 tune ingredients We focus on the following parameters: trees, tree_depth, learn_rate, min_n, mtry, loss_reduction, and sample_size tune_spec &lt;- xg_spec &lt;- boost_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters # The number of trees to fit, aka boosting iterations trees = tune(), # The depth of the decision tree (how many levels of splits). tree_depth = tune(), # Learning rate: lower means the ensemble will adapt more slowly. learn_rate = tune(), # Stop splitting a tree if we only have this many obs in a tree node. min_n = tune(), loss_reduction = tune(), # The number of randomly selected parameters mtry = tune(), # The size of the data set used for modeling within an iteration sample_size = tune() ) %&gt;% set_engine(&quot;xgboost&quot;) # Space-filling parameter grids xg_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), learn_rate(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), train_x_class), size = 30 ) # 10-fold cross-validation set.seed(1234) # for reproducibility xg_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.5.4.3.2 Add these elements to a workflow # Update workflow xg_wf &lt;- xg_wf %&gt;% update_model(tune_spec) # Tuning results xg_res &lt;- xg_wf %&gt;% tune_grid( resamples = xg_folds, grid = xg_grid, control = control_grid(save_pred = TRUE) ) 7.5.4.3.3 Visualize xg_res %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;roc_auc&quot;) %&gt;% pivot_longer(mtry:sample_size, values_to = &quot;value&quot;, names_to = &quot;parameter&quot;) %&gt;% ggplot(aes(x = value, y = mean, color = parameter)) + geom_point(alpha = 0.8, show.legend = FALSE) + facet_wrap(~parameter, scales = &quot;free_x&quot;) + labs(y = &quot;AUC&quot;, x = NULL) # Optimal parameter best_xg &lt;- select_best(xg_res, &quot;roc_auc&quot;) best_xg ## # A tibble: 1 x 8 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 11 326 3 13 0.0176 0.00000254 0.544 Model27 # Add the parameter to the workflow finalize_xg &lt;- xg_wf %&gt;% finalize_workflow(best_xg) xg_fit_tuned &lt;- finalize_xg %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (xg_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(xg_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (xg_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(xg_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance xg_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. 7.5.4.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_xg %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.833 ## 2 precision binary 0.810 ## 3 recall binary 0.829 7.5.5 Applications 7.5.5.1 Bandit algorithm (optimizing an experiment) 7.5.5.2 Causal forest (estimating heterogeneous treatment effect) 7.6 Unsupervised learning x -&gt; f - &gt; y (not defined) 7.6.1 Dimension reduction Projecting 2D-data to a line (PCA). From vas3k.com 7.6.1.1 Correlation analysis Notice some problems? NAs Scaling issues data_original %&gt;% corrr::correlate() ## ## Correlation method: &#39;pearson&#39; ## Missing treated using: &#39;pairwise.complete.obs&#39; ## # A tibble: 14 x 15 ## rowname age sex cp trestbps chol fbs restecg thalach ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age NA -0.0984 -0.0687 0.279 0.214 0.121 -0.116 -0.399 ## 2 sex -0.0984 NA -0.0494 -0.0568 -0.198 0.0450 -0.0582 -0.0440 ## 3 cp -0.0687 -0.0494 NA 0.0476 -0.0769 0.0944 0.0444 0.296 ## 4 trestb… 0.279 -0.0568 0.0476 NA 0.123 0.178 -0.114 -0.0467 ## 5 chol 0.214 -0.198 -0.0769 0.123 NA 0.0133 -0.151 -0.00994 ## 6 fbs 0.121 0.0450 0.0944 0.178 0.0133 NA -0.0842 -0.00857 ## 7 restecg -0.116 -0.0582 0.0444 -0.114 -0.151 -0.0842 NA 0.0441 ## 8 thalach -0.399 -0.0440 0.296 -0.0467 -0.00994 -0.00857 0.0441 NA ## 9 exang 0.0968 0.142 -0.394 0.0676 0.0670 0.0257 -0.0707 -0.379 ## 10 oldpeak 0.210 0.0961 -0.149 0.193 0.0540 0.00575 -0.0588 -0.344 ## 11 slope -0.169 -0.0307 0.120 -0.121 -0.00404 -0.0599 0.0930 0.387 ## 12 ca 0.276 0.118 -0.181 0.101 0.0705 0.138 -0.0720 -0.213 ## 13 thal 0.0680 0.210 -0.162 0.0622 0.0988 -0.0320 -0.0120 -0.0964 ## 14 target -0.225 -0.281 0.434 -0.145 -0.0852 -0.0280 0.137 0.422 ## # … with 6 more variables: exang &lt;dbl&gt;, oldpeak &lt;dbl&gt;, slope &lt;dbl&gt;, ca &lt;dbl&gt;, ## # thal &lt;dbl&gt;, target &lt;dbl&gt; 7.6.1.2 Preprocessing recipe is essential for preprocesssing multiple features at once. pca_recipe &lt;- recipe(~., data = data_original) %&gt;% # Imputing NAs using mean step_meanimpute(all_predictors()) %&gt;% # Normalize some numeric variables step_normalize(c(&quot;age&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;)) 7.6.1.3 PCA analysis pca_res &lt;- pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() pca_res %&gt;% tidy(id = &quot;pca&quot;) ## # A tibble: 196 x 4 ## terms value component id ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 age -0.00101 PC1 pca ## 2 sex 0.216 PC1 pca ## 3 cp 0.321 PC1 pca ## 4 trestbps 0.00118 PC1 pca ## 5 chol -0.000292 PC1 pca ## 6 fbs 0.0468 PC1 pca ## 7 restecg 0.166 PC1 pca ## 8 thalach 0.0137 PC1 pca ## 9 exang 0.0962 PC1 pca ## 10 oldpeak -0.00863 PC1 pca ## # … with 186 more rows 7.6.1.4 Screeplot pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() %&gt;% tidy(id = &quot;pca&quot;, type = &quot;variance&quot;) %&gt;% filter(terms == &quot;percent variance&quot;) %&gt;% ggplot(aes(x = component, y = value)) + geom_col() + labs(x = &quot;PCAs of heart disease&quot;, y = &quot;% of variance&quot;, title = &quot;Scree plot&quot;) 7.6.1.5 View factor loadings pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() %&gt;% tidy(id = &quot;pca&quot;) %&gt;% filter(component %in% c(&quot;PC1&quot;, &quot;PC2&quot;)) %&gt;% ggplot(aes(x = fct_reorder(terms, value), y = value, fill = component)) + geom_col(position = &quot;dodge&quot;) + coord_flip() + labs(x = &quot;Terms&quot;, y = &quot;Contribtutions&quot;, fill = &quot;PCAs&quot;) 7.6.2 Clustering 7.6.2.1 Topic modeling 7.7 Bias and fairness in machine learning 7.8 Resources 7.8.1 Books An Introduction to Statistical Learning - with Applications in R (2013) by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. Amazon or free PDF. Hands-On Machine Learning with R (2020) by Bradley Boehmke &amp; Brandon Greenwell. CRC Press or Amazon Applied Predictive Modeling (2013) by Max Kuhn and Kjell Johnson. Springer: New York. Amazon Feature Engineering and Selection: A Practical Approach for Predictive Models (2019) by Kjell Johnson and Max Kuhn. Taylor &amp; Francis. Amazon or free HTML. Tidy Modeling with R (2020) by Max Kuhn and Julia Silge (work-in-progress) 7.8.2 Lecture slides An introduction to supervised and unsupervised learning (2015) by Susan Athey and Guido Imbens “Introduction Machine Learning with the Tidyverse” by Alison Hill 7.8.3 Blog posts “Using the recipes package for easy pre-processing” by Rebecca Barter "],
["big-data.html", "Chapter 8 Big data 8.1 Database and SQL 8.2 Motivation", " Chapter 8 Big data 8.1 Database and SQL Special thanks to Jacob Coblnetz ((???)) for sharing his slides on the SQL workshop used at MIT. 8.2 Motivation Big data problem: data is too big to fit into memory (=local environment). R reads data into random-access memory (RAM) at once and this object lives in memory entirely. So, object &gt; memory will crash R. So, the key to deal with big data in R is reducing the size of data you want to bring into it. Techniques to deal with big data Medium sized file (1-2 GB) Try to reduce the size of the file using slicing and dicing Tools: R:data.table::fread(file path, select = c(\"column 1\", \"column 2\")). This command imports data faster than read.csv() does. Command line: csvkit - a suite of command-line tools to and working with CSV Large file (&gt; 2-10 GB) Put the data into a database and ACCESS it Explore the data and pull the objects of interest Types of databases Relational database = a collection of tables (fixed columns and rows): SQL is a staple tool to define and query (focus of the workshop today) this type of database Non-relational database = a collection of documents (MongoDB), key-values (Redis and DyanoDB), wide-column stores (Cassandra and HBase), or graph (Neo4j and JanusGraph). This type of database does not preclude SQL. Note that NoSQL stands for “not only SQL.” Relational database "],
["what-is-sql.html", "Chapter 9 What is SQL?", " Chapter 9 What is SQL? Structured Query Language. Called SEQUEL and developed by IBM Corporation in the 1970s. Remains the standard language for a relational database management system. It’s a DECLARATIVE language (what to do &gt; how to do) Database management systems figures optimal way to execute query (query optimization) SELECT COLUMN FROM TABLE "],
["learning-objectives.html", "Chapter 10 Learning objectives 10.1 Setup 10.2 Workflow 10.3 Create a database 10.4 Copy an object as a table to the database (push) 10.5 Quick demonstrations: 10.6 Tidy-way: dplyr -&gt; SQL 10.7 Collect (pull) 10.8 Disconnect 10.9 References", " Chapter 10 Learning objectives Embracing a new mindset: shifting from ownership (opening CSVs in your laptop) to access (accessing data stored in a database) Learning how to use R and SQL to access and query a database SQL and R SQL R SELECT select() for columns, mutate() for expressions, summarise() for aggregates FROM which data frame WHERE filter() GROUP BY group_by() HAVING filter() after group_by() ORDER BY arrange() LIMIT head() Challenge 1 1. Can you tell me the difference in the order in which the following R and SQL code were written to wrangle data? For instance, in R, what command comes first? In contrast, in SQL, what command comes first? R example data %&gt;% # Data select() %&gt;% # Column filter() %&gt;% # Row group_by() %&gt;% # Group by summarise(n = n()) %&gt;% # Aggregation filter() %&gt;% # Row order_by() # Arrange SQL example SELECT column, aggregation (count())` # Column FROM data # Data WHERE condition # Row GROUP BY column # Group by HAVING condition # Row ORDER BY column # Arrange SQL Zine by by Julia Evans 10.1 Setup Let’s get to work. 10.1.1 Packages pacman::p_load() reduces steps for installing and loading several packages simultaneously. # pacman if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman # The rest of pkgs pacman::p_load( tidyverse, # tidyverse packages DBI, # using SQL queries RSQLite, # SQLite dbplyr, # use database with dplyr glue, # glue to automate workflow nycflights13 # toy data ) 10.1.2 Data sets The flight on-time performance data from the Bureau of Transpiration Statistics of the U.S. government. The data goes back to 1987 and its size is more than 20 gigabytes. For practice, we only use a small subset of the original data (flight data departing NYC in 2013) provided by RStudio. From RStudio. 10.2 Workflow Create/connect to a database Note that server also can be your laptop (called localhost). Short answer: To do so, you need interfaces between R and a database. We use RSQLite in this tutorial because it’s easy to set up. Long answer: The DBI package in R provides a client-side interface that allows dplyr to work with databases. DBI is automatically installed when you installed dbplyr. However, you need to install a specific backend engine (a tool for communication between R and a database management system) for the database (e.g., RMariaDB, RPostgres, RSQLite). In this workshop, we use SQLite because it is the easiest to get started with. Personally, I love PostgreSQL because it’s an open-source and also powerful to do many amazing things (e.g., text mining, geospatial analysis). Copy a table to the database Option 1: You can create a table and insert rows manually. Table Collection of rows Collection of columns (fields or attributes) Each col has a type: String: VARCHAR(20) Integer: INTEGER Floating-point: FLOAT, DOUBLE Date/time: DATE, TIME, DATETIME Schema: the structure of the database The table name The names and types of its columns Various optional additional information (e.g., constraints) CREATE TABLE students ( id INT AUTO_INCREMENT, name VARCHAR(30), birth DATE, gpa FLOAT, grad INT, PRIMARY KEY(id)); INSERT INTO students(name, birth, gpa, grad) VALUES (&#39;Adam&#39;, &#39;2000-08-04&#39;, 4.0, 2020); Option 2: Copy a file (object) to a table in a database using copy_to). We take this option as it’s fast and we would like to focus on querying in this workshop. Query the table Main focus Pull the results of interests (data) using collect() Disconnect the database 10.3 Create a database # Define a backend engine drv &lt;- RSQLite::SQLite() # Create an empty in-memory database con &lt;- DBI::dbConnect(drv, dbname = &quot;:memory:&quot;) # Connect to an existing database #con &lt;- DBI::dbConnect(RMariaDB::MariaDB(), # host = &quot;database.rstudio.com&quot;, # user = &quot;hadley&quot;, # password = rstudioapi::askForPassword(&quot;Database password&quot;) #) dbListTables(con) ## character(0) Note that con is empty at this stage. 10.4 Copy an object as a table to the database (push) # Copy objects to the data copy_to(dest = con, df = flights) copy_to(dest = con, df = airports) copy_to(dest = con, df = planes) copy_to(dest = con, df = weather) # If you want you can also decide what columns you want to copy: # copy_to(dest = con, # df = flights, # name = &quot;flights&quot;, # indexes = list(c(&quot;year&quot;, &quot;tailnum&quot;, &quot;dest&quot;))) # Show two tables in the database dbListTables(con) ## [1] &quot;airports&quot; &quot;flights&quot; &quot;planes&quot; &quot;sqlite_stat1&quot; &quot;sqlite_stat4&quot; ## [6] &quot;weather&quot; # Show the columns/attributes/fields of a table dbListFields(con, &quot;flights&quot;) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; ## [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot; &quot;arr_time&quot; &quot;sched_arr_time&quot; ## [9] &quot;arr_delay&quot; &quot;carrier&quot; &quot;flight&quot; &quot;tailnum&quot; ## [13] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; ## [17] &quot;hour&quot; &quot;minute&quot; &quot;time_hour&quot; dbListFields(con, &quot;weather&quot;) ## [1] &quot;origin&quot; &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;hour&quot; ## [6] &quot;temp&quot; &quot;dewp&quot; &quot;humid&quot; &quot;wind_dir&quot; &quot;wind_speed&quot; ## [11] &quot;wind_gust&quot; &quot;precip&quot; &quot;pressure&quot; &quot;visib&quot; &quot;time_hour&quot; 10.5 Quick demonstrations: SELECT desired columns FROM tables Select all columns (*) from flights table and show the first ten rows Note that you can combine SQL and R commands thanks to dbplyr. Option 1 DBI::dbGetQuery(con, &quot;SELECT * FROM flights;&quot;) %&gt;% head(10) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 7 19 B6 507 N516JB EWR FLL 158 1065 6 0 ## 8 -14 EV 5708 N829AS LGA IAD 53 229 6 0 ## 9 -8 B6 79 N593JB JFK MCO 140 944 6 0 ## 10 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## time_hour ## 1 1357034400 ## 2 1357034400 ## 3 1357034400 ## 4 1357034400 ## 5 1357038000 ## 6 1357034400 ## 7 1357038000 ## 8 1357038000 ## 9 1357038000 ## 10 1357038000 Option 2 (works faster) SELECT * FROM flights LIMIT 5 Table 10.1: 5 records year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest air_time distance hour minute time_hour 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 5 29 1357034400 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 5 40 1357034400 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN 183 1576 5 45 1357034400 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 6 0 1357038000 Option 3 (automating workflow) When local variables are updated, the SQL query is also automatically updated. # Local variables tbl &lt;- &quot;flights&quot; var &lt;- &quot;dep_delay&quot; num &lt;- 5 # Glue SQL query string # Note that to indicate a numeric value, you don&#39;t need `` sql_query &lt;- glue_sql(&quot; SELECT {`var`} FROM {`tbl`} LIMIT {num} &quot;, .con = con) # Run the query dbGetQuery(con, sql_query) ## dep_delay ## 1 2 ## 2 4 ## 3 2 ## 4 -1 ## 5 -6 Challenge 2 Can you rewrite the above code using LIMIT instead of head(10) You may notice that using only SQL code makes querying faster. Select dep_delay and arr_delay from flights table, show the first ten rows, then turn the result into a tibble. Challenge 3 Could you remind me how to see the list of attributes of a table? Let’s say you want to see the attributes of flights table. Collect the selected columns and filtered rows df &lt;- dbGetQuery(con, &quot;SELECT dep_delay, arr_delay FROM flights;&quot;) %&gt;% head(10) %&gt;% collect() Counting rows Count all (*) dbGetQuery(con, &quot;SELECT COUNT(*) FROM flights;&quot;) ## COUNT(*) ## 1 336776 dbGetQuery(con, &quot;SELECT COUNT(dep_delay) FROM flights;&quot;) ## COUNT(dep_delay) ## 1 328521 Count distinct values dbGetQuery(con, &quot;SELECT COUNT(DISTINCT dep_delay) FROM flights;&quot;) ## COUNT(DISTINCT dep_delay) ## 1 527 10.6 Tidy-way: dplyr -&gt; SQL Thanks to the dbplyr package you can use the dplyr syntax to query SQL. Note that pipe (%) works. # tbl select tables flights &lt;- con %&gt;% tbl(&quot;flights&quot;) airports &lt;- con %&gt;% tbl(&quot;airports&quot;) planes &lt;- con %&gt;% tbl(&quot;planes&quot;) weather &lt;- con %&gt;% tbl(&quot;weather&quot;) 10.6.1 select = SELECT flights %&gt;% select(contains(&quot;delay&quot;)) ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.30.1 [:memory:] ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # … with more rows Challenge 4 Your turn: write the same code in SQL 10.6.2 mutate = SELECT AS flights %&gt;% select(distance, air_time) %&gt;% mutate(speed = distance / (air_time / 60)) ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.30.1 [:memory:] ## distance air_time speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400 227 370. ## 2 1416 227 374. ## 3 1089 160 408. ## 4 1576 183 517. ## 5 762 116 394. ## 6 719 150 288. ## 7 1065 158 404. ## 8 229 53 259. ## 9 944 140 405. ## 10 733 138 319. ## # … with more rows Challenge 5 Your turn: write the same code in SQL (hint: mutate(new_var = var 1 * var2 = SELECT var1 * var2 AS near_var) 10.6.3 filter = WHERE flights %&gt;% filter(month == 1, day == 1) ## # Source: lazy query [?? x 19] ## # Database: sqlite 3.30.1 [:memory:] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with more rows, and 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dbl&gt; Challenge 6 Your turn: write the same code in SQL (hint: filter(condition1, condition2) = WHERE condition1 and condition2) Note that R and SQL operators are not exactly alike. R uses != for Not equal to. SQL uses &lt;&gt; or !=. Furthermore, there are some cautions about using NULL (NA; unknown or missing): it should be IS NULL or IS NOT NULL not =NULL or !=NULL. 10.6.4 arrange = ORDER BY flights %&gt;% arrange(carrier, desc(arr_delay)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT * ## FROM `flights` ## ORDER BY `carrier`, `arr_delay` DESC Challenge 7 Your turn: write the same code in SQL (hint: arrange(var1, desc(var2)) = ORDER BY var1, var2 DESC) 10.6.5 summarise = SELECT AS and group by = GROUP BY flights %&gt;% group_by(month, day) %&gt;% summarise(delay = mean(dep_delay)) ## Warning: Missing values are always removed in SQL. ## Use `mean(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.30.1 [:memory:] ## # Groups: month ## month day delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 11.5 ## 2 1 2 13.9 ## 3 1 3 11.0 ## 4 1 4 8.95 ## 5 1 5 5.73 ## 6 1 6 7.15 ## 7 1 7 5.42 ## 8 1 8 2.55 ## 9 1 9 2.28 ## 10 1 10 2.84 ## # … with more rows Challenge 8 Your turn: write the same code in SQL (hint: in SQL the order should be SELECT group_var1, group_var2, AVG(old_var) AS new_var -&gt; FROM -&gt; GROUP BY) If you feel too much challenged, here’s a help. flights %&gt;% group_by(month, day) %&gt;% summarise(delay = mean(dep_delay)) %&gt;% show_query() # Show the SQL equivalent! ## &lt;SQL&gt; ## SELECT `month`, `day`, AVG(`dep_delay`) AS `delay` ## FROM `flights` ## GROUP BY `month`, `day` 10.6.6 Joins Using joins is simpler in R than it is in SQL. However, more flexible joins exist in SQL and they are not available in R. Joins involving 3+ tables are not supported. Some advanced joins available in SQL are not supported. For more information, check out tidyquery to see the latest developments. flights %&gt;% left_join(weather, by = c(&quot;year&quot;, &quot;month&quot;)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT `LHS`.`year` AS `year`, `LHS`.`month` AS `month`, `LHS`.`day` AS `day.x`, `LHS`.`dep_time` AS `dep_time`, `LHS`.`sched_dep_time` AS `sched_dep_time`, `LHS`.`dep_delay` AS `dep_delay`, `LHS`.`arr_time` AS `arr_time`, `LHS`.`sched_arr_time` AS `sched_arr_time`, `LHS`.`arr_delay` AS `arr_delay`, `LHS`.`carrier` AS `carrier`, `LHS`.`flight` AS `flight`, `LHS`.`tailnum` AS `tailnum`, `LHS`.`origin` AS `origin.x`, `LHS`.`dest` AS `dest`, `LHS`.`air_time` AS `air_time`, `LHS`.`distance` AS `distance`, `LHS`.`hour` AS `hour.x`, `LHS`.`minute` AS `minute`, `LHS`.`time_hour` AS `time_hour.x`, `RHS`.`origin` AS `origin.y`, `RHS`.`day` AS `day.y`, `RHS`.`hour` AS `hour.y`, `RHS`.`temp` AS `temp`, `RHS`.`dewp` AS `dewp`, `RHS`.`humid` AS `humid`, `RHS`.`wind_dir` AS `wind_dir`, `RHS`.`wind_speed` AS `wind_speed`, `RHS`.`wind_gust` AS `wind_gust`, `RHS`.`precip` AS `precip`, `RHS`.`pressure` AS `pressure`, `RHS`.`visib` AS `visib`, `RHS`.`time_hour` AS `time_hour.y` ## FROM `flights` AS `LHS` ## LEFT JOIN `weather` AS `RHS` ## ON (`LHS`.`year` = `RHS`.`year` AND `LHS`.`month` = `RHS`.`month`) 10.7 Collect (pull) collect() is used to pull the data. Depending on the data size, it may take a long time to run. The following code won’t work. Error in UseMethod(“collect”) : no applicable method for ‘collect’ applied to an object of class “c(‘LayerInstance’, ‘Layer’, ‘ggproto’, ‘gg’)” origin_flights_plot &lt;- flights %&gt;% group_by(origin) %&gt;% tally() %&gt;% ggplot() + geom_col(aes(x = origin, y = n)) %&gt;% collect() This works. df &lt;- flights %&gt;% group_by(origin) %&gt;% tally() %&gt;% collect() origin_flights_plot &lt;- ggplot(df) + geom_col(aes(x = origin, y = n)) origin_flights_plot 10.8 Disconnect DBI::dbDisconnect(con) 10.9 References csv2db - for loading large CSV files in to a database R Studio, Database using R Ian Cook, “Bridging the Gap between SQL and R” rstudio::conf 2020 slides Video recording Data Carpentry contributors, SQL database and R, Data Carpentry, September 10, 2019. Introduction to dbplyr Josh Erickson, SQL in R, STAT 701, University of Michigan SQL zine by Julia Evans q - a command line tool that allows direct execution of SQL-like queries on CSVs/TSVs (and any other tabular text files) "]
]
