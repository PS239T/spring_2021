[
["index.html", "Computational Thinking for Social Scientists Chapter 1 PS239T 1.1 Textbook 1.2 Objectives 1.3 Logistics 1.4 Course schedule 1.5 Questions, comments, or suggestions 1.6 Special thanks 1.7 License", " Computational Thinking for Social Scientists Jae Yeon Kim 2020-09-30 Chapter 1 PS239T Welcome to PS239T This course will help social science graduate students to think computationally and develop proficiency with computational tools and techniques, necessary to conduct research in computational social science. Mastering these tools and techniques not only enables students to collect, wrangle, analyze, and interpret data with less pain and more fun, but it also let students to work on research projects that would previously seem impossible. 1.1 Textbook The online book version of the course materials is currently work in progress. I am loosely aiming the completion of this book in 2020. 1.2 Objectives The goal of this course is to help students become a more efficient and innovative researcher by leveraging the power of automation. The course is currently divided into two main subjects (fundamentals and applications) and six main sessions. 1.2.1 Part I Fundamentals In the first section, students learn best practices in data and code management using Git and Bash. In the second, students learn how to wrangle, model, and visualize data easier and faster. In the third, students learn how to use functions to automate repeated things and develop their own data tools (e.g., packages). 1.2.2 Part II Applications In the fourth, students learn how to collect and parse semi-structured data at scale (e.g., using APIs and webscraping). In the fifth, students learn how to analyze high-dimensional data (e.g., text) using machine learning. In the final, students learn how to access, query, and manage big data using SQL. We will learn how to do all of the above mostly in R, and sometimes in bash and Python. R is free, easy to learn (thanks to tidyverse and RStudio), fast (thanks to rcpp), runs everywhere, open (16,000+ packages; counting only ones available at CRAN), and has a growing massive and inclusive community (#rstats). 1.3 Logistics 1.3.1 Contributors Instructor and content developer: Jae Yeon Kim: jaeyeonkim@berkeley.edu 1.3.2 Time and location Lecture: TBD (Zoom) Section: TBD (Zoom) 1.3.3 Office hours By appointment with … 1.3.4 Slack &amp; GitHub Slack for communication (announcements and questions). You should ask questions about class material and assignments through the Slack channels so that everyone can benefit from the discussion. We encourage you to respond to each other’s questions as well. GitHub for everything else, including turning in assignments (except final project proposals, which will be submitted to Slack). Students are required to use GitHub for their final projects, which will be publicly available, unless they have special considerations (e.g. proprietary data). All course materials will be posted on GitHub at https://github.com/jaeyk/PS239T, including class notes, code demonstrations, sample data, and assignments. 1.3.5 Accessibility This class is committed to creating an environment in which everyone can participate, regardless of background, discipline, or disability. If you have a particular concern, please come to me as soon as possible so that we can make special arrangements. 1.3.6 Code for conduct TBD 1.3.7 Course requirements and grades This is a graded class based on the following: Completion of assigned homework (50%) Participation (25%) Final project (25%) 1.3.7.1 Assignments Assignments will be assigned at the end of every session. They will be due at the start of the following class unless otherwise noted. The assignments will be frequent but each of them should be fairly short. You are encouraged to work in groups, but the work you turn in must be your own. Group submission of homework, or turning in copies of the same code or output, is not acceptable. Remember, the only way you actually learn how to write code is to write code. Unless otherwise specified, assignments should be turned in as pdf documents via the bCourses site. 1.3.7.2 Class participation The class participation portion of the grade can be satisfied in one or more of the following ways: attending the lecture and section (note that section is non-optional) asking and answering questions in class contributing to class discussion through the bCourse site, and/or collaborating with the campus computing community, either by attending a D-Lab or BIDS workshop, submitting a pull request to a campus github repository (including the class repository), answering a question on StackExchange, or other involvement in the social computing / digital humanities community. Because we will be using laptops every class, the temptation to attend to other things during slow moments will be high. While you may choose to do so, I do request that you think of your laptop screen as in the public domain for the duration of classtime. Please do not load anything that will distract your classmates or is otherwise inappropriate to a classroom setting. 1.3.7.3 Final project The final project consists of using the tools we learned in class on your own data of interest. First- and second-year students in the political science department are encouraged to use this as an opportunity to gather data to be used for other courses or the second-year thesis. Students are required to write a short proposal by March (no more than 2 paragraphs) in order to get approval and feedback from the instructor. During sections in April we will have lightning talk sessions where students present their projects in a maximum 5 minute talk, with 5 minutes for class Q&amp;A. Since there is no expectation of a formal paper, you should select a project that is completable by the end of the term. In other words, submitting a research design for your future dissertation that will use skills from the class but collects no data is not acceptable, but completing a viably small portion of a study or thesis is. Final project rubric Final project template Final project examples 1.3.8 Class activities and materials 1.3.8.1 Lecture Classes will follow a “workshop” style, combining lecture and lab formats. The class is interactive, with students programming every session. During the “skills” parts of the class, we will be learning how to program in R, UNIX (bash), and Python by following course notes and tutorials. During the “applications” sections, we will follow a similar structure, with occasional guest speakers. 1.3.8.2 Section The “lab” section will generally be a less formal session dedicated to helping students with materials from lecture and homework. It will be mostly student led, so come with questions. If there are no questions, the lab turns into a “hackathon” where groups can work on the assignments together. Section is required unless prior permission to miss it is obtained from both the instructor and one’s groupmates. Attending office hours is not a substitute for attending section. 1.3.8.3 Computer requirements The software needed for the course is as follows: Access to the UNIX command line (e.g., a Mac laptop, a Bash wrapper on Windows) Git R and RStudio (latest versions) Anaconda and Python 3 (latest versions) Pandoc and LaTeX This requires a computer that can handle all this software. Almost any Mac will do the job. Most Windows machines are fine too if they have enough space and memory. You must have all the software downloaded and installed PRIOR to the first day of class. If there are issues with installation on your machine, please contact the section assistant, Julia Christensen, for assistance. See B_Install.md for more information. 1.4 Course schedule 1.4.1 Part I Fundamentals Week 1 Computational thinking and setup Week 2 Managing data and code Week 3 Tidy data and why it matters Week 4 Wrangling data Week 5 Wrangling data at scale Week 6 Modeling and visualizing tidy data Week 7 From for loop to functional programming Week 8 Developing your own data tools 1.4.2 Part II Applications Week 9 HTML/CSS: web scraping Week 10 XML/JSON: social media scraping Week 11 Supervised machine learning Week 12 Unsupervised machine learning Week 13 Database, SQL, MongoDB, and Spark Week 14 Wrap-up Week 15 Final presentation 1.5 Questions, comments, or suggestions Please create issues [TBD: issue template], if you have questions, comments, or suggestions. 1.6 Special thanks This course is a remix version of the course originally developed by Rochelle Terman then revised by Rachel Bernhard. Other teaching materials draw from the workshops I created for D-Lab and Data Science Discovery Program at UC Berkeley. 1.7 License This work is licensed under a Creative Commons Attribution 4.0 International License. "],
["motivation.html", "Chapter 2 Computational thinking 2.1 Why computational thinking 2.2 Computational way of thinking about data 2.3 Computational way of thinking about research process", " Chapter 2 Computational thinking 2.1 Why computational thinking If social scientists want to know how to work smart and not just hard, they need to take full advantage of the power of modern programming languages, and that power is automation. Let’s think about the following two cases. Case 1: Suppose a social scientist needs to collect data on civic organizations in the United States from websites, Internal Revenue Service reports, and social media posts. As the number of these organizations is large, the researcher could not collect a large volume of data from diverse sources, so they would hire undergraduates and distribute tasks among them. This is a typical data collection plan in social science research, and it is labor-intensive. Automation is not part of the game plan. Yet, it is critical for so many reasons. Because the process is costly, no one is likely to either replicate or update the data collection effort. Put differently, without making the process efficient, it is difficult for it to be reproducible and scalable. Case 2: An alternative is to write computer programs that collect such data automatically, parse them, and store them in interconnected databases. Additionally, someone may need to maintain and validate the quality of the data infrastructure. Nevertheless, this approach lowers the cost of the data collection process, thereby substantially increasing the reproducibility and scalability of the process. Furthermore, the researcher can document their code and publicly share it using their GitHub repository or even gather some of the functions they used and distribute them as open-source libraries. Programming is as valuable a skill as writing in social science research. The extent to which a researcher can automate the research process can determine its efficiency, reproducibility, and scalability. Every modern statistical and data analysis problem needs code to solve it. You shouldn’t learn just the basics of programming, spend some time gaining mastery. Improving your programming skills pays off because code is a force multiplier: once you’ve solved a problem once, code allows you to solve it much faster in the future. As your programming skill increases, the generality of your solutions improves: you solve not just the precise problem you encountered, but a wider class of related problems (in this way programming skill is very much like mathematical skill). Finally, sharing your code with others allows them to benefit from your experience. - Hadley Wickham How can we automate our research process? How can we talk to and teach a machine so that it could become (hopefully) the most competent and reliable research assistant ever? From BBC Bitesize 2.2 Computational way of thinking about data 2.2.1 Structure Structured data (Excel spreadsheets, CSVs) Tidy data Semi-structured data HTML/CSS: Websites JSON/XML: APIs 2.2.2 Dimension Low-dimensional data (n &gt; p) Survey, experimental, and administrative data High-dimensional data (n &lt; p) Text, speech, image, video, etc. 2.2.3 Size Data fit in your laptop’s memory Data don’t fit in your laptop’s memory (=big data) 2.3 Computational way of thinking about research process Computational tools and techniques make … Doing traditional research easier, faster, scalable, and more reproducible Data wrangling Modeling Visualization Documentation and collaboration easier, faster, scalable, safer, and more experimental Dynamic reporting (markdown) Version control system (Git and GitHub) Collecting and analyzing large and complex data possible Digital data collection (API and web scraping) Building a data infrastructure (SQL) Machine learning "],
["intro.html", "Chapter 3 Managing data and code 3.1 Project-oriented research 3.2 Writing code: How to code like a professional 3.3 Asking questions: Minimal reproducible example 3.4 References", " Chapter 3 Managing data and code 3.1 Project-oriented research 3.1.1 Computational reproducibility 3.1.1.1 Setup pacman::p_load( tidyverse, # tidyverse here # computational reproducibility ) 3.1.1.2 Motivation Why do you need to make your research project computationally reproducible? For your self-interest and public benefits. 3.1.1.3 How to organize files in a project You won’t be able to reproduce your project unless it is efficiently organized. Step 1. Environment is part of your project. If someone can’t reproduce your environment, they won’t be able to run your code. Launch R Studio. Choose Tools &gt; Global Options. You should not check Restor .RData into workspace at startup and set saving workspace option to NEVER. Step 2. For each project, create a project directory named after the project. # Don&#39;t name it a project. Use a name that&#39;s more informative. For instance, us_election not my_project. dir.create(&quot;../us_election&quot;) Step 3. Launch R Studio. Choose File &gt; New project &gt; Browse existing directories &gt; Create project This allows each project has its own workspace. Step 4. Organize files by putting them in separate subdirectories and naming them in a sensible way. Treat raw data as read only (raw data should be RAW!) and put in the data subdirectory. Note that version control does not need replace backup. You still need to backup your raw data. dir.create(here::here(&quot;us_election&quot;, &quot;data&quot;)) Separate read-only data from processed data and put in the processed_data subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;processed_data&quot;)) Put your code in the src subdirectory. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Put generated outputs (e.g., tables, figures) in the outputs subdirectory and treat them as disposable. dir.create(here::here(&quot;us_election&quot;, &quot;outputs&quot;)) Put your custom functions in the functions subdirectory. You can gather some of these functions and distribute them as an open-source library. dir.create(here::here(&quot;us_election&quot;, &quot;src&quot;)) Challenge 2 Set a project structure for a project named “starwars”. 3.1.1.4 How to organize code in a R markdown file In addition to environment, workflow is an important component of project efficiency and reproducibility. What is R markdown? An R package, developed by Yihui Xie, that provides an authoring framework for data science. Xie is also a developer of many widely popular R packages such as knitr, xaringan (cool kids use xaringan not Beamer these days), blogdown (used to create my personal website), and bookdown (used to create this book) among many others. Many applications: reports, presentations, dashboards, websites Check out Communicating with R markdown workshop by Alison Hill (RStudio) Alison Hill is a co-author of blogdown: Creating Websites with R Markdown. Key strengths: dynamic reporting + reproducible science + easy deployment R Markdown The bigger picture - Garrett Grolemund R-Ladies Oslo (English) - Reports to impress your boss! Rmarkdown magic - Athanasia Mowinckel R Markdown basic syntax # Header 1 ## Header 2 ### Header 3 Use these section headers to indicate workflow. # Import packages and data # Tidy data # Wrangle data # Model data # Visualize data Press ctrl + shift + o. You can see a document outline based on these headers. This is a nice feature for finding code you need to focus. If your project’s scale is large, then divide these sections into files, number, and save them in code subdirectory. 01_wrangling.Rmd 02_modeling.Rmd … 3.1.1.5 Making a project computationally reproducible setwd(): set a working directory. Note that using setwd() is not a reproducible way to set up your project. For instance, none will be able to run the following code except me. # Set a working directory setwd(&quot;/home/jae/starwars&quot;) # Do something ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() # Export the object. # dot means the working directory set by setwd() ggsave(&quot;./outputs/example.png&quot;) # This is called relative path Instead, learn how to use here()’. Key idea: separate workflow (e.g., workspace information) from products (code and data). For more information, read Jenny Bryan’s wonderful piece on project-oriented workflow. Example # New: Reproducible ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() ggsave(here(&quot;project&quot;, &quot;outputs&quot;, &quot;example.png&quot;)) How here works here() function shows what’s the top-level project directory. here::here() Build a path including subdirectories here::here(&quot;project&quot;, &quot;outputs&quot;) #depth 1 #depth 2 How here defines the top-level project directory. The following list came from the here package vignette). Is a file named .here present? Is this an RStudio Project? (Note that we already set up an RStudio Project! So, if you use RStudio’s project feature, then you are ready to use here.) Is this an R package? Does it have a DESCRIPTION file? Is this a remake project? Does it have a file named remake.yml? Is this a projectile project? Does it have a file named .projectile? Is this a checkout from a version control system? Does it have a directory named .git or .svn? Currently, only Git and Subversion are supported. If there’s no match then use set_here() to create an empty .here file. Challenge 1 Can you define computational reproducibility? Can you explain why sharing code and data is not enough for computational reproducibility? 3.1.2 Version control (Git and Bash) 3.1.2.1 What Is Bash? 3.1.2.1.1 Writing your first shell script Write a shell script that creates a directory called /pdfs under /Download directory, then find PDF files in /Download and copy those files to pdfs. This shell script creates a backup. #!/bin/sh mkdir /home/jae/Downloads/pdfs cd Download cp *.pdf pdfs/ echo &quot;Copied pdfs&quot; 3.1.2.2 What Are Git and GitHub? Figure 2.1. A schematic git workflow from Healy’s “The Plain Person’s Guide to Plain Text Social Science” 3.1.2.2.1 Basics: git push and git pull 3.1.2.2.2 Time machine: git revert 3.1.2.2.3 Parallel universe: git branch 3.1.2.2.4 User-manual: readme README.md In this simple markdown file, note some basic information about the project including the project structure. This is how I used the README.md file for this course. Check out my GitHub account to see how I manage my projects. 3.1.2.3 Deployment: GitHub Pages 3.1.2.4 Tracking progress: GitHub Issues 3.1.2.5 Project management: GitHub Dashboards 3.2 Writing code: How to code like a professional 3.2.1 Write readable code What is code style? Every major open-source project has its own style guide: a set of conventions (sometimes arbitrary) about how to write code for that project. It is much easier to understand a large codebase when all the code in it is in a consistent style. - Google Style Guides 10 Tips For Clean Code - Michael Toppa How to avoid smelly code? Check out the code-smells Git repository by Jenny Bryan. Code smells and feels - Jenny Bryan \"Code smell\" is an evocative term for that vague feeling of unease we get when reading certain bits of code. It's not necessarily wrong, but neither is it obviously correct. We may be reluctant to work on such code, because past experience suggests it's going to be fiddly and bug-prone. In contrast, there's another type of code that just feels good to read and work on. What's the difference? If we can be more precise about code smells and feels, we can be intentional about writing code that is easier and more pleasant to work on. I've been fortunate to spend the last couple years embedded in a group of developers working on the tidyverse and r-lib packages. Based on this experience, I'll talk about specific code smells and deodorizing strategies for R. - Jenny Bryan Naming matters When naming files: Don’t use special characters. (Spaces make filenames awkward in the console/command-line.) Don’t capitalize. (UNIX is case sensitive.) Numbering them if files should be run in an order. # Good fit_models.R # Bad fit models.R When naming objects: Don’t use special characters. Don’t capitalize. # Good day_one # Bad DayOne When naming functions: Don’t use special characters. Don’t capitalize. Use verbs instead of nouns. (Functions do something!) # Good run_rdd # Bad rdd Spacing # Good x[, 1] mean(x, na.rm = TRUE) # Bad x[,1] mean (x, na.rm = TRUE) Indenting # Good if (y &lt; 0) { message(&quot;y is negative&quot;) } # Bad if (y &lt; 0) { message(&quot;Y is negative&quot;)} Long lines # Good do_something_very_complicated( something = &quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) # Bad do_something_very_complicated(&quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) Comments Use comments to explain your decisions. But, show your code; Do not try to explain your code by comments. Also, try to comment out rather than delete the code that you experiment with. # Average sleep hours of Jae jae %&gt;% # By week group_by(week) %&gt;% # Mean sleep hours summarise(week_sleep = mean(sleep, na.rm = TRUE)) Pipes (chaining commands) # Good iris %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, mean) %&gt;% ungroup() %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) # Bad iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% ungroup %&gt;% gather(measure, value, -Species) %&gt;% arrange(value) Additional tips Use lintr to check whether your code complies with a recommended style guideline (e.g., tidyverse) and styler package to format your code according to the style guideline. how lintr works 3.2.2 Write reusable code Pasting Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia It’s okay for pasting for the first attempt to solve a problem. But if you copy and paste three times (a.k.a. Rule of Three in programming), something’s wrong. You’re working too hard. You need to be lazy. What do I mean and how can you do that? Example Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble(&quot;a&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5 , replace= TRUE)) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Using a function function: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. # Create a custom function fix_missing &lt;- function(x) { # INPUT x[x == -99] &lt;- NA # COMPUTATION x # OUTPUT } # Apply the function to each column (vector) # This iterated part can and should be automated. df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df Automation Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) # What is this magic? We will unpack the blackbox (`map_df()`) later. df Takeaways Your code becomes more reusable, when it’s easier to change, debug, and scale up. Don’t repeat yourself and embrace the power of lazy programming. Lazy, because only lazy programmers will want to write the kind of tools that might replace them in the end. Lazy, because only a lazy programmer will avoid writing monotonous, repetitive code—thus avoiding redundancy, the enemy of software maintenance and flexible refactoring. Mostly, the tools and processes that come out of this endeavor fired by laziness will speed up the production. - Philipp Lenssen Only when your code becomes reusable, you would become efficient in your data work. Otherwise, you need to start from scratch or copy and paste, when you work on a new project. Code reuse aims to save time and resources and reduce redundancy by taking advantage of assets that have already been created in some form within the software product development process.[2] The key idea in reuse is that parts of a computer program written at one time can be or should be used in the construction of other programs written at a later time. - Wikipedia 3.2.3 Test your code systematically 3.3 Asking questions: Minimal reproducible example 3.3.1 How to create a minimal reproducible example 3.4 References Project-oriented research Computational reproducibility “Good Enough Practices in Scientific Computing” by PLOS Project Management with RStudio by Software Carpentry Initial steps toward reproducible research by Karl Broman Version control Version Control with Git by Software Carpentry The Plain Person’s Guide to Plain Text Social Science by Kieran Healy Writing code Style guides R Google’s R style guide R code style guide by Hadley Wickham The tidyverse style guide by Hadley Wickham Python Google Python Style Guide Code Style by the Hitchhiker’s Guide to Python Asking questions "],
["tidy-data.html", "Chapter 4 Tidy data and its friends 4.1 Setup 4.2 Tidyverse 4.3 Tidy data 4.4 Reshaping 4.5 Rearranging 4.6 Subset observations (rows) 4.7 Subset variables (columns) 4.8 Counting 4.9 Summarizing 4.10 Grouping 4.11 Nesting 4.12 Mapping 4.13 Joining", " Chapter 4 Tidy data and its friends 4.1 Setup Check your dplyr package is up-to-date by typing packageVersion(\"dplyr\"). If the current installed version is less than 1.0, then update by typing update.packages(\"dplyr\"). You may need to restart R to make it work. ifelse(packageVersion(&quot;dplyr&quot;) &gt; 1, &quot;The installed version of dplyr package is greater than or equal to 1.0.0&quot;, update.packages(&quot;dplyr&quot;)) ## [1] &quot;The installed version of dplyr package is greater than or equal to 1.0.0&quot; if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load( tidyverse, # for the tidyverse framework here, # for computational reproducibility gapminder, # toy data nycflights13 # for exercise ) The rest of the chapter follows the basic structure in the Data Wrangling Cheat Sheet created by RStudio. 4.2 Tidyverse Tidyverse design guide Human centered Consistent Composable (modualized) Inclusive Influenced by the Basics of the Unix Philosophy, The Zen of Python, and the Design Principles Behind Smalltalk 4.3 Tidy data “Tidy data sets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” - Hadley Wickham Variables -&gt; Columns Observations -&gt; Rows Values -&gt; Cells Tidy Data Example (Source: R for Data Science) If dataframes are tidy, it’s easy to transform, visualize, model, and program them using tidyverse packages (a whole workflow). Tidyverse: an opinionated collection of R packages Nevertheless, don’t be religious. In summary, tidy data is a useful conceptual idea and is often the right way to go for general, small data sets, but may not be appropriate for all problems. - Jeff Leek For instance, in many data science applications, linear algebra-based computations are essential (e.g., Principal Component Analysis). These computations are optimized to work on matrices, not tidy data frames (for more information, read Jeff Leek’s blog post). This is what a tidy data looks like. library(tidyverse) table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 4.4 Reshaping Let’s take a look at the cases of untidy data. Messy Data Case 1 (Source: R for Data Science) Make It Longer Challenge: Why this data is not tidy? table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 Let’s pivot (rotate by 90 degree). pivot_longer() increases the number of rows (longer) and decreases the number of columns. The inverse function is pivot_wider(). These functions improve the usability of gather() and spread(). What pivot_longer() does (Source: https://www.storybench.org) # Old way, less intuitive table4a %&gt;% gather(key = &quot;cases&quot;, # Current column names value = &quot;year&quot;, # The values matched to cases c(&quot;1999&quot;, &quot;2000&quot;)) # Selected columns ## # A tibble: 6 x 3 ## country cases year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 # New way, more intuitive table4a %&gt;% pivot_longer( cols = c(&quot;1999&quot;, &quot;2000&quot;), # Selected columns names_to = &quot;year&quot;, # Shorter columns (the columns going to be in one column called year) values_to = &quot;cases&quot;) # Longer rows (the values are going to be in a separate column called named cases) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 There’s another problem, did you catch it? The data type of year variable should be numeric not character. By default, pivot_longer() transforms uninformative columns to character. You can fix this problem by using names_transform argument. table4a %&gt;% pivot_longer( cols = c(&quot;1999&quot;, &quot;2000&quot;), # Put two columns together names_to = &quot;year&quot;, # Shorter columns (the columns going to be in one column called year) values_to = &quot;cases&quot;, # Longer rows (the values are going to be in a separate column called named cases) names_transform = list(year = readr::parse_number) ) # Transform the variable ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 Additional tips parse_number() also keeps only numeric information in a variable. parse_number(&quot;reply1994&quot;) ## [1] 1994 A flat file (e.g., CSV) is a rectangular shaped combination of strings. Parsing determines the type of each column and turns into a vector of a more specific type. Tidyverse has parse_ functions (from readr package) that are flexible and fast (e.g., parse_integer(), parse_double(), parse_logical(), parse_datetime(), parse_date(), parse_time(), parse_factor(), etc). Let’s do another practice. Challenge Why this data is not tidy? (This exercise comes from pivot function vigenette.) Too long or too wide? billboard ## # A tibble: 317 x 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+h… The … 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doo… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doo… Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 B… Wobb… 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Tee… Danc… 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliy… I Do… 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliy… Try … 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams… Open… 2000-08-26 76 76 74 69 68 67 61 58 ## # … with 307 more rows, and 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, ## # wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, ## # wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, ## # wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, ## # wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, ## # wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, ## # wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, ## # wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, wk49 &lt;dbl&gt;, wk50 &lt;dbl&gt;, wk51 &lt;dbl&gt;, wk52 &lt;dbl&gt;, ## # wk53 &lt;dbl&gt;, wk54 &lt;dbl&gt;, wk55 &lt;dbl&gt;, wk56 &lt;dbl&gt;, wk57 &lt;dbl&gt;, wk58 &lt;dbl&gt;, ## # wk59 &lt;dbl&gt;, wk60 &lt;dbl&gt;, wk61 &lt;dbl&gt;, wk62 &lt;dbl&gt;, wk63 &lt;dbl&gt;, wk64 &lt;dbl&gt;, ## # wk65 &lt;dbl&gt;, wk66 &lt;lgl&gt;, wk67 &lt;lgl&gt;, wk68 &lt;lgl&gt;, wk69 &lt;lgl&gt;, wk70 &lt;lgl&gt;, ## # wk71 &lt;lgl&gt;, wk72 &lt;lgl&gt;, wk73 &lt;lgl&gt;, wk74 &lt;lgl&gt;, wk75 &lt;lgl&gt;, wk76 &lt;lgl&gt; How can you fix it? Which pivot? # Old way billboard %&gt;% gather(key = &quot;week&quot;, value = &quot;rank&quot;, starts_with(&quot;wk&quot;)) %&gt;% # Use regular expressions drop_na() # Drop NAs ## # A tibble: 5,307 x 5 ## artist track date.entered week rank ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk1 87 ## 2 2Ge+her The Hardest Part Of ... 2000-09-02 wk1 91 ## 3 3 Doors Down Kryptonite 2000-04-08 wk1 81 ## 4 3 Doors Down Loser 2000-10-21 wk1 76 ## 5 504 Boyz Wobble Wobble 2000-04-15 wk1 57 ## 6 98^0 Give Me Just One Nig... 2000-08-19 wk1 51 ## 7 A*Teens Dancing Queen 2000-07-08 wk1 97 ## 8 Aaliyah I Don&#39;t Wanna 2000-01-29 wk1 84 ## 9 Aaliyah Try Again 2000-03-18 wk1 59 ## 10 Adams, Yolanda Open My Heart 2000-08-26 wk1 76 ## # … with 5,297 more rows Note that pivot_longer() is more versatile than gather(). # New way billboard %&gt;% pivot_longer( cols = starts_with(&quot;wk&quot;), # Use regular expressions names_to = &quot;week&quot;, values_to = &quot;rank&quot;, values_drop_na = TRUE # Drop NAs ) ## # A tibble: 5,307 x 5 ## artist track date.entered week rank ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk1 87 ## 2 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk2 82 ## 3 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk3 72 ## 4 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk4 77 ## 5 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk5 87 ## 6 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk6 94 ## 7 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk7 99 ## 8 2Ge+her The Hardest Part Of ... 2000-09-02 wk1 91 ## 9 2Ge+her The Hardest Part Of ... 2000-09-02 wk2 87 ## 10 2Ge+her The Hardest Part Of ... 2000-09-02 wk3 92 ## # … with 5,297 more rows Make It Wider Why this data is not tidy? table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Each observation is spread across two rows. How can you fix it?: pivot_wider(). Two differences between pivot_longer() and pivot_wider() In pivot_longer(), the arguments are named names_to and values_to (to). In pivot_wider(), this pattern is opposite. The arguments are named names_from and values_from (from). The number of required arguments for pivot_longer() is 3 (col, names_to, values_to). The number of required arguments for pivot_wider() is 2 (names_from, values_from). What pivot_wider() does (Source: https://www.storybench.org) # Old way table2 %&gt;% spread(key = type, value = count) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 # New way table2 %&gt;% pivot_wider( names_from = type, # first values_from = count # second ) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Sometimes, a consultee came to me and asked: “I don’t have missing values in my original dataframe. Then R said that I have missing values after I’ve done some data transformations. What happened?” Here’s an answer. R defines missing values in two ways. Implicit missing values: simply not present in the data. Explicit missing values: flagged with NA Challenge The example comes from R for Data Science. stocks &lt;- tibble( year = c(2019, 2019, 2019, 2020, 2020, 2020), qtr = c(1, 2, 3, 2, 3, 4), return = c(1, 2, 3, NA, 2, 3) ) stocks ## # A tibble: 6 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019 1 1 ## 2 2019 2 2 ## 3 2019 3 3 ## 4 2020 2 NA ## 5 2020 3 2 ## 6 2020 4 3 Where is explicit missing value? Does stocks have implicit missing values? # implicit missing values become explicit stocks %&gt;% pivot_wider(names_from = year, values_from = return) ## # A tibble: 4 x 3 ## qtr `2019` `2020` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 NA ## 2 2 2 NA ## 3 3 3 2 ## 4 4 NA 3 Challenge This exercise comes from pivot function vigenette. Could you make station a series of dummy variables using pivot_wider()? fish_encounters ## # A tibble: 114 x 3 ## fish station seen ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 4842 Release 1 ## 2 4842 I80_1 1 ## 3 4842 Lisbon 1 ## 4 4842 Rstr 1 ## 5 4842 Base_TD 1 ## 6 4842 BCE 1 ## 7 4842 BCW 1 ## 8 4842 BCE2 1 ## 9 4842 BCW2 1 ## 10 4842 MAE 1 ## # … with 104 more rows Which pivot you should use? Are there explicit missing values? How could you turn these NAs into 0s? Check values_fill argument in the pivot_wider() function. Separate Messy Data Case 2 (Source: R for Data Science) # Toy example df &lt;- data.frame(x = c(NA, &quot;Dad.apple&quot;, &quot;Mom.orange&quot;, &quot;Daughter.banana&quot;)) df ## x ## 1 &lt;NA&gt; ## 2 Dad.apple ## 3 Mom.orange ## 4 Daughter.banana # Separate df %&gt;% separate(x, into = c(&quot;Name&quot;, &quot;Preferred_fruit&quot;)) ## Name Preferred_fruit ## 1 &lt;NA&gt; &lt;NA&gt; ## 2 Dad apple ## 3 Mom orange ## 4 Daughter banana # Don&#39;t need the first variable df %&gt;% separate(x, into = c(NA, &quot;Preferred_fruit&quot;)) ## Preferred_fruit ## 1 &lt;NA&gt; ## 2 apple ## 3 orange ## 4 banana Practice table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Note sep argument. You can specify how to separate joined values. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Note convert argument. You can specify whether automatically convert the new values or not. table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, convert = TRUE) # cases and population become integers ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Unite pivot_longer() &lt;-&gt; pivot_wider() separate() &lt;-&gt; unite() # Create a toy example df &lt;- data.frame( name = c(&quot;Jae&quot;, &quot;Sun&quot;, &quot;Jane&quot;, NA), birthmonth = c(&quot;April&quot;, &quot;April&quot;, &quot;June&quot;, NA)) # Include missing values df %&gt;% unite(&quot;contact&quot;, c(&quot;name&quot;, &quot;birthmonth&quot;)) ## contact ## 1 Jae_April ## 2 Sun_April ## 3 Jane_June ## 4 NA_NA # Do not include missing values df %&gt;% unite(&quot;contact&quot;, c(&quot;name&quot;, &quot;birthmonth&quot;), na.rm = TRUE) ## contact ## 1 Jae_April ## 2 Sun_April ## 3 Jane_June ## 4 4.5 Rearranging Arrange Order rows dplyr::arrange(mtcars, mpg) # Low to High (default) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 dplyr::arrange(mtcars, desc(mpg)) # High to Row ## mpg cyl disp hp drat wt qsec vs am gear carb ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Rename Rename columns df &lt;- tibble(y = c(2011, 2012, 2013)) df ## # A tibble: 3 x 1 ## y ## &lt;dbl&gt; ## 1 2011 ## 2 2012 ## 3 2013 df %&gt;% rename(Year = # OLD name y) # NEW name ## # A tibble: 3 x 1 ## Year ## &lt;dbl&gt; ## 1 2011 ## 2 2012 ## 3 2013 4.6 Subset observations (rows) Choose row by logical condition Single condition starwars %&gt;% filter(gender == &quot;female&quot;) %&gt;% arrange(desc(height)) ## # A tibble: 0 x 14 ## # … with 14 variables: name &lt;chr&gt;, height &lt;int&gt;, mass &lt;dbl&gt;, hair_color &lt;chr&gt;, ## # skin_color &lt;chr&gt;, eye_color &lt;chr&gt;, birth_year &lt;dbl&gt;, sex &lt;chr&gt;, ## # gender &lt;chr&gt;, homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; The following filtering example was inspired by the suzanbert’s dplyr blog post. Multiple conditions (numeric) # First example starwars %&gt;% filter(height &lt; 180, height &gt; 160) %&gt;% nrow() ## [1] 24 # Same as above starwars %&gt;% filter(height &lt; 180 &amp; height &gt; 160) %&gt;% nrow() ## [1] 24 # Not same as above starwars %&gt;% filter(height &lt; 180 | height &gt; 160) %&gt;% nrow() ## [1] 81 Challenge Use filter(between()) to find characters whose heights are between 180 and 160 and (2) count the number of these observations. Minimum reproducible example df &lt;- tibble( heights = c(160:180), char = rep(&quot;none&quot;, length(c(160:180))) ) df %&gt;% filter(between(heights, 161, 179)) ## # A tibble: 19 x 2 ## heights char ## &lt;int&gt; &lt;chr&gt; ## 1 161 none ## 2 162 none ## 3 163 none ## 4 164 none ## 5 165 none ## 6 166 none ## 7 167 none ## 8 168 none ## 9 169 none ## 10 170 none ## 11 171 none ## 12 172 none ## 13 173 none ## 14 174 none ## 15 175 none ## 16 176 none ## 17 177 none ## 18 178 none ## 19 179 none Multiple conditions (character) # Filter names include ars; `grepl` is a base R function starwars %&gt;% filter(grepl(&quot;ars&quot;, tolower(name))) ## # A tibble: 4 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Owen… 178 120 brown, gr… light blue 52 male mascu… ## 2 Beru… 165 75 brown light blue 47 fema… femin… ## 3 Quar… 183 NA black dark brown 62 &lt;NA&gt; &lt;NA&gt; ## 4 Clie… 183 NA brown fair blue 82 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # Or, if you prefer dplyr way starwars %&gt;% filter(str_detect(tolower(name), &quot;ars&quot;)) ## # A tibble: 4 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Owen… 178 120 brown, gr… light blue 52 male mascu… ## 2 Beru… 165 75 brown light blue 47 fema… femin… ## 3 Quar… 183 NA black dark brown 62 &lt;NA&gt; &lt;NA&gt; ## 4 Clie… 183 NA brown fair blue 82 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # Filter brown and black hair_color starwars %&gt;% filter(hair_color %in% c(&quot;black&quot;, &quot;brown&quot;)) ## # A tibble: 31 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Leia… 150 49 brown light brown 19 fema… femin… ## 2 Beru… 165 75 brown light blue 47 fema… femin… ## 3 Bigg… 183 84 black light brown 24 male mascu… ## 4 Chew… 228 112 brown unknown blue 200 male mascu… ## 5 Han … 180 80 brown fair brown 29 male mascu… ## 6 Wedg… 170 77 brown fair hazel 21 male mascu… ## 7 Jek … 180 110 brown fair blue NA male mascu… ## 8 Boba… 183 78.2 black fair brown 31.5 male mascu… ## 9 Land… 177 79 black dark brown 31 male mascu… ## 10 Arve… NA NA brown fair brown NA male mascu… ## # … with 21 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Challenge Use str_detect() to find characters whose names include “Han”. Choose row by position (row index) starwars %&gt;% arrange(desc(height)) %&gt;% slice(1:6) ## # A tibble: 6 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yara… 264 NA none white yellow NA male mascu… ## 2 Tarf… 234 136 brown brown blue NA male mascu… ## 3 Lama… 229 88 none grey black NA male mascu… ## 4 Chew… 228 112 brown unknown blue 200 male mascu… ## 5 Roos… 224 82 none grey orange NA male mascu… ## 6 Grie… 216 159 none brown, wh… green, y… NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Sample by fraction # For reproducibility set.seed(1234) # Old way starwars %&gt;% sample_frac(0.10, replace = FALSE) # Without replacement ## # A tibble: 9 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arve… NA NA brown fair brown NA male mascu… ## 2 Sly … 178 48 none pale white NA &lt;NA&gt; &lt;NA&gt; ## 3 IG-88 200 140 none metal red 15 none mascu… ## 4 Bigg… 183 84 black light brown 24 male mascu… ## 5 Leia… 150 49 brown light brown 19 fema… femin… ## 6 Watto 137 NA black blue, grey yellow NA male mascu… ## 7 Jabb… 175 1358 &lt;NA&gt; green-tan… orange 600 herm… mascu… ## 8 Dart… 202 136 none white yellow 41.9 male mascu… ## 9 Taun… 213 NA none grey black NA fema… femin… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # New way starwars %&gt;% slice_sample(prop = 0.10, replace = FALSE) ## # A tibble: 8 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Raym… 188 79 brown light brown NA male mascu… ## 2 Tarf… 234 136 brown brown blue NA male mascu… ## 3 Han … 180 80 brown fair brown 29 male mascu… ## 4 Mas … 196 NA none blue blue NA male mascu… ## 5 Barr… 166 50 black yellow blue 40 fema… femin… ## 6 Dart… 202 136 none white yellow 41.9 male mascu… ## 7 Finn NA NA black dark dark NA male mascu… ## 8 Boba… 183 78.2 black fair brown 31.5 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Sample by number # Old way starwars %&gt;% sample_n(20, replace = FALSE) # Without replacement ## # A tibble: 20 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Quar… 183 NA black dark brown 62 &lt;NA&gt; &lt;NA&gt; ## 2 Poe … NA NA brown light brown NA male mascu… ## 3 Mas … 196 NA none blue blue NA male mascu… ## 4 Zam … 168 55 blonde fair, gre… yellow NA fema… femin… ## 5 Leia… 150 49 brown light brown 19 fema… femin… ## 6 Jang… 183 79 black tan brown 66 male mascu… ## 7 Ben … 163 65 none grey, gre… orange NA male mascu… ## 8 Padm… 165 45 brown light brown 46 fema… femin… ## 9 Mace… 188 84 none dark brown 72 male mascu… ## 10 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 11 Shmi… 163 NA black fair brown 72 fema… femin… ## 12 Ratt… 79 15 none grey, blue unknown NA male mascu… ## 13 Nute… 191 90 none mottled g… red NA male mascu… ## 14 Dart… 175 80 none red yellow 54 male mascu… ## 15 Bib … 180 NA none pale pink NA male mascu… ## 16 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 17 Yara… 264 NA none white yellow NA male mascu… ## 18 Ki-A… 198 82 white pale yellow 92 male mascu… ## 19 BB8 NA NA none none black NA none mascu… ## 20 Eeth… 171 NA black brown brown NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # New way starwars %&gt;% slice_sample(n = 20, replace = FALSE) # Without replacement ## # A tibble: 20 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Owen… 178 120 brown, gr… light blue 52 male mascu… ## 2 Ki-A… 198 82 white pale yellow 92 male mascu… ## 3 Capt… NA NA unknown unknown unknown NA &lt;NA&gt; &lt;NA&gt; ## 4 Greg… 185 85 black dark brown NA male mascu… ## 5 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 6 Ackb… 180 83 none brown mot… orange 41 male mascu… ## 7 Wedg… 170 77 brown fair hazel 21 male mascu… ## 8 Dormé 165 NA brown light brown NA fema… femin… ## 9 Rey NA NA brown light hazel NA fema… femin… ## 10 IG-88 200 140 none metal red 15 none mascu… ## 11 Roos… 224 82 none grey orange NA male mascu… ## 12 Shmi… 163 NA black fair brown 72 fema… femin… ## 13 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 14 Poe … NA NA brown light brown NA male mascu… ## 15 Obi-… 182 77 auburn, w… fair blue-gray 57 male mascu… ## 16 Plo … 188 80 none orange black 22 male mascu… ## 17 Tarf… 234 136 brown brown blue NA male mascu… ## 18 Lobot 175 79 none light blue 37 male mascu… ## 19 San … 191 NA none grey gold NA male mascu… ## 20 Kit … 196 87 none green black NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Top 10 rows orderd by height # Old way starwars %&gt;% top_n(10, height) ## # A tibble: 10 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Dart… 202 136 none white yellow 41.9 male mascu… ## 2 Chew… 228 112 brown unknown blue 200 male mascu… ## 3 Roos… 224 82 none grey orange NA male mascu… ## 4 Rugo… 206 NA none green orange NA male mascu… ## 5 Yara… 264 NA none white yellow NA male mascu… ## 6 Lama… 229 88 none grey black NA male mascu… ## 7 Taun… 213 NA none grey black NA fema… femin… ## 8 Grie… 216 159 none brown, wh… green, y… NA male mascu… ## 9 Tarf… 234 136 brown brown blue NA male mascu… ## 10 Tion… 206 80 none grey black NA male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; # New way starwars %&gt;% slice_max(height, n = 10) # Variable first, Argument second ## # A tibble: 10 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yara… 264 NA none white yellow NA male mascu… ## 2 Tarf… 234 136 brown brown blue NA male mascu… ## 3 Lama… 229 88 none grey black NA male mascu… ## 4 Chew… 228 112 brown unknown blue 200 male mascu… ## 5 Roos… 224 82 none grey orange NA male mascu… ## 6 Grie… 216 159 none brown, wh… green, y… NA male mascu… ## 7 Taun… 213 NA none grey black NA fema… femin… ## 8 Rugo… 206 NA none green orange NA male mascu… ## 9 Tion… 206 80 none grey black NA male mascu… ## 10 Dart… 202 136 none white yellow 41.9 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; 4.7 Subset variables (columns) names(msleep) ## [1] &quot;name&quot; &quot;genus&quot; &quot;vore&quot; &quot;order&quot; &quot;conservation&quot; ## [6] &quot;sleep_total&quot; &quot;sleep_rem&quot; &quot;sleep_cycle&quot; &quot;awake&quot; &quot;brainwt&quot; ## [11] &quot;bodywt&quot; Select only numeric columns # Only numeric msleep %&gt;% select(where(is.numeric)) ## # A tibble: 83 x 6 ## sleep_total sleep_rem sleep_cycle awake brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.1 NA NA 11.9 NA 50 ## 2 17 1.8 NA 7 0.0155 0.48 ## 3 14.4 2.4 NA 9.6 NA 1.35 ## 4 14.9 2.3 0.133 9.1 0.00029 0.019 ## 5 4 0.7 0.667 20 0.423 600 ## 6 14.4 2.2 0.767 9.6 NA 3.85 ## 7 8.7 1.4 0.383 15.3 NA 20.5 ## 8 7 NA NA 17 NA 0.045 ## 9 10.1 2.9 0.333 13.9 0.07 14 ## 10 3 NA NA 21 0.0982 14.8 ## # … with 73 more rows Challenge Use select(where()) to find only non-numeric columns Select the columns that include “sleep” in their names msleep %&gt;% select(contains(&quot;sleep&quot;)) ## # A tibble: 83 x 3 ## sleep_total sleep_rem sleep_cycle ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.1 NA NA ## 2 17 1.8 NA ## 3 14.4 2.4 NA ## 4 14.9 2.3 0.133 ## 5 4 0.7 0.667 ## 6 14.4 2.2 0.767 ## 7 8.7 1.4 0.383 ## 8 7 NA NA ## 9 10.1 2.9 0.333 ## 10 3 NA NA ## # … with 73 more rows Select the columns that include either “sleep” or “wt” in their names Basic R way grepl is one of the R base pattern matching functions. msleep[grepl(&#39;sleep|wt&#39;, names(msleep))] ## # A tibble: 83 x 5 ## sleep_total sleep_rem sleep_cycle brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.1 NA NA NA 50 ## 2 17 1.8 NA 0.0155 0.48 ## 3 14.4 2.4 NA NA 1.35 ## 4 14.9 2.3 0.133 0.00029 0.019 ## 5 4 0.7 0.667 0.423 600 ## 6 14.4 2.2 0.767 NA 3.85 ## 7 8.7 1.4 0.383 NA 20.5 ## 8 7 NA NA NA 0.045 ## 9 10.1 2.9 0.333 0.07 14 ## 10 3 NA NA 0.0982 14.8 ## # … with 73 more rows Challenge Use select(match()) to find columns whose names include either “sleep” or “wt”. Select the columns that starts with “b” msleep %&gt;% select(starts_with(&quot;b&quot;)) ## # A tibble: 83 x 2 ## brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 NA 50 ## 2 0.0155 0.48 ## 3 NA 1.35 ## 4 0.00029 0.019 ## 5 0.423 600 ## 6 NA 3.85 ## 7 NA 20.5 ## 8 NA 0.045 ## 9 0.07 14 ## 10 0.0982 14.8 ## # … with 73 more rows Select the columns that ends with “wt” msleep %&gt;% select(ends_with(&quot;wt&quot;)) ## # A tibble: 83 x 2 ## brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 NA 50 ## 2 0.0155 0.48 ## 3 NA 1.35 ## 4 0.00029 0.019 ## 5 0.423 600 ## 6 NA 3.85 ## 7 NA 20.5 ## 8 NA 0.045 ## 9 0.07 14 ## 10 0.0982 14.8 ## # … with 73 more rows Select the columns using both beginning and end string patterns The key idea is you can use Boolean operators (!, &amp;, |)to combine different string pattern matching statements. msleep %&gt;% select(starts_with(&quot;b&quot;) &amp; ends_with(&quot;wt&quot;)) ## # A tibble: 83 x 2 ## brainwt bodywt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 NA 50 ## 2 0.0155 0.48 ## 3 NA 1.35 ## 4 0.00029 0.019 ## 5 0.423 600 ## 6 NA 3.85 ## 7 NA 20.5 ## 8 NA 0.045 ## 9 0.07 14 ## 10 0.0982 14.8 ## # … with 73 more rows Select order and move it before everything # By specifying a column msleep %&gt;% select(order, everything()) ## # A tibble: 83 x 11 ## order name genus vore conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Carn… Chee… Acin… carni lc 12.1 NA NA 11.9 ## 2 Prim… Owl … Aotus omni &lt;NA&gt; 17 1.8 NA 7 ## 3 Rode… Moun… Aplo… herbi nt 14.4 2.4 NA 9.6 ## 4 Sori… Grea… Blar… omni lc 14.9 2.3 0.133 9.1 ## 5 Arti… Cow Bos herbi domesticated 4 0.7 0.667 20 ## 6 Pilo… Thre… Brad… herbi &lt;NA&gt; 14.4 2.2 0.767 9.6 ## 7 Carn… Nort… Call… carni vu 8.7 1.4 0.383 15.3 ## 8 Rode… Vesp… Calo… &lt;NA&gt; &lt;NA&gt; 7 NA NA 17 ## 9 Carn… Dog Canis carni domesticated 10.1 2.9 0.333 13.9 ## 10 Arti… Roe … Capr… herbi lc 3 NA NA 21 ## # … with 73 more rows, and 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; Select variables from a character vector. msleep %&gt;% select(any_of(c(&quot;name&quot;, &quot;order&quot;))) %&gt;% colnames() ## [1] &quot;name&quot; &quot;order&quot; Select the variables named in the character + number pattern msleep$week8 &lt;- NA msleep$week12 &lt;- NA msleep$week_extra &lt;- 0 msleep %&gt;% select(num_range(&quot;week&quot;, c(1:12))) ## # A tibble: 83 x 2 ## week8 week12 ## &lt;lgl&gt; &lt;lgl&gt; ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## 7 NA NA ## 8 NA NA ## 9 NA NA ## 10 NA NA ## # … with 73 more rows 4.8 Counting How may countries in each continent? gapminder %&gt;% count(continent) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Let’s arrange the result. # Just add a new argument `sort = TRUE` gapminder %&gt;% count(continent, sort = TRUE) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Asia 396 ## 3 Europe 360 ## 4 Americas 300 ## 5 Oceania 24 # Same as above; How nice! gapminder %&gt;% count(continent) %&gt;% arrange(desc(n)) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Asia 396 ## 3 Europe 360 ## 4 Americas 300 ## 5 Oceania 24 Challenge Count the number of observations per continent as well as year and arrange them with descending order. Let’s take a deeper look at how things work under the hood. tally() works similar to nrow(): Calculate the total number of cases in a dataframe count = group_by() + tally() gapminder %&gt;% tally() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1704 add_tally() = mutate(n = n()) Challenge What does n in the below example represent? gapminder %&gt;% select(continent, country) %&gt;% add_tally() ## # A tibble: 1,704 x 3 ## continent country n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Asia Afghanistan 1704 ## 2 Asia Afghanistan 1704 ## 3 Asia Afghanistan 1704 ## 4 Asia Afghanistan 1704 ## 5 Asia Afghanistan 1704 ## 6 Asia Afghanistan 1704 ## 7 Asia Afghanistan 1704 ## 8 Asia Afghanistan 1704 ## 9 Asia Afghanistan 1704 ## 10 Asia Afghanistan 1704 ## # … with 1,694 more rows add_count Add count as a column # Add count as a column gapminder %&gt;% group_by(continent) %&gt;% add_count(year) ## # A tibble: 1,704 x 7 ## # Groups: continent [5] ## country continent year lifeExp pop gdpPercap n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 33 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 33 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 33 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 33 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 33 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 33 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 33 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 33 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 33 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 33 ## # … with 1,694 more rows Challenge Do the cases 1 and 2 in the below code chunk produce same outputs? If so, why? # Case 1 gapminder %&gt;% group_by(continent, year) %&gt;% count() ## # A tibble: 60 x 3 ## # Groups: continent, year [60] ## continent year n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 1952 52 ## 2 Africa 1957 52 ## 3 Africa 1962 52 ## 4 Africa 1967 52 ## 5 Africa 1972 52 ## 6 Africa 1977 52 ## 7 Africa 1982 52 ## 8 Africa 1987 52 ## 9 Africa 1992 52 ## 10 Africa 1997 52 ## # … with 50 more rows # Case 2 gapminder %&gt;% group_by(continent) %&gt;% count(year) ## # A tibble: 60 x 3 ## # Groups: continent [5] ## continent year n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 1952 52 ## 2 Africa 1957 52 ## 3 Africa 1962 52 ## 4 Africa 1967 52 ## 5 Africa 1972 52 ## 6 Africa 1977 52 ## 7 Africa 1982 52 ## 8 Africa 1987 52 ## 9 Africa 1992 52 ## 10 Africa 1997 52 ## # … with 50 more rows 4.9 Summarizing 4.9.1 Basic Create a summary gapminder %&gt;% group_by(continent) %&gt;% summarise(n = n(), mean_gdp = mean(gdpPercap), sd_gdp = sd(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 4 ## continent n mean_gdp sd_gdp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 624 2194. 2828. ## 2 Americas 300 7136. 6397. ## 3 Asia 396 7902. 14045. ## 4 Europe 360 14469. 9355. ## 5 Oceania 24 18622. 6359. tablea &lt;- gapminder %&gt;% group_by(continent) %&gt;% summarise(n = n(), mean_gdp = mean(gdpPercap), sd_gdp = sd(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) Produce publishable tables # For HTML and LaTeX tablea %&gt;% kableExtra::kable() continent n mean_gdp sd_gdp Africa 624 2193.755 2827.930 Americas 300 7136.110 6396.764 Asia 396 7902.150 14045.373 Europe 360 14469.476 9355.213 Oceania 24 18621.609 6358.983 # For HTML and MS Office suite tablea %&gt;% flextable::flextable() .cl-21addff4{font-family:'Roboto';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;}.cl-21adf336{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;line-height: 1.00;background-color:transparent;}.cl-21adf34a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;line-height: 1.00;background-color:transparent;}.cl-21ae1be0{width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-21ae1bfe{width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-21ae1c08{width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-21ae1c1c{width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-21ae1c26{width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-21ae1c30{width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}continentnmean_gdpsd_gdpAfrica6242193.7552827.930Americas3007136.1106396.764Asia3967902.15014045.373Europe36014469.4769355.213Oceania2418621.6096358.983 4.9.2 Scoped summaries Old way summarise_all() # Create a wide-shaped data example wide_gapminder &lt;- gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% pivot_wider(names_from = country, values_from = gdpPercap) # Apply summarise_all wide_gapminder %&gt;% select(-c(1:4)) %&gt;% summarise_all(mean, na.rm = TRUE) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; summarise_if(): using a logical condition wide_gapminder %&gt;% summarise_if(is.double, mean, na.rm = TRUE) ## # A tibble: 1 x 31 ## lifeExp Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.9 3255. 20412. 19901. 3485. 6384. 9332. ## # … with 24 more variables: `Czech Republic` &lt;dbl&gt;, Denmark &lt;dbl&gt;, ## # Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, ## # Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, ## # Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, ## # Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, ## # Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United ## # Kingdom` &lt;dbl&gt; summarise_at() vars() = select() wide_gapminder %&gt;% summarise_at(vars(-c(1:4)), mean, na.rm = TRUE) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; wide_gapminder %&gt;% summarise_at(vars(contains(&quot;life&quot;)), mean, na.rm = TRUE) ## # A tibble: 1 x 1 ## lifeExp ## &lt;dbl&gt; ## 1 71.9 New way summarise() + across() If you find using summarise_all(), summarise_if() and summarise_at() confusing, here’s a solution: use summarise() with across(). summarise_all() wide_gapminder %&gt;% summarise(across(Albania:`United Kingdom`, mean, na.rm = TRUE)) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; wide_gapminder %&gt;% summarise(across(-c(1:4), mean, na.rm = TRUE)) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; summarise_if() wide_gapminder %&gt;% summarise(across(is.double, mean, na.rm = TRUE)) ## Warning: Predicate functions must be wrapped in `where()`. ## ## # Bad ## data %&gt;% select(is.double) ## ## # Good ## data %&gt;% select(where(is.double)) ## ## ℹ Please update your code. ## This message is displayed once per session. ## # A tibble: 1 x 31 ## lifeExp Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.9 3255. 20412. 19901. 3485. 6384. 9332. ## # … with 24 more variables: `Czech Republic` &lt;dbl&gt;, Denmark &lt;dbl&gt;, ## # Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, ## # Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, ## # Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, ## # Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, ## # Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United ## # Kingdom` &lt;dbl&gt; summarise_at() wide_gapminder %&gt;% summarise(across(-c(1:4), mean, na.rm = TRUE)) ## # A tibble: 1 x 30 ## Albania Austria Belgium `Bosnia and Her… Bulgaria Croatia `Czech Republic` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3255. 20412. 19901. 3485. 6384. 9332. 13920. ## # … with 23 more variables: Denmark &lt;dbl&gt;, Finland &lt;dbl&gt;, France &lt;dbl&gt;, ## # Germany &lt;dbl&gt;, Greece &lt;dbl&gt;, Hungary &lt;dbl&gt;, Iceland &lt;dbl&gt;, Ireland &lt;dbl&gt;, ## # Italy &lt;dbl&gt;, Montenegro &lt;dbl&gt;, Netherlands &lt;dbl&gt;, Norway &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Portugal &lt;dbl&gt;, Romania &lt;dbl&gt;, Serbia &lt;dbl&gt;, `Slovak ## # Republic` &lt;dbl&gt;, Slovenia &lt;dbl&gt;, Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, ## # Switzerland &lt;dbl&gt;, Turkey &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt; wide_gapminder %&gt;% summarise(across(contains(&quot;life&quot;), mean, na.rm = TRUE)) ## # A tibble: 1 x 1 ## lifeExp ## &lt;dbl&gt; ## 1 71.9 wide_gapminder %&gt;% summarise(across(contains(&quot;A&quot;, ignore.case = FALSE))) ## # A tibble: 360 x 2 ## Albania Austria ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1601. NA ## 2 1942. NA ## 3 2313. NA ## 4 2760. NA ## 5 3313. NA ## 6 3533. NA ## 7 3631. NA ## 8 3739. NA ## 9 2497. NA ## 10 3193. NA ## # … with 350 more rows Note that this workshop does not cover creating and manipulating variables using mutate() because many techniques you learned from playing with summarise() can be directly applied to mutate(). Challenge Summarize average GDP of countries whose names starting with alphabet “A”. Turn the summary dataframe into a publishable table using either kableExtra or flextable package. 4.10 Grouping 4.10.1 Grouped summaries Calculate the mean of gdpPercap. gapminder %&gt;% group_by(continent) %&gt;% # summarise(mean_gdp = mean(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent mean_gdp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 2194. ## 2 Americas 7136. ## 3 Asia 7902. ## 4 Europe 14469. ## 5 Oceania 18622. Calculate multiple summary statistics. gapminder %&gt;% group_by(continent) %&gt;% # summarise(mean_gdp = mean(gdpPercap), count = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## continent mean_gdp count ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 2194. 624 ## 2 Americas 7136. 300 ## 3 Asia 7902. 396 ## 4 Europe 14469. 360 ## 5 Oceania 18622. 24 Optional Other summary statistics Measures of spread: median(x), sd(x), IQR(x), mad(x) (the median absolute deviation) # The Interquartile Range = The Difference Between 75t and 25t Percentiles gapminder %&gt;% group_by(continent) %&gt;% # summarise(IQR_gdp = IQR(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent IQR_gdp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 1616. ## 2 Americas 4402. ## 3 Asia 7492. ## 4 Europe 13248. ## 5 Oceania 8072. Measures of rank: min(x), quantile(x, 0.25), max(x) gapminder %&gt;% group_by(continent) %&gt;% # summarise(min_gdp = min(gdpPercap), max_gdp = max(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## continent min_gdp max_gdp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 241. 21951. ## 2 Americas 1202. 42952. ## 3 Asia 331 113523. ## 4 Europe 974. 49357. ## 5 Oceania 10040. 34435. Measures of position: first(x), last(x), nth(x, 2) gapminder %&gt;% group_by(continent) %&gt;% summarise(first_gdp = first(gdpPercap), last_gdp = last(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## continent first_gdp last_gdp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 2449. 470. ## 2 Americas 5911. 11416. ## 3 Asia 779. 2281. ## 4 Europe 1601. 33203. ## 5 Oceania 10040. 25185. gapminder %&gt;% group_by(continent) %&gt;% arrange(gdpPercap) %&gt;% # Adding arrange summarise(first_gdp = first(gdpPercap), last_gdp = last(gdpPercap)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## continent first_gdp last_gdp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 241. 21951. ## 2 Americas 1202. 42952. ## 3 Asia 331 113523. ## 4 Europe 974. 49357. ## 5 Oceania 10040. 34435. Measures of counts: n(x) (all rows), sum(!is.na(x)) (only non-missing rows) = n_distinct(x) gapminder %&gt;% group_by(continent) %&gt;% summarise(ns = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent ns ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Counts and proportions of logical values: sum(condition about x) (the number of TRUEs in x), mean(condition about x) (the proportion of TRUEs in x) gapminder %&gt;% group_by(continent) %&gt;% summarise(rich_countries = mean(gdpPercap &gt; 20000)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent rich_countries ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 0.00481 ## 2 Americas 0.05 ## 3 Asia 0.111 ## 4 Europe 0.261 ## 5 Oceania 0.333 4.11 Nesting 4.11.1 nest The following example comes from R for Data Science by by Garrett Grolemund and Hadley Wickham. How can you run multiple models simultaneously? Using a nested data frame. Grouped data: each row = an observation Nested data: each row = a group Challenge In the following example, why did we use country and continent for nesting variables? nested &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% nest() head(nested) ## # A tibble: 6 x 3 ## # Groups: country, continent [6] ## country continent data ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; nested$data[[1]] ## # A tibble: 12 x 4 ## year lifeExp pop gdpPercap ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 8425333 779. ## 2 1957 30.3 9240934 821. ## 3 1962 32.0 10267083 853. ## 4 1967 34.0 11537966 836. ## 5 1972 36.1 13079460 740. ## 6 1977 38.4 14880372 786. ## 7 1982 39.9 12881816 978. ## 8 1987 40.8 13867957 852. ## 9 1992 41.7 16317921 649. ## 10 1997 41.8 22227415 635. ## 11 2002 42.1 25268405 727. ## 12 2007 43.8 31889923 975. Custom function lm_model &lt;- function(df) { lm(lifeExp ~ year, data = df) } Apply function to the nested data # Apply m_model to the nested data nested &lt;- nested %&gt;% mutate(models = map(data, lm_model)) # Add the list object as a new column head(nested) ## # A tibble: 6 x 4 ## # Groups: country, continent [6] ## country continent data models ## &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 Afghanistan Asia &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 2 Albania Europe &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 3 Algeria Africa &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 4 Angola Africa &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 5 Argentina Americas &lt;tibble [12 × 4]&gt; &lt;lm&gt; ## 6 Australia Oceania &lt;tibble [12 × 4]&gt; &lt;lm&gt; S3 is part of R’s object oriented systems. If you need more information, check this section in Hadley’s Advanced R out. 4.11.2 unnest glance() function from broom package inspects the quality of a statistical model. Additional tips broom::glance(model): for evaluating model quality and/or complexity broom::tidy(model): for extracting each coefficient in the model (the estimates + its variability) broom::augment(model, data): for getting extra values (residuals, and influence statistics) glanced &lt;- nested %&gt;% mutate(glance = map(models, broom::glance)) glanced$glance[[1]] ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.948 0.942 1.22 181. 9.84e-8 1 -18.3 42.7 44.1 ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; unnest() unpacks the list objects stored in glance column glanced %&gt;% unnest(glance) %&gt;% arrange(BIC) # Low to High; Lower BIC indicates a better model fit ## # A tibble: 142 x 16 ## # Groups: country, continent [142] ## country continent data models r.squared adj.r.squared sigma statistic ## &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Sweden Europe &lt;tib… &lt;lm&gt; 0.995 0.995 0.212 2203. ## 2 Switze… Europe &lt;tib… &lt;lm&gt; 0.997 0.997 0.215 3823. ## 3 France Europe &lt;tib… &lt;lm&gt; 0.998 0.997 0.220 4200. ## 4 Canada Americas &lt;tib… &lt;lm&gt; 0.996 0.996 0.249 2757. ## 5 Argent… Americas &lt;tib… &lt;lm&gt; 0.996 0.995 0.292 2246. ## 6 Belgium Europe &lt;tib… &lt;lm&gt; 0.995 0.994 0.293 1822. ## 7 Brazil Americas &lt;tib… &lt;lm&gt; 0.998 0.998 0.326 5111. ## 8 Equato… Africa &lt;tib… &lt;lm&gt; 0.997 0.997 0.329 3184. ## 9 Nether… Europe &lt;tib… &lt;lm&gt; 0.982 0.980 0.348 552. ## 10 Finland Europe &lt;tib… &lt;lm&gt; 0.994 0.993 0.354 1613. ## # … with 132 more rows, and 8 more variables: p.value &lt;dbl&gt;, df &lt;dbl&gt;, ## # logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, ## # nobs &lt;int&gt; glanced %&gt;% unnest(glance) %&gt;% ggplot(aes(continent, BIC)) + geom_jitter(width = 0.5) 4.12 Mapping We tasted a little bit about how map() function works. Let’s dig into it deeper as this family of functions is really useful. For more information, see Rebecca Barter’s wonderful tutorial on the purrr package. In her words, this is “the tidyverse’s answer to apply functions for iteration”. map() function can take a vector (of any type), a list, and a dataframe for input. multiply &lt;- function(x){ x*x } df &lt;- list(first_obs = rnorm(7, 1, sd =1), second_obs = rnorm(7, 2, sd = 2)) # normal distribution Challenge Try map_df(.x = df, .f = multiply) and tell me what’s the difference between the output you got and what you saw earlier. If you want to know more about the power and joy of functional programming in R (e.g., purrr::map()), then please take “How to Automate Repeated Things in R” workshop. 4.13 Joining 4.13.1 Mutating joins Add new variables to one data frame from matching observations in another\" Using a simple toy example is great because it is easy to see how things work in that much narrow context. Toy example # Table 1 x &lt;- tibble(key = c(1:4), val_x = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;)) # Table 2 y &lt;- tibble(key = c(1:5), val_y = c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;y5&quot;)) Inner Join inner_join() keeps the matched values in both tables. If the left table is a subset of the right table, then the result of left_join() is same as inner_join(). Challenge What are going to be the shared keys? inner_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 4 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 Mutating joins Left Join left_join(), right_join() and full_join() are outer join functions. Unlike inner_join(), outer join functions keep observations that appear in at least one of the tables. left_join() keeps only the matched observations in the right table. left_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 4 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 Right Join right_join() does the opposite. right_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 5 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 ## 5 5 &lt;NA&gt; y5 Full Join full_join() keeps the observations from both tables. If they were unmatched, then NAs were recoded in one of the two tables. full_join(x, y) ## Joining, by = &quot;key&quot; ## # A tibble: 5 x 3 ## key val_x val_y ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 ## 5 5 &lt;NA&gt; y5 4.13.2 Filtering joins Filter observations from one data frame based on whether or not they match an observation in the other table. Semi Join In SQL, this type of query is also called subqueries. Filtering without joining # Create the list of the top 10 destinations top_dest &lt;- flights %&gt;% count(dest, sort = TRUE) %&gt;% top_n(10) ## Selecting by n # Filter filtered &lt;- flights %&gt;% filter(dest %in% top_dest$dest) Using semi join: only keep (INCLUDE) the rows that were matched between the two tables joined &lt;- flights %&gt;% semi_join(top_dest) ## Joining, by = &quot;dest&quot; head(filtered == joined) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## [1,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [4,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## [1,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [4,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6,] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## time_hour ## [1,] TRUE ## [2,] TRUE ## [3,] TRUE ## [4,] TRUE ## [5,] TRUE ## [6,] TRUE Anti Join anti_join() dose the opposite. Exclude the rows that were matched between the two tables. Great technique to filter stopwords when you do a computational text analysis. flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(tailnum, sort = TRUE) ## # A tibble: 722 x 2 ## tailnum n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 2512 ## 2 N725MQ 575 ## 3 N722MQ 513 ## 4 N723MQ 507 ## 5 N713MQ 483 ## 6 N735MQ 396 ## 7 N0EGMQ 371 ## 8 N534MQ 364 ## 9 N542MQ 363 ## 10 N531MQ 349 ## # … with 712 more rows "],
["functional-programming.html", "Chapter 5 Automating repeated things 5.1 Why functional programming 5.2 Automote 2 or 2+ tasks 5.3 Automate plotting 5.4 Automate joining 5.5 Make automation slower or faster 5.6 Make error handling easier 5.7 Developing your own data tools", " Chapter 5 Automating repeated things Anything that can be automated should be automated. Do as little as possible by hand. Do as much as possible with functions. - Hadley Wickam 5.1 Why functional programming Setup # Install packages if (!require(&quot;pacman&quot;)) { install.packages(&quot;pacman&quot;) } ## Loading required package: pacman pacman::p_load( tidyverse, # tidyverse pkgs including purrr tictoc, # performance test broom, # tidy modeling glue, # paste string and objects furrr, # parallel processing rvest, # web scraping devtools, # dev tools usethis, # workflow roxygen2, # documentation testthat) # testing 5.1.1 Why map? 5.1.1.1 Objectives How to use purrr to automate workflow in a cleaner, faster, and more extendable way 5.1.1.2 Copy-and-paste programming Copy-and-paste programming, sometimes referred to as just pasting, is the production of highly repetitive computer programming code, as produced by copy and paste operations. It is primarily a pejorative term; those who use the term are often implying a lack of programming competence. It may also be the result of technology limitations (e.g., an insufficiently expressive development environment) as subroutines or libraries would normally be used instead. However, there are occasions when copy-and-paste programming is considered acceptable or necessary, such as for boilerplate, loop unrolling (when not supported automatically by the compiler), or certain programming idioms, and it is supported by some source code editors in the form of snippets. - Wikipedia Example Let’s imagine df is a survey dataset. a, b, c, d = Survey questions -99: non-responses Your goal: replace -99 with NA # Data set.seed(1234) # for reproducibility df &lt;- tibble( &quot;a&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;b&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;c&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE), &quot;d&quot; = sample(c(-99, 1:3), size = 5, replace = TRUE) ) # Copy and paste df$a[df$a == -99] &lt;- NA df$b[df$b == -99] &lt;- NA df$c[df$c == -99] &lt;- NA df$d[df$d == -99] &lt;- NA df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge. Explain why this solution is not very efficient (Hint: If df$a[df$a == -99] &lt;- NA has an error, how are you going to fix it? A solution is not scalable if it’s not automatable. 5.1.1.3 Using a function Let’s recall what’s function in R: input + computation + output If you write a function, you gain efficiency because you don’t need to copy and paste the computation part. ` function(input){ computation return(output) } ` # Function fix_missing &lt;- function(x) { x[x == -99] &lt;- NA x } # Apply function to each column (vector) df$a &lt;- fix_missing(df$a) df$b &lt;- fix_missing(df$b) df$c &lt;- fix_missing(df$c) df$d &lt;- fix_missing(df$d) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 Challenge Why using function is more efficient than 100% copying and pasting? Can you think about a way we can automate the process? Many options for automation in R: for loop, apply family, etc. Here’s a tidy solution comes from purrr package. The power and joy of one-liner. df &lt;- purrr::map_df(df, fix_missing) df ## # A tibble: 5 x 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 3 1 ## 2 3 2 3 1 ## 3 1 NA 1 2 ## 4 1 NA 2 1 ## 5 NA 1 1 3 map() is a higher-order function that applies a given function to each element of a list/vector. This is how map() works. It’s easier to understand with a picture. - Input: Takes a vector/list. - Computation: Calls the function once for each element of the vector - Output: Returns in a list or whatever data format you prefer (e.g., `_df helper: dataframe`) Challenge If you run the code below, what’s going to be the data type of the output? map(df, fix_missing) ## $a ## [1] 3 3 1 1 NA ## ## $b ## [1] 3 2 NA NA 1 ## ## $c ## [1] 3 3 1 2 1 ## ## $d ## [1] 1 1 2 1 3 Why map() is a good alternative to for loop. The Joy of Functional Programming (for Data Science) - Hadley Wickham # Built-in data data(&quot;airquality&quot;) tic() # Placeholder out1 &lt;- vector(&quot;double&quot;, ncol(airquality)) # Sequence variable for (i in seq_along(airquality)) { # # Assign a computation result to each element out1[[i]] &lt;- mean(airquality[[i]], na.rm = TRUE) } toc() ## 0.006 sec elapsed tic() out1 &lt;- airquality %&gt;% map_dbl(mean, na.rm = TRUE) toc() ## 0.002 sec elapsed In short, map() is more readable, faster, and easily extendable with other data science tasks (e.g., wrangling, modeling, and visualization) using %&gt;%. Final point: Why not base R apply family? Short answer: purrr::map() is simpler to write. For instance, map_dbl(x, mean, na.rm = TRUE) = vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1)) 5.1.1.4 Application (many models) One popular application of map() is to run regression models (or whatever model you want to run) on list-columns. No more copying and pasting for running many regression models on subgroups! # Have you ever tried this? lm_A &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_A&quot;)) lm_B &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_B&quot;)) lm_C &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_C&quot;)) lm_D &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_D&quot;)) lm_E &lt;- lm(y ~ x, subset(data, subgroup == &quot;group_E&quot;)) For more information on this technique, read the Many Models subchapter of the R for Data Science. # Function lm_model &lt;- function(df) { lm(Temp ~ Ozone, data = df) } # Map models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, lm_model)) # Map models$ols[1] ## [[1]] ## ## Call: ## lm(formula = Temp ~ Ozone, data = df) ## ## Coefficients: ## (Intercept) Ozone ## 62.8842 0.1629 # Add tidying tidy_lm_model &lt;- purrr::compose( # compose multiple functions broom::tidy, # convert lm objects into tidy tibbles lm_model ) tidied_models &lt;- airquality %&gt;% group_by(Month) %&gt;% nest() %&gt;% # Create list-columns mutate(ols = map(data, tidy_lm_model)) tidied_models$ols[1] ## [[1]] ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 62.9 1.61 39.2 2.88e-23 ## 2 Ozone 0.163 0.0500 3.26 3.31e- 3 5.2 Automote 2 or 2+ tasks 5.2.1 Objectives Learning how to use map2() and pmap() to avoid writing nested loops. 5.2.2 Problem Problem: How can you create something like below? [1] “University = Berkeley | Department = waterbenders” [1] “University = Berkeley | Department = earthbenders” [1] “University = Berkeley | Department = firebenders” [1] “University = Berkeley | Department = airbenders” [1] “University = Stanford | Department = waterbenders” [1] “University = Stanford | Department = earthbenders” [1] “University = Stanford | Department = firebenders” [1] “University = Stanford | Department = airbenders” The most manual way: You can copy and paste eight times. paste(&quot;University = Berkeley | Department = CS&quot;) ## [1] &quot;University = Berkeley | Department = CS&quot; 5.2.3 For loop A slightly more efficient way: using a for loop. Think about which part of the statement is constant and which part varies ( = parameters). Do we need a placeholder? No. We don’t need a placeholder because we don’t store the result of iterations. Challenge: How many parameters do you need to solve the problem below? # Outer loop for (univ in c(&quot;Berkeley&quot;, &quot;Stanford&quot;)) { # Inner loop for (dept in c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;)) { print(paste(&quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept)) } } ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Berkeley | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Berkeley | Department = airbenders&quot; ## [1] &quot;University = Stanford | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Stanford | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; This is not bad, but … n arguments -&gt; n-nested for loops. As a scale of your problem grows, your code gets really complicated. To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs. — Bjarne Stroustrup 5.2.4 map2 &amp; pmap Step 1: Define inputs and a function. Challenge Why are we using rep() to create input vectors? For instance, for univ_list why not just use c(\"Berkeley\", \"Stanford\")? # Inputs (remember the length of these inputs should be identical) univ_list &lt;- rep(c(&quot;Berkeley&quot;, &quot;Stanford&quot;), 4) dept_list &lt;- rep(c(&quot;waterbenders&quot;, &quot;earthbenders&quot;, &quot;firebenders&quot;, &quot;airbenders&quot;), 2) # Function print_lists &lt;- function(univ, dept) { print(paste( &quot;University = &quot;, univ, &quot;|&quot;, &quot;Department = &quot;, dept )) } # Test print_lists(univ_list[1], dept_list[1]) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; Step2: Using map2() or pmap() # 2 arguments map2_output &lt;- map2(univ_list, dept_list, print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; # 3+ arguments pmap_output &lt;- pmap(list(univ_list, dept_list), print_lists) ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; ## [1] &quot;University = Berkeley | Department = waterbenders&quot; ## [1] &quot;University = Stanford | Department = earthbenders&quot; ## [1] &quot;University = Berkeley | Department = firebenders&quot; ## [1] &quot;University = Stanford | Department = airbenders&quot; Challenge Have you noticed that we used a slightly different input for pmap() compared to map() or map2()? What is the difference? 5.3 Automate plotting 5.3.1 Objective Learning how to use map() and glue() to automate creating multiple plots 5.3.2 Problem Making the following data visualization process more efficient. data(&quot;airquality&quot;) airquality %&gt;% ggplot(aes(x = Ozone, y = Solar.R)) + geom_point() + labs( title = &quot;Relationship between Ozone and Solar.R&quot;, y = &quot;Solar.R&quot; ) ## Warning: Removed 42 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Wind)) + geom_point() + labs( title = &quot;Relationship between Ozone and Wind&quot;, y = &quot;Wind&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). airquality %&gt;% ggplot(aes(x = Ozone, y = Temp)) + geom_point() + labs( title = &quot;Relationship between Ozone and Temp&quot;, y = &quot;Temp&quot; ) ## Warning: Removed 37 rows containing missing values (geom_point). 5.3.3 Solution Learn how glue() works. glue() combines strings and objects and it works simpler and faster than paste() or sprintif(). names &lt;- c(&quot;Jae&quot;, &quot;Aniket&quot;, &quot;Avery&quot;) fields &lt;- c(&quot;Political Science&quot;, &quot;Law&quot;, &quot;Public Health&quot;) glue(&quot;{names} studies {fields}.&quot;) ## Jae studies Political Science. ## Aniket studies Law. ## Avery studies Public Health. So, our next step is to combine glue() and map(). Let’s first think about writing a function that includes glue(). Challenge How can you create the character vector of column names? Challenge How can you make ggplot2() take strings as x and y variable names? (Hint: Type ?aes_string()) airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[2])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[2]}&quot;), y = glue(&quot;{names(airquality)[2]}&quot;) ) ## Warning: Removed 42 rows containing missing values (geom_point). The next step is to write an automatic plotting function. Note that in the function argument i (abstract) replaced 2 (specific): abstraction create_point_plot &lt;- function(i) { airquality %&gt;% ggplot(aes_string(x = names(airquality)[1], y = names(airquality)[i])) + geom_point() + labs( title = glue(&quot;Relationship between Ozone and {names(airquality)[i]}&quot;), y = glue(&quot;{names(airquality)[i]}&quot;) ) } The final step is to put the function in map(). map(2:ncol(airquality), create_point_plot) ## [[1]] ## Warning: Removed 42 rows containing missing values (geom_point). ## ## [[2]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[3]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[4]] ## Warning: Removed 37 rows containing missing values (geom_point). ## ## [[5]] ## Warning: Removed 37 rows containing missing values (geom_point). 5.4 Automate joining 5.4.1 Objective Learning how to use reduce() to automate joining multiple dataframes 5.4.2 Problem How can you make joining multiple dataframes more efficient? Note that we will use dplyr::left_join() = merge(x, y, all.x = TRUE). df1 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df2 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) df3 &lt;- tibble( x = sample(1:10, size = 3, replace = TRUE), y = sample(1:10, size = 3, replace = TRUE), z = sample(1:10, size = 3, replace = TRUE) ) 5.4.3 Copy and paste first_join &lt;- left_join(df1, df2) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) second_join &lt;- left_join(first_join, df3) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) second_join ## # A tibble: 3 x 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 5 8 ## 2 4 8 3 ## 3 4 4 4 Challenge Why the above solution is not efficient? 5.4.4 reduce How reduce() works. - Input: Takes a vector of length n - Computation: Calls a function with a pair of values at a time - Output: Returns a vector of length 1 reduced &lt;- reduce(list(df1, df2, df3), left_join) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) reduced ## # A tibble: 3 x 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 5 8 ## 2 4 8 3 ## 3 4 4 4 5.5 Make automation slower or faster 5.5.1 Objectives Learning how to use slowly() and future_ to make automation process either slower or faster 5.5.2 How to make automation slower Scraping 50 pages from a website and you don’t want to overload the server. How can you do that? 5.5.3 For loop 5.5.4 Map walk() works same as map() but doesn’t store its output. If you’re web scraping, one problem with this approach is it’s too fast by human standards. If you want to make the function run slowly … slowly() takes a function and modifies it to wait a given amount of time between each call. - purrr package vignette - If a function is a verb, then a helper function is an adverb (modifying the behavior of the verb). 5.5.5 How to make automation Faster In a different situation, you want to make your function run faster. This is a common situation when you collect and analyze data at large-scale. You can solve this problem using parallel processing. For more on the parallel processing in R, read this review. Parallel processing setup Step1: Determine the number of max workers (availableCores()) Step2: Determine the parallel processing mode (plan()) 5.6 Make error handling easier 5.6.1 Learning objective Learning how to use safely() and possibly() to make error handling easier ### Problem Challenge Explain why we can’t run map(url_lists, read_html) url_lists &lt;- c( &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot;, &quot;https://en.wikipedia.org/wiki/Stanford_University&quot;, &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot;, &quot;https://DLAB&quot; ) map(url_lists, read_html) This is a very simple problem so it’s easy to tell where the problem is. How can you make your error more informative? 5.6.2 Solution 5.6.2.1 Try-catch There are three kinds of messages you will run into, if your code has an error based on the following functions. stop(): errors; Functions must stop. warning(): warnings; Functions may still work. Nonetheless, something is possibly messed up. message(): messages; Some actions happened. The basic logic of try-catch, R’s basic error handling function, works like the following. tryCatch( { map(url_lists, read_html) }, warning = function(w) { &quot;Warning&quot; }, error = function(e) { &quot;Error&quot; }, finally = { &quot;Message&quot; } ) ## [1] &quot;Error&quot; Here’s purrr version of the try-catch mechanism (evaluates code and assigns exception handlers). 5.6.2.2 safely Outputs result: result or NULL error: NULL or error map(url_lists, safely(read_html)) ## [[1]] ## [[1]]$result ## NULL ## ## [[1]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Timeout was reached: [en.wikipedia.org] Connection timed out after 10001 milliseconds&gt; ## ## ## [[2]] ## [[2]]$result ## NULL ## ## [[2]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Timeout was reached: [en.wikipedia.org] Connection timed out after 10002 milliseconds&gt; ## ## ## [[3]] ## [[3]]$result ## NULL ## ## [[3]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Timeout was reached: [en.wikipedia.org] Connection timed out after 10001 milliseconds&gt; ## ## ## [[4]] ## [[4]]$result ## NULL ## ## [[4]]$error ## &lt;simpleError in open.connection(x, &quot;rb&quot;): Could not resolve host: DLAB&gt; The easier way to solve this problem is just avoiding the error. map(url_lists, safely(read_html)) %&gt;% map(&quot;result&quot;) %&gt;% # = map(function(x) x[[&quot;result&quot;]]) = map(~.x[[&quot;name&quot;]]) purrr::compact() # Remove empty elements ## list() 5.6.2.3 possibly What if the best way to solve the problem is not ignoring the error … # If error occurred, &quot;The URL is broken.&quot; will be stored in that element(s). out &lt;- map( url_lists, possibly(read_html, otherwise = &quot;The URL is broken.&quot; ) ) # Let&#39;s find the broken URL. url_lists[out[seq(out)] == &quot;The URL is broken.&quot;] ## [1] &quot;https://en.wikipedia.org/wiki/University_of_California,_Berkeley&quot; ## [2] &quot;https://en.wikipedia.org/wiki/Stanford_University&quot; ## [3] &quot;https://en.wikipedia.org/wiki/Carnegie_Mellon_University&quot; ## [4] &quot;https://DLAB&quot; 5.7 Developing your own data tools 5.7.1 Why developing R packages? Reuse your code Automate your workflow Help others (be part of an open source development community) 5.7.2 Workflow Write code in \\R Document code in \\man (automated by roxygen2 package) devtools::document() Check dependencies in NAMESPACE devtools::update() updates the documentation devtools::check() to see whether your package is ready to be submitted to CRAN Build a package (for more information, read this section in Hadley’s R package development book) devtools::build() (Optional) Test (devtools::test()), teach in \\vignettes, and add data in \\data Distribute the package either via CRAN or GitHub []http://r-pkgs.had.co.nz/diagrams/package-files.png 5.7.3 Developing an R package The 4 required components are necessary to build and distribute a minimally viable R package. The other steps are optional. 5.7.4 Required Components Package \\R: R functions \\man: function documentations DESCRIPTION: provides meta data about the package (e.g., author) LICENSE GNU, MIT, etc. NAMESPACE: package dependencies (to make your package self-contained) README (optional) Setup (DESCRIPTION) # This function creates DESCRIPTION file usethis::create_package(here(&quot;mypkg&quot;)) # Initialize git repo usethis::use_git() # License the package # You can use the MIT license by typing devtools::use_mit_license(&quot;author name&quot;). The function produces MIT license related files (LICENSE, LICENSE.md). use_mit_license(&quot;Jae Yeon Kim&quot;) # Add README (optional) # Makes the package more use-friendly use_readme_md() # Add news (optional) # Helps track changes use_news_md() Write code (R) usethis::use_r(&quot;rbind_mutate&quot;) #&#39; Add two numbers #&#39; #&#39; @param x A number #&#39; @param y A number #&#39; @return The sum of x and y #&#39; @export add &lt;- function(x, y){ x + y } Document (man) # Document # The function creates documentation related files (NAMESPACE, function_name.rd) devtools::document() # Load all devtools::load_all() # Check devtools::check() Organize (NAMESPACE) usethis::use_package(&quot;dplyr&quot;) 5.7.5 Optional Components Test (test) usethis::use_testthat() usethis::use_test(&quot;rbind_mutate&quot;) Add data (data) x &lt;- &quot;Jae&quot; y &lt;- &quot;Sun&quot; z &lt;- &quot;Jane&quot; usethis::use_data(x, y, z, overwrite = TRUE) Teach (vignetts) usethis::use_vignette(&quot;rbind_mutate&quot;) title: &quot;Vignette title&quot; author: &quot;Vignette author&quot; date: &quot;2020-09-30&quot; output: rmarkdown::html_vignette vignette: blah blah You can build a package website using pkgdown # install.packages(&quot;pkgdown&quot;) usethis::use_pkgdown() pkgdown::build_site() A package site includes information on METADATA, Function references, Articles, News, etc. 5.7.6 Building an R package CMD (in the terminal) You can run R commands in the terminal using R CMD. R CMD build mypkg R CMD INSTALL mypkg devtools # Build devtools::build() # Install devtools::install() 5.7.7 Distributing an R package # Version update usethis::use_version() # Spell check usethis::use_spell_check() CRAN (The Comprehensive R Archive Network) R package submission should comply with the CRAN Repository Policy GitHub Push everything to the Git repository (you can do it using command-line interface or RStudio). git add . git commit -m &quot;first push&quot; git push Don’t forget that your repository should be public. I highly recommend connecting GitHub with SSH. For more information, visit this link. git remote set-url origin git@github.com:user/repo "],
["semi-structured-data.html", "Chapter 6 Semi-structured data 6.1 Objectives 6.2 What is semi-structured data? 6.3 Workflow 6.4 HTML/CSS: web scraping 6.5 XML/JSON: social media scraping", " Chapter 6 Semi-structured data 6.1 Objectives Automating the process of turning semi-structured data (input) into structured data (output) 6.2 What is semi-structured data? Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. - Wikipedia Examples: HTML (Hypertext Markup Language) files (e.g., websites) and JSON (JavaScript Object Notation) files (e.g., tweets) Why should we care semi-structured data? Because this is what the data frontier looks like: # of unstructured data &gt; # of semi-structured data &gt; # of structured data There are easy and fast ways to turn semi-structured data into structured data (ideally in a tidy format) using R, Python, and command-line tools. See my own examples (tidyethnicnews and tidytweetjson). 6.3 Workflow Import/connect to a semi-structured file using rvest, jsonlite, xml2, pdftools, tidyjson, etc. Define target elements in the single file and extract them readr package providers parse_ functions that are useful for vector parsing. stringr package for string manipulations (e.g., using regular expressions in a tidy way). Quite useful for parsing PDF files (see this example). rvest package for parsing HTML (R equivalent to beautiful soup in Python) tidyjson package for parsing JSON data Create a list of files (in this case URLs) to parse Write a parsing function Automate parsing process 6.4 HTML/CSS: web scraping 6.5 XML/JSON: social media scraping 6.5.1 API 6.5.1.1 Objectives Learning what kind of social media data are accessible through application programming interfaces (APIs) Review question In the previous session, we learned the difference between semi-structured data and structured data. Can anyone tell us the difference between them? 6.5.1.2 The big picture for digital data collection Input: semi-structured data Output: structured data Process: Getting target data from a remote server The target data is usually huge (&gt;10GB) by the traditional social science standard. Parsing the target data your laptop/database Laptop (sample-parse): Downsamle the large target data and parse it on your laptop. This is just one option to deal with big data in R. It’s a simple strategy as it doesn’t require storing target data in your own database. Database (push-parse): Push the large target data to a database, then explore, select, and filter it. If you were interested in using this option, then check out my SQL for R Users workshop. Sample-Parse. From RStudio. Push-Parse. From RStudio. But what exactly is this target data? When you scrape websites, you mostly deal with HTML (defines a structure of a website), CSS (its style), and JavaScript (its dynamic interactions). When you access social media data through API, you deal with either XML or JSON (major formats for storing and transporting data; they are light and flexible). XML and JSON have tree-like (nested; a root and branches) structures and keys and values (or elements and attributes). If HTML, CSS, and JavaScript are storefronts, then XML and JSON are warehouses. By Andreas Praefcke (Own work), via Wikimedia Commons 6.5.1.3 Opportunities and challenges for parsing social media data This explanation draws on Pablo Barbara’s LSE social media workshop slides. Basic information What is an API?: An interface (you can think of it as something akin to a restaurant menu. API parameters are menu items.) REST (Representational state transfer) API: static information (e.g., user profiles, list of followers and friends) R packages: tweetscores, twitteR, rtweet Streaming API: dynamic information (e..g, new tweets) This streaming data is filtered by (1) keywords, (2) location, and (3) sample (1% of the total tweets) R packages: streamR Status Twitter API is still widely accessible (v2 recently released; new fields available such as conversation threads). Twitter data is unique from data shared by most other social platforms because it reflects information that users choose to share publicly. Our API platform provides broad access to public Twitter data that users have chosen to share with the world. - Twitter Help Center What does this policy mean? If Twitter users don’t share the locations of their tweets (e.g., GPS), you can’t collect them. Facebook API access has become much constrained with the exception of Social Science One since the 2016 U.S. election. YouTube API access is somewhat limited (but you need to check as I’m not updated on this). Upside Legal and well-documented. Web scraping (Wild Wild West) &lt;&gt; API (Big Gated Garden) You have legal but limited access to (growing) big data that can be divided into text, image, and video and transformed into cross-sectional (geocodes), longitudinal (timestamps), and event historical data (hashtags). For more information, see Zachary C. Steinert-Threlkeld’s 2020 APSA Short Course Generating Event Data From Social Media. Social media data are also well-organized, managed, and curated data. It’s easy to navigate because XML and JSON have keys and values. If you find keys, you will find observations you look for. Downside Rate-limited. If you want to access to more and various data than those available, you need to pay for premium access. 6.5.1.4 Next steps If you want to know how to sign up a new Twitter developer account and access Twitter API, then see Steinert-Threlkeld’s APSA workshop slides. If you want to know about how to use tweetscore package, then see Pablo Barbara’s R markdown file for scraping data from Twitter’s REST API 6.5.2 Hydrating 6.5.2.1 Objectives Learning how hydrating works Learning how to use Twarc to communicate with Twitter’s API Review question What are the main two types of Twitter’s API? 6.5.2.2 Hydrating: An Alternative Way to Collect Historical Twitter Data You can collect Twitter data using Twitter’s API or you can hydrate Tweet IDs collected by other researchers. This is a good resource to collect historical Twitter data. Covid-19 Twitter chatter dataset for scientic use by Panacealab Women’s March Dataset by Littman and Park Harvard Dataverse has a number of dehydrated Tweet IDs that could be of interest to social scientists. Dehydrated Tweet IDs 6.5.2.3 Twarc: one solution to (almost) all Twitter’s API problems Why Twarc? A command-line tool and Python library that works for almost every Twitter’s API related problem. It’s really well-documented, tested, and maintained. Twarc documentation covers basic commands. Tward-cloud documentation explains how to collect data from Twitter’s API using Twarc running in Amazon Web Services (AWS). Twarc was developed as part of the Documenting the Now project which was funded by the Mellon Foundation. One ring that rules them all. There’s no reason to be afraid of using a command-line tool and Python library, even though you primarily use R. It’s easy to embed Python code and shell scripts in R Markdown. Even though you don’t know how to write Python code or shell scripts, it’s really useful to know how to integrate them in your R workflow. I assume that you have already installed Python 3. pip3 install twarc 6.5.2.3.1 Applications The following examples are created by the University of Virginia library. 6.5.2.3.1.1 Search Download pre-existing tweets (7-day window) matching certain conditions In command-line, &gt; = Create a file I recommend running the following commands in the terminal because it’s more stable than doing so in R Markdown. You can type commands in the Terminal in R Studio. # Key word twarc search blacklivesmatter &gt; blm_tweets.jsonl # Hashtag twarc search &#39;#blacklivesmatter&#39; &gt; blm_tweets_hash.jsonl # Hashtag + Language twarc search &#39;#blacklivesmatter&#39; --lang en &gt; blm_tweets_hash.jsonl It is really important to save these tweets into a jsonl format; jsonl extension refers to JSON Lines files. This structure is useful for splitting JSON data into smaller chunks, if it is too large. 6.5.2.3.1.2 Filter Download tweets meeting certain conditions as they happen. # Key word twarc filter blacklivesmatter &gt; blm_tweets.jsonl 6.5.2.3.1.3 Sample Use Twitter’s random sample of recent tweets. twarc sample &gt; tweets.jsonl 6.5.2.3.1.4 Hydrate Tweet IDs -&gt; Tweets twarc hydrate tweet_ids.txt &gt; tweets.jsonl 6.5.2.3.1.5 Dehydrate Hydrate &lt;&gt; Dehydrate Tweets -&gt; Tweet IDs twarc dehydrate tweets.jsonl &gt; tweet_ids.txt Challenge Collect tweets contain some key words of your choice using twarc search and save them as tweets.jsonl. Using less command in the terminal, inspect twarc.log. Using less command in the terminal, inspect tweets.json. 6.5.3 Parsing JSON # Install packages if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman pacman::p_load(tidyverse, # tidyverse pkgs including purrr furrr, # parallel processing tictoc, # performance test tcltk, # GUI for choosing a dir path tidyjson) # tidying JSON files ## Install the current development version from GitHub devtools::install_github(&quot;jaeyk/tidytweetjson&quot;, dependencies = TRUE) ## Skipping install of &#39;tidytweetjson&#39; from a github remote, the SHA1 (3ab642b2) has not changed since last install. ## Use `force = TRUE` to force installation library(tidytweetjson) ## Warning: replacing previous import &#39;maps::map&#39; by &#39;purrr::map&#39; when loading ## &#39;tidytweetjson&#39; 6.5.3.1 Objectives Learning chunk and pull strategy Learning how tidyjson works Learning how to apply tidyjson to tweets 6.5.3.2 Chunk and Pull 6.5.3.2.1 Problem What if the size of the Twitter data you downloaded is too big (e.g., &gt;10GB) to do complex wrangling in R? 6.5.3.2.2 Solution Chunk and Pull. From Studio. Step1: Split the large JSON file in small chunks. #Divide the JSON file by 100 lines (tweets) # Linux and Windows (in Bash) $ split -100 search.jsonl # macOS $ gsplit -100 search.jsonl After that, you will see several files appeared in the directory. Each of these files should have 100 tweets or fewer. All of these file names should start with “x”, as in “xaa”. Step 2: Apply the parsing function to each chunk and pull all of these chunks together. # You need to choose a Tweet JSON file filepath &lt;- file.choose() # Assign the parsed result to the `df` object # 11.28 sec elapsed to parse 17,928 tweets tic() df &lt;- jsonl_to_df(filepath) toc() # Setup n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers # You need to designate a directory path where you saved the list of JSON files. # 9.385 sec elapsed to parse 17,928 tweets dirpath &lt;- tcltk::tk_choose.dir() tic() df_all &lt;- tidytweetjson::jsonl_to_df_all(dirpath) toc() 6.5.3.2.3 tidyjson The tidyjson package helps to use tidyverse framework to JSON data. toy example # JSON collection; nested structure + keys and values worldbank[1] ## [1] &quot;{\\&quot;_id\\&quot;:{\\&quot;$oid\\&quot;:\\&quot;52b213b38594d8a2be17c780\\&quot;},\\&quot;boardapprovaldate\\&quot;:\\&quot;2013-11-12T00:00:00Z\\&quot;,\\&quot;closingdate\\&quot;:\\&quot;2018-07-07T00:00:00Z\\&quot;,\\&quot;countryshortname\\&quot;:\\&quot;Ethiopia\\&quot;,\\&quot;majorsector_percent\\&quot;:[{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:46},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:26},{\\&quot;Name\\&quot;:\\&quot;Public Administration, Law, and Justice\\&quot;,\\&quot;Percent\\&quot;:16},{\\&quot;Name\\&quot;:\\&quot;Education\\&quot;,\\&quot;Percent\\&quot;:12}],\\&quot;project_name\\&quot;:\\&quot;Ethiopia General Education Quality Improvement Project II\\&quot;,\\&quot;regionname\\&quot;:\\&quot;Africa\\&quot;,\\&quot;totalamt\\&quot;:130000000}&quot; # Check out keys (objects) worldbank %&gt;% as.tbl_json() %&gt;% gather_object() %&gt;% filter(document.id == 1) ## # A tbl_json: 8 x 3 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id name ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 &quot;{\\&quot;$oid\\&quot;:\\&quot;52b213...&quot; 1 _id ## 2 &quot;\\&quot;2013-11-12T00:...&quot; 1 boardapprovaldate ## 3 &quot;\\&quot;2018-07-07T00:...&quot; 1 closingdate ## 4 &quot;\\&quot;Ethiopia\\&quot;&quot; 1 countryshortname ## 5 &quot;[{\\&quot;Name\\&quot;:\\&quot;Educa...&quot; 1 majorsector_percent ## 6 &quot;\\&quot;Ethiopia Gener...&quot; 1 project_name ## 7 &quot;\\&quot;Africa\\&quot;&quot; 1 regionname ## 8 &quot;130000000&quot; 1 totalamt # Get the values associated with the keys worldbank %&gt;% as.tbl_json() %&gt;% # Turn JSON into tbl_json object enter_object(&quot;project_name&quot;) %&gt;% # Enter the objects append_values_string() %&gt;% # Append the values as_tibble() # To reduce the size of the file ## # A tibble: 500 x 2 ## document.id string ## &lt;int&gt; &lt;chr&gt; ## 1 1 Ethiopia General Education Quality Improvement Project II ## 2 2 TN: DTF Social Protection Reforms Support ## 3 3 Tuvalu Aviation Investment Project - Additional Financing ## 4 4 Gov&#39;t and Civil Society Organization Partnership ## 5 5 Second Private Sector Competitiveness and Economic Diversificati… ## 6 6 Additional Financing for Cash Transfers for Orphans and Vulnerab… ## 7 7 National Highways Interconnectivity Improvement Project ## 8 8 China Renewable Energy Scale-Up Program Phase II ## 9 9 Rajasthan Road Sector Modernization Project ## 10 10 MA Accountability and Transparency DPL ## # … with 490 more rows The following example draws on my tidytweetjson R package. The package applies tidyjson to Tweets. 6.5.3.2.3.1 Individual file jsonl_to_df &lt;- function(file_path){ # Save file name file_name &lt;- strsplit(x = file_path, split = &quot;[/]&quot;) file_name &lt;- file_name[[1]][length(file_name[[1]])] # Import a Tweet JSON file listed &lt;- read_json(file_path, format = c(&quot;jsonl&quot;)) # IDs of the tweets with country codes ccodes &lt;- listed %&gt;% enter_object(&quot;place&quot;) %&gt;% enter_object(&quot;country_code&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(&quot;country_code&quot; = &quot;string&quot;) # IDs of the tweets with location locations &lt;- listed %&gt;% enter_object(&quot;user&quot;) %&gt;% enter_object(&quot;location&quot;) %&gt;% append_values_string() %&gt;% as_tibble() %&gt;% rename(location = &quot;string&quot;) # Extract other key elements from the JSON file df &lt;- listed %&gt;% spread_values( id = jnumber(&quot;id&quot;), created_at = jstring(&quot;created_at&quot;), full_text = jstring(&quot;full_text&quot;), retweet_count = jnumber(&quot;retweet_count&quot;), favorite_count = jnumber(&quot;favorite_count&quot;), user.followers_count = jnumber(&quot;user.followers_count&quot;), user.friends_count = jnumber(&quot;user.friends_count&quot;) ) %&gt;% as_tibble message(paste(&quot;Parsing&quot;, file_name, &quot;done.&quot;)) # Full join outcome &lt;- full_join(ccodes, df) %&gt;% full_join(locations) # Or you can write this way: outcome &lt;- reduce(list(df, ccodes, locations), full_join) # Select outcome %&gt;% select(-c(&quot;document.id&quot;))} 6.5.3.2.3.2 Many files Set up parallel processing. n_cores &lt;- availableCores() - 1 n_cores # This number depends on your computer spec. ## system ## 7 plan(multiprocess, # multicore, if supported, otherwise multisession workers = n_cores) # the maximum number of workers ## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled ## in future (&gt;= 1.13.0) when running R from RStudio, because it is ## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall ## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to ## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more details, ## how to control forked processing or not, and how to silence this warning in ## future R sessions, see ?future::supportsMulticore Parsing in parallel. Review There are at least three ways you can use function + purrr::map(). squared &lt;- function(x){ x*2 } # Named function map(1:3, squared) # Anonymous function map(1:3, function(x){ x *2 }) # Using formula; ~ = formula, .x = input map(1:3,~.x*2) # Create a list of file paths filename &lt;- list.files(dir_path, pattern = &#39;^x&#39;, full.names = TRUE) df &lt;- filename %&gt;% # Apply jsonl_to_df function to items on the list future_map(~jsonl_to_df(.)) %&gt;% # Full join the list of dataframes reduce(full_join, by = c(&quot;id&quot;, &quot;location&quot;, &quot;country_code&quot;, &quot;created_at&quot;, &quot;full_text&quot;, &quot;retweet_count&quot;, &quot;favorite_count&quot;, &quot;user.followers_count&quot;, &quot;user.friends_count&quot;)) # Output df "],
["machine-learning.html", "Chapter 7 High-dimensional data 7.1 Overview 7.2 Dataset 7.3 Workflow 7.4 Tidymodels 7.5 Pre-processing 7.6 Supervised learning 7.7 Unsupervised learning 7.8 Bias and fairness in machine learning 7.9 Resources", " Chapter 7 High-dimensional data 7.1 Overview The rise of high-dimensional data. The new data frontiers in social sciences—text (Gentzkow et al. 2019; Grimmer and Stewart 2013) and and image (Joo and Steinert-Threlkeld 2018)—are all high-dimensional data. 1000 common English words for 30-word tweets: \\(1000^{30}\\) similar to N of atoms in the universe (Gentzkow et al. 2019) Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. “High-dimensional methods and inference on structural and treatment effects.” Journal of Economic Perspectives 28, no. 2 (2014): 29-50. The rise of new approach: statistics + computer science = machine learning Statistical inference \\(y\\) &lt;- some probability models (e.g., linear regression, logistic regression) &lt;- \\(x\\) \\(y\\) = \\(X\\beta\\) + \\(\\epsilon\\) The goal is to estimate \\(\\beta\\) Machine learning \\(y\\) &lt;- unknown &lt;- \\(x\\) \\(y\\) &lt;-&gt; decision trees, neutral nets &lt;-&gt; \\(x\\) For the main idea behind prediction modeling, see Breiman, Leo (Berkeley stat faculty who passed away in 2005). “Statistical modeling: The two cultures (with comments and a rejoinder by the author).” Statistical science 16, no. 3 (2001): 199-231. “The problem is to find an algorithm \\(f(x)\\) such that for future \\(x\\) in a test set, \\(f(x)\\) will be a good predictor of \\(y\\).” “There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.” Algorithmic models, both in theory and practice, has developed rapidly in fields of outside statistics. It can be used on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. - Leo Breiman How ML differs from econometrics? A review by Athey, Susan, and Guido W. Imbens. “Machine learning methods that economists should know about.” Annual Review of Economics 11 (2019): 685-725. Stat: Specifying a target (i.e., an estimand) Fitting a model to data using an objective function (e.g., the sum of squared errors) Reporting point estimates (effect size) and standard errors (uncertainty) Validation by yes-no using goodness-of-fit tests and residual examination ML: Developing algorithms (estimating f(x)) Prediction power not structural/causal parameters Basically, high-dimensional data statistics (N &lt; P) The major problem is to avoid “the curse of dimensionality” (too many features - &gt; overfitting) Validation: out-of-sample comparisons (cross-validation) not in-sample goodness-of-fit measures So, it’s curve-fitting but the primary focus is unseen (test data) not seen data (training data) A quick review on ML lingos for those trained in econometrics Sample to estimate parameters = Training sample Estimating the model = Being trained Regressors, covariates, or predictors = Features Regression parameters = weights Prediction problems = Supervised (some \\(y\\) are known) + Unsupervised (\\(y\\) unknown) How to teach machines. Based on vas3k blog. Many images in this chapter come from vas3k blog. The main types of machine learning. Based on vas3k blog The map of the machine learning universe. Based on vas3k blog Classical machine learning. Based on vas3k blog 7.2 Dataset Heart disease data from UCI One of the popular datasets used in machine learning competitions # Load packages ## CRAN packages pacman::p_load(here, tidyverse, tidymodels, doParallel, # parallel processing patchwork) # arranging ggplots ## Jae&#39;s custom functions source(here(&quot;functions&quot;, &quot;ml_utils.r&quot;)) # Import the dataset data_original &lt;- read_csv(here(&quot;data&quot;, &quot;heart.csv&quot;)) ## Parsed with column specification: ## cols( ## age = col_double(), ## sex = col_double(), ## cp = col_double(), ## trestbps = col_double(), ## chol = col_double(), ## fbs = col_double(), ## restecg = col_double(), ## thalach = col_double(), ## exang = col_double(), ## oldpeak = col_double(), ## slope = col_double(), ## ca = col_double(), ## thal = col_double(), ## target = col_double() ## ) glimpse(data_original) ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, … ## $ sex &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, … ## $ cp &lt;dbl&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, … ## $ trestbps &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, … ## $ chol &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, … ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ restecg &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ thalach &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, … ## $ exang &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ oldpeak &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, … ## $ slope &lt;dbl&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, … ## $ ca &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, … ## $ thal &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ target &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … # Createa a copy data &lt;- data_original theme_set(theme_minimal()) For more information on the Iowa housing data, read Cook (2011). This is one of the famous datastets used in many prediction modeling competitions. 7.3 Workflow Preprocessing Model building Model fitting Model evaluation Model tuning Prediction 7.4 Tidymodels Like tidyverse, tidymodels is a collection of packages. rsample: for data splitting recipes: for pre-processing parsnip: for model building tune: parameter tuning yardstick: for model evaluations workflows: for bundling a pieplne that bundles together pre-processing, modeling, and post-processing requests Why taking a tidyverse approach to machine learning? Benefits Readable code Reusable data structures Extendable code Tidymodels. From RStudio. tidymodels are an integrated, modular, extensible set of packages that implement a framework that facilitates creating predicative stochastic models. - Joseph Rickert@RStudio Currently, 238 models are available The following materials are based on the machine learning with tidymodels workshop I developed for D-Lab. The original workshop was designed by Chris Kennedy and [Evan Muzzall](https://dlab.berkeley.edu/people/evan-muzzall. 7.5 Pre-processing recipes: for pre-processing textrecipes for text pre-processing Step 1: recipe() defines target and predictor variables (ingredients). Step 2: step_*() defines preprocessing steps to be taken (recipe). The list of the preprocessing steps draws on the vignette of the parsnip package. dummy: Also called one-hot encoding zero variance: Removing columns (or features) with a single unique value impute: Imputing missing values decorrelate: Mitigating correlated predictors (e.g., principal component analysis) normalize: Centering and/or scaling predictors (e.g., log scaling) transform: Making predictors symmetric Step 3: prep() prepares a dataset to base each step on. Step 4: bake() applies the pre-processing steps to your datasets. In this course, we focus on two preprocessing tasks. One-hot encoding (creating dummy/indicator variables) # Turn selected numeric variables into factor variables data &lt;- data %&gt;% dplyr::mutate(across(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;), as.factor)) glimpse(data) ## Rows: 303 ## Columns: 14 ## $ age &lt;dbl&gt; 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, … ## $ sex &lt;fct&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, … ## $ cp &lt;fct&gt; 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, … ## $ trestbps &lt;dbl&gt; 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, … ## $ chol &lt;dbl&gt; 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, … ## $ fbs &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, … ## $ restecg &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, … ## $ thalach &lt;dbl&gt; 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, … ## $ exang &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ oldpeak &lt;dbl&gt; 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, … ## $ slope &lt;fct&gt; 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, … ## $ ca &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, … ## $ thal &lt;fct&gt; 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ target &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Imputation # Check missing values map_df(data, ~ is.na(.) %&gt;% sum()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 0 0 0 0 ## # … with 3 more variables: ca &lt;int&gt;, thal &lt;int&gt;, target &lt;int&gt; # Add missing values data$oldpeak[sample(seq(data), size = 10)] &lt;- NA # Check missing values # Check the number of missing values data %&gt;% map_df(~is.na(.) %&gt;% sum()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 0 0 10 0 ## # … with 3 more variables: ca &lt;int&gt;, thal &lt;int&gt;, target &lt;int&gt; # Check the rate of missing values data %&gt;% map_df(~is.na(.) %&gt;% mean()) ## # A tibble: 1 x 14 ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 0 0 0.0330 0 ## # … with 3 more variables: ca &lt;dbl&gt;, thal &lt;dbl&gt;, target &lt;dbl&gt; 7.5.1 Regression setup 7.5.1.1 Outcome variable # Continuous variable data$age %&gt;% class() ## [1] &quot;numeric&quot; 7.5.1.2 Data splitting using random sampling # for reproducibility set.seed(1234) # split split_reg &lt;- initial_split(data, prop = 0.7) # training set raw_train_x_reg &lt;- training(split_reg) # test set raw_test_x_reg &lt;- testing(split_reg) 7.5.1.3 recipe # Regression recipe rec_reg &lt;- raw_train_x_reg %&gt;% # Define the outcome variable recipe(age ~ .) %&gt;% # Median impute oldpeak column step_medianimpute(oldpeak) %&gt;% # Expand &quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, and &quot;thal&quot; features out into dummy variables (indicators). step_dummy(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;)) # Prepare a dataset to base each step on prep_reg &lt;- rec_reg %&gt;% prep(retain = TRUE) # x features train_x_reg &lt;- juice(prep_reg, all_predictors()) test_x_reg &lt;- bake(object = prep_reg, new_data = raw_test_x_reg, all_predictors()) # y variables train_y_reg &lt;- juice(prep_reg, all_outcomes())$age %&gt;% as.numeric() test_y_reg &lt;- bake(prep_reg, raw_test_x_reg, all_outcomes())$age %&gt;% as.numeric() # Checks names(train_x_reg) # Make sure there&#39;s no age variable! ## [1] &quot;trestbps&quot; &quot;chol&quot; &quot;fbs&quot; &quot;restecg&quot; &quot;thalach&quot; &quot;exang&quot; ## [7] &quot;oldpeak&quot; &quot;target&quot; &quot;sex_X1&quot; &quot;ca_X1&quot; &quot;ca_X2&quot; &quot;ca_X3&quot; ## [13] &quot;ca_X4&quot; &quot;cp_X1&quot; &quot;cp_X2&quot; &quot;cp_X3&quot; &quot;slope_X1&quot; &quot;slope_X2&quot; ## [19] &quot;thal_X1&quot; &quot;thal_X2&quot; &quot;thal_X3&quot; class(train_y_reg) # Make sure this is a continuous variable! ## [1] &quot;numeric&quot; Note that other imputation methods are also available. grep(&quot;impute&quot;, ls(&quot;package:recipes&quot;), value = TRUE) ## [1] &quot;step_bagimpute&quot; &quot;step_knnimpute&quot; ## [3] &quot;step_lowerimpute&quot; &quot;step_meanimpute&quot; ## [5] &quot;step_medianimpute&quot; &quot;step_modeimpute&quot; ## [7] &quot;step_rollimpute&quot; &quot;tunable.step_bagimpute&quot; ## [9] &quot;tunable.step_knnimpute&quot; &quot;tunable.step_meanimpute&quot; ## [11] &quot;tunable.step_rollimpute&quot; You can also create your own step_ functions. For more information, see tidymodels.org. 7.5.2 Classification setup 7.5.2.1 Outcome variable data$target %&gt;% class() ## [1] &quot;numeric&quot; data$target &lt;- as.factor(data$target) data$target %&gt;% class() ## [1] &quot;factor&quot; 7.5.2.2 Data splitting using stratified random sampling # split split_class &lt;- initial_split(data %&gt;% mutate(target = as.factor(target)), prop = 0.7, strata = target) # training set raw_train_x_class &lt;- training(split_class) # testing set raw_test_x_class &lt;- testing(split_class) 7.5.2.3 recipe # Classification recipe rec_class &lt;- raw_train_x_class %&gt;% # Define the outcome variable recipe(target ~ .) %&gt;% # Median impute oldpeak column step_medianimpute(oldpeak) %&gt;% # Expand &quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, and &quot;thal&quot; features out into dummy variables (indicators). step_normalize(age) %&gt;% step_dummy(c(&quot;sex&quot;, &quot;ca&quot;, &quot;cp&quot;, &quot;slope&quot;, &quot;thal&quot;)) # Prepare a dataset to base each step on prep_class &lt;- rec_class %&gt;%prep(retain = TRUE) # x features train_x_class &lt;- juice(prep_class, all_predictors()) test_x_class &lt;- bake(prep_class, raw_test_x_class, all_predictors()) # y variables train_y_class &lt;- juice(prep_class, all_outcomes())$target %&gt;% as.factor() test_y_class &lt;- bake(prep_class, raw_test_x_class, all_outcomes())$target %&gt;% as.factor() # Checks names(train_x_class) # Make sure there&#39;s no target variable! ## [1] &quot;age&quot; &quot;trestbps&quot; &quot;chol&quot; &quot;fbs&quot; &quot;restecg&quot; &quot;thalach&quot; ## [7] &quot;exang&quot; &quot;oldpeak&quot; &quot;sex_X1&quot; &quot;ca_X1&quot; &quot;ca_X2&quot; &quot;ca_X3&quot; ## [13] &quot;ca_X4&quot; &quot;cp_X1&quot; &quot;cp_X2&quot; &quot;cp_X3&quot; &quot;slope_X1&quot; &quot;slope_X2&quot; ## [19] &quot;thal_X1&quot; &quot;thal_X2&quot; &quot;thal_X3&quot; class(train_y_class) # Make sure this is a factor variable! ## [1] &quot;factor&quot; 7.6 Supervised learning x -&gt; f - &gt; y (defined) 7.6.1 OLS and Lasso 7.6.1.1 parsnip Build models (parsnip) Specify a model Specify an engine Specify a mode # OLS spec ols_spec &lt;- linear_reg() %&gt;% # Specify a model set_engine(&quot;lm&quot;) %&gt;% # Specify an engine: lm, glmnet, stan, keras, spark set_mode(&quot;regression&quot;) # Declare a mode: regression or classification # Lasso spec lasso_spec &lt;- linear_reg(penalty = 0.1, # tuning parameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) # If you don&#39;t understand parsnip arguments lasso_spec %&gt;% translate() # See the documentation ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 1 ## ## Computational engine: glmnet ## ## Model fit template: ## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), ## alpha = 1, family = &quot;gaussian&quot;) Fit models ols_fit &lt;- ols_spec %&gt;% fit_xy(x = train_x_reg, y= train_y_reg) # fit(train_y_reg ~ ., train_x_reg) # When you data are not preprocessed lasso_fit &lt;- lasso_spec %&gt;% fit_xy(x = train_x_reg, y= train_y_reg) 7.6.1.2 yardstick Visualize model fits map2(list(ols_fit, lasso_fit), c(&quot;OLS&quot;, &quot;Lasso&quot;), visualize_fit) ## [[1]] ## ## [[2]] # Define performance metrics metrics &lt;- yardstick::metric_set(rmse, mae, rsq) # Evaluate many models evals &lt;- purrr::map(list(ols_fit, lasso_fit), evaluate_reg) %&gt;% reduce(bind_rows) %&gt;% mutate(type = rep(c(&quot;OLS&quot;, &quot;Lasso&quot;), each = 3)) # Visualize the test results evals %&gt;% ggplot(aes(x = fct_reorder(type, .estimate), y = .estimate)) + geom_point() + labs(x = &quot;Model&quot;, y = &quot;Estimate&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free_y&quot;) - For more information, read Tidy Modeling with R by Max Kuhn and Julia Silge. 7.6.1.3 tune 7.6.1.3.1 tune ingredients # tune() = placeholder tune_spec &lt;- linear_reg(penalty = tune(), # tuning parameter mixture = 1) %&gt;% # 1 = lasso, 0 = ridge set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;regression&quot;) tune_spec ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet # penalty() searches 50 possible combinations lambda_grid &lt;- grid_regular(penalty(), levels = 50) # 10-fold cross-validation set.seed(1234) # for reproducibility rec_folds &lt;- vfold_cv(train_x_reg %&gt;% bind_cols(tibble(age = train_y_reg))) 7.6.1.3.2 Add these elements to a workflow # Workflow rec_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_formula(age~.) # Tuning results rec_res &lt;- rec_wf %&gt;% tune_grid( resamples = rec_folds, grid = lambda_grid ) 7.6.1.3.3 Visualize # Visualize rec_res %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, col = .metric)) + geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err ), alpha = 0.3 ) + geom_line(size = 2) + scale_x_log10() + labs(x = &quot;log(lambda)&quot;) + facet_wrap(~glue(&quot;{toupper(.metric)}&quot;), scales = &quot;free&quot;, nrow = 2) + theme(legend.position = &quot;none&quot;) 7.6.1.3.4 Select top_rmse &lt;- show_best(rec_res, metric = &quot;rmse&quot;) best_rmse &lt;- select_best(rec_res, metric = &quot;rmse&quot;) best_rmse ## # A tibble: 1 x 2 ## penalty .config ## &lt;dbl&gt; &lt;chr&gt; ## 1 0.153 Model46 glue(&#39;The RMSE of the intiail model is {evals %&gt;% filter(type == &quot;Lasso&quot;, .metric == &quot;rmse&quot;) %&gt;% select(.estimate) %&gt;% round(2)}&#39;) ## The RMSE of the intiail model is ## 7.87 glue(&#39;The RMSE of the tuned model is {rec_res %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% arrange(mean) %&gt;% dplyr::slice(1) %&gt;% select(mean) %&gt;% round(2)}&#39;) ## The RMSE of the tuned model is 7.71 Finalize your workflow and visualize variable importance finalize_lasso &lt;- rec_wf %&gt;% finalize_workflow(best_rmse) finalize_lasso %&gt;% fit(train_x_reg %&gt;% bind_cols(tibble(age = train_y_reg))) %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.1.3.5 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_lasso %&gt;% fit(test_x_reg %&gt;% bind_cols(tibble(age = test_y_reg))) evaluate_reg(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 7.09 ## 2 mae standard 5.84 ## 3 rsq standard 0.414 7.6.2 Decision tree 7.6.2.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow tree_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec tree_spec &lt;- decision_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters cost_complexity = NULL, tree_depth = NULL) %&gt;% set_engine(&quot;rpart&quot;) # rpart, c5.0, spark tree_wf &lt;- tree_wf %&gt;% add_model(tree_spec) Fit a model tree_fit &lt;- tree_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 7.6.2.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) From wikipedia To learn more about other metrics, check out the yardstick package references. # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) # Visualize tree_fit_viz_metr &lt;- visualize_class_eval(tree_fit) tree_fit_viz_metr tree_fit_viz_mat &lt;- visualize_class_conf(tree_fit) tree_fit_viz_mat 7.6.2.3 tune 7.6.2.3.1 tune ingredients complexity parameter: a high CP means a simple decision tree with few splits. tree_depth tune_spec &lt;- decision_tree( cost_complexity = tune(), tree_depth = tune(), mode = &quot;classification&quot; ) %&gt;% set_engine(&quot;rpart&quot;) tree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = 5) # 2 parameters -&gt; 5*5 = 25 combinations tree_grid %&gt;% count(tree_depth) ## # A tibble: 5 x 2 ## tree_depth n ## &lt;int&gt; &lt;int&gt; ## 1 1 5 ## 2 4 5 ## 3 8 5 ## 4 11 5 ## 5 15 5 # 10-fold cross-validation set.seed(1234) # for reproducibility tree_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.6.2.3.2 Add these elements to a workflow # Update workflow tree_wf &lt;- tree_wf %&gt;% update_model(tune_spec) # Determine the number of cores no_cores &lt;- detectCores() - 1 # Initiate cl &lt;- makeCluster(no_cores) registerDoParallel(cl) # Tuning results tree_res &lt;- tree_wf %&gt;% tune_grid( resamples = tree_folds, grid = tree_grid, metrics = metrics ) 7.6.2.3.3 Visualize The following plot draws on the vignette of the tidymodels package. tree_res %&gt;% collect_metrics() %&gt;% mutate(tree_depth = factor(tree_depth)) %&gt;% ggplot(aes(cost_complexity, mean, col = .metric)) + geom_point(size = 3) + # Subplots facet_wrap(~ tree_depth, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;Cost complexity&quot;, col = &quot;Tree depth&quot;, y = NULL) + coord_flip() ##### Select # Optimal parameter best_tree &lt;- select_best(tree_res, &quot;recall&quot;) # Add the parameter to the workflow finalize_tree &lt;- tree_wf %&gt;% finalize_workflow(best_tree) tree_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (tree_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (tree_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(tree_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance tree_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.2.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.744 ## 2 precision binary 0.705 ## 3 recall binary 0.756 7.6.3 Random forest 7.6.3.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow rand_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec rand_spec &lt;- rand_forest( # Mode mode = &quot;classification&quot;, # Tuning parameters mtry = NULL, # The number of predictors to available for splitting at each node min_n = NULL, # The minimum number of data points needed to keep splitting nodes trees = 500) %&gt;% # The number of trees set_engine(&quot;ranger&quot;, # We want the importance of predictors to be assessed. seed = 1234, importance = &quot;permutation&quot;) rand_wf &lt;- rand_wf %&gt;% add_model(rand_spec) Fit a model rand_fit &lt;- rand_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) 7.6.3.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) # Define performance metrics metrics &lt;- yardstick::metric_set(accuracy, precision, recall) rand_fit_viz_metr &lt;- visualize_class_eval(rand_fit) rand_fit_viz_metr Visualize the confusion matrix. rand_fit_viz_mat &lt;- visualize_class_conf(rand_fit) rand_fit_viz_mat 7.6.3.3 tune 7.6.3.3.1 tune ingredients We focus on the following two parameters: mtry: The number of predictors to available for splitting at each node. min_n: The minimum number of data points needed to keep splitting nodes. tune_spec &lt;- rand_forest( mode = &quot;classification&quot;, # Tuning parameters mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, seed = 1234, importance = &quot;permutation&quot;) rand_grid &lt;- grid_regular(mtry(range = c(1, 10)), min_n(range = c(2, 10)), levels = 5) rand_grid %&gt;% count(min_n) ## # A tibble: 5 x 2 ## min_n n ## &lt;int&gt; &lt;int&gt; ## 1 2 5 ## 2 4 5 ## 3 6 5 ## 4 8 5 ## 5 10 5 # 10-fold cross-validation set.seed(1234) # for reproducibility rand_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.6.3.3.2 Add these elements to a workflow # Update workflow rand_wf &lt;- rand_wf %&gt;% update_model(tune_spec) # Tuning results rand_res &lt;- rand_wf %&gt;% tune_grid( resamples = rand_folds, grid = rand_grid, metrics = metrics ) 7.6.3.3.3 Visualize rand_res %&gt;% collect_metrics() %&gt;% mutate(min_n = factor(min_n)) %&gt;% ggplot(aes(mtry, mean, color = min_n)) + # Line + Point plot geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + # Subplots facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + # Log scale x scale_x_log10(labels = scales::label_number()) + # Discrete color scale scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) + labs(x = &quot;The number of predictors to be sampled&quot;, col = &quot;The minimum number of data points needed for splitting&quot;, y = NULL) + theme(legend.position=&quot;bottom&quot;) # Optimal parameter best_tree &lt;- select_best(rand_res, &quot;accuracy&quot;) best_tree ## # A tibble: 1 x 3 ## mtry min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 2 Model01 # Add the parameter to the workflow finalize_tree &lt;- rand_wf %&gt;% finalize_workflow(best_tree) rand_fit_tuned &lt;- finalize_tree %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (rand_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(rand_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (rand_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(rand_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance rand_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() 7.6.3.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_tree %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.933 ## 2 precision binary 0.973 ## 3 recall binary 0.878 7.6.4 XGboost 7.6.4.1 parsnip Build a model Specify a model Specify an engine Specify a mode # workflow xg_wf &lt;- workflow() %&gt;% add_formula(target~.) # spec xg_spec &lt;- boost_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters # The number of trees to fit, aka boosting iterations trees = c(100, 300, 500, 700, 900), # The depth of the decision tree (how many levels of splits). tree_depth = c(1, 6), # Learning rate: lower means the ensemble will adapt more slowly. learn_rate = c(0.0001, 0.01, 0.2), # Stop splitting a tree if we only have this many obs in a tree node. min_n = 10L ) %&gt;% set_engine(&quot;xgboost&quot;) xg_wf &lt;- xg_wf %&gt;% add_model(xg_spec) Fit a model xg_fit &lt;- xg_wf %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) ## Warning in begin_iteration:end_iteration: numerical expression has 5 elements: ## only the first used 7.6.4.2 yardstick Let’s formally test prediction performance. Metrics accuracy: The proportion of the data predicted correctly precision: Positive predictive value recall (specificity): True positive rate (e.g., healthy people really healthy) metrics &lt;- metric_set(yardstick::accuracy, yardstick::precision, yardstick::recall) evaluate_class(xg_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.733 ## 2 precision binary 0.730 ## 3 recall binary 0.659 xg_fit_viz_metr &lt;- visualize_class_eval(xg_fit) xg_fit_viz_metr Visualize the confusion matrix. xg_fit_viz_mat &lt;- visualize_class_conf(xg_fit) xg_fit_viz_mat 7.6.4.3 tune 7.6.4.3.1 tune ingredients We focus on the following parameters: trees, tree_depth, learn_rate, min_n, mtry, loss_reduction, and sample_size tune_spec &lt;- xg_spec &lt;- boost_tree( # Mode mode = &quot;classification&quot;, # Tuning parameters # The number of trees to fit, aka boosting iterations trees = tune(), # The depth of the decision tree (how many levels of splits). tree_depth = tune(), # Learning rate: lower means the ensemble will adapt more slowly. learn_rate = tune(), # Stop splitting a tree if we only have this many obs in a tree node. min_n = tune(), loss_reduction = tune(), # The number of randomly selected parameters mtry = tune(), # The size of the data set used for modeling within an iteration sample_size = tune() ) %&gt;% set_engine(&quot;xgboost&quot;) # Space-filling parameter grids xg_grid &lt;- grid_latin_hypercube( trees(), tree_depth(), learn_rate(), min_n(), loss_reduction(), sample_size = sample_prop(), finalize(mtry(), train_x_class), size = 30 ) # 10-fold cross-validation set.seed(1234) # for reproducibility xg_folds &lt;- vfold_cv(train_x_class %&gt;% bind_cols(tibble(target = train_y_class)), strata = target) 7.6.4.3.2 Add these elements to a workflow # Update workflow xg_wf &lt;- xg_wf %&gt;% update_model(tune_spec) # Tuning results xg_res &lt;- xg_wf %&gt;% tune_grid( resamples = xg_folds, grid = xg_grid, control = control_grid(save_pred = TRUE) ) 7.6.4.3.3 Visualize xg_res %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;roc_auc&quot;) %&gt;% pivot_longer(mtry:sample_size, values_to = &quot;value&quot;, names_to = &quot;parameter&quot;) %&gt;% ggplot(aes(x = value, y = mean, color = parameter)) + geom_point(alpha = 0.8, show.legend = FALSE) + facet_wrap(~parameter, scales = &quot;free_x&quot;) + labs(y = &quot;AUC&quot;, x = NULL) # Optimal parameter best_xg &lt;- select_best(xg_res, &quot;roc_auc&quot;) best_xg ## # A tibble: 1 x 8 ## mtry trees min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 98 4 13 0.000000211 0.00000000336 0.422 Model26 # Add the parameter to the workflow finalize_xg &lt;- xg_wf %&gt;% finalize_workflow(best_xg) xg_fit_tuned &lt;- finalize_xg %&gt;% fit(train_x_class %&gt;% bind_cols(tibble(target = train_y_class))) # Metrics (xg_fit_viz_metr + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_eval(xg_fit_tuned) + labs(title = &quot;Tuned&quot;)) # Confusion matrix (xg_fit_viz_mat + labs(title = &quot;Non-tuned&quot;)) / (visualize_class_conf(xg_fit_tuned) + labs(title = &quot;Tuned&quot;)) Visualize variable importance xg_fit_tuned %&gt;% pull_workflow_fit() %&gt;% vip::vip() ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. 7.6.4.3.4 Test fit Apply the tuned model to the test dataset test_fit &lt;- finalize_xg %&gt;% fit(test_x_class %&gt;% bind_cols(tibble(target = test_y_class))) evaluate_class(test_fit) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.8 ## 2 precision binary 0.795 ## 3 recall binary 0.756 7.6.5 Applications 7.6.5.1 Bandit algorithm (optimizing an experiment) 7.6.5.2 Causal forest (estimating heterogeneous treatment effect) 7.7 Unsupervised learning x -&gt; f - &gt; y (not defined) 7.7.1 Dimension reduction Projecting 2D-data to a line (PCA). From vas3k.com 7.7.1.1 Correlation analysis Notice some problems? NAs Scaling issues data_original %&gt;% corrr::correlate() ## ## Correlation method: &#39;pearson&#39; ## Missing treated using: &#39;pairwise.complete.obs&#39; ## # A tibble: 14 x 15 ## rowname age sex cp trestbps chol fbs restecg thalach ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age NA -0.0984 -0.0687 0.279 0.214 0.121 -0.116 -0.399 ## 2 sex -0.0984 NA -0.0494 -0.0568 -0.198 0.0450 -0.0582 -0.0440 ## 3 cp -0.0687 -0.0494 NA 0.0476 -0.0769 0.0944 0.0444 0.296 ## 4 trestb… 0.279 -0.0568 0.0476 NA 0.123 0.178 -0.114 -0.0467 ## 5 chol 0.214 -0.198 -0.0769 0.123 NA 0.0133 -0.151 -0.00994 ## 6 fbs 0.121 0.0450 0.0944 0.178 0.0133 NA -0.0842 -0.00857 ## 7 restecg -0.116 -0.0582 0.0444 -0.114 -0.151 -0.0842 NA 0.0441 ## 8 thalach -0.399 -0.0440 0.296 -0.0467 -0.00994 -0.00857 0.0441 NA ## 9 exang 0.0968 0.142 -0.394 0.0676 0.0670 0.0257 -0.0707 -0.379 ## 10 oldpeak 0.210 0.0961 -0.149 0.193 0.0540 0.00575 -0.0588 -0.344 ## 11 slope -0.169 -0.0307 0.120 -0.121 -0.00404 -0.0599 0.0930 0.387 ## 12 ca 0.276 0.118 -0.181 0.101 0.0705 0.138 -0.0720 -0.213 ## 13 thal 0.0680 0.210 -0.162 0.0622 0.0988 -0.0320 -0.0120 -0.0964 ## 14 target -0.225 -0.281 0.434 -0.145 -0.0852 -0.0280 0.137 0.422 ## # … with 6 more variables: exang &lt;dbl&gt;, oldpeak &lt;dbl&gt;, slope &lt;dbl&gt;, ca &lt;dbl&gt;, ## # thal &lt;dbl&gt;, target &lt;dbl&gt; 7.7.1.2 Preprocessing recipe is essential for preprocesssing multiple features at once. pca_recipe &lt;- recipe(~., data = data_original) %&gt;% # Imputing NAs using mean step_meanimpute(all_predictors()) %&gt;% # Normalize some numeric variables step_normalize(c(&quot;age&quot;, &quot;trestbps&quot;, &quot;chol&quot;, &quot;thalach&quot;, &quot;oldpeak&quot;)) 7.7.1.3 PCA analysis pca_res &lt;- pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() pca_res %&gt;% tidy(id = &quot;pca&quot;) ## # A tibble: 196 x 4 ## terms value component id ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 age -0.00101 PC1 pca ## 2 sex 0.216 PC1 pca ## 3 cp 0.321 PC1 pca ## 4 trestbps 0.00118 PC1 pca ## 5 chol -0.000292 PC1 pca ## 6 fbs 0.0468 PC1 pca ## 7 restecg 0.166 PC1 pca ## 8 thalach 0.0137 PC1 pca ## 9 exang 0.0962 PC1 pca ## 10 oldpeak -0.00863 PC1 pca ## # … with 186 more rows 7.7.1.3.1 Screeplot pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() %&gt;% tidy(id = &quot;pca&quot;, type = &quot;variance&quot;) %&gt;% filter(terms == &quot;percent variance&quot;) %&gt;% ggplot(aes(x = component, y = value)) + geom_col() + labs(x = &quot;PCAs of heart disease&quot;, y = &quot;% of variance&quot;, title = &quot;Scree plot&quot;) 7.7.1.3.2 View factor loadings pca_recipe %&gt;% step_pca(all_predictors(), id = &quot;pca&quot;) %&gt;% # id argument identifies each PCA step prep() %&gt;% tidy(id = &quot;pca&quot;) %&gt;% filter(component %in% c(&quot;PC1&quot;, &quot;PC2&quot;)) %&gt;% ggplot(aes(x = fct_reorder(terms, value), y = value, fill = component)) + geom_col(position = &quot;dodge&quot;) + coord_flip() + labs(x = &quot;Terms&quot;, y = &quot;Contribtutions&quot;, fill = &quot;PCAs&quot;) 7.7.2 Topic modeling 7.7.2.1 Setup pacman::p_load(tidytext, # tidy text analysis glue, # paste string and objects stm, # structural topic modeling gutenbergr) # toy datasets 7.7.2.2 Dataset The data munging process draws on Julia Silge’s blog post. sherlock_raw &lt;- gutenberg_download(1661) ## Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest ## Using mirror http://aleph.gutenberg.org glimpse(sherlock_raw) ## Rows: 12,648 ## Columns: 2 ## $ gutenberg_id &lt;int&gt; 1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661, 16… ## $ text &lt;chr&gt; &quot;THE ADVENTURES OF SHERLOCK HOLMES&quot;, &quot;&quot;, &quot;by&quot;, &quot;&quot;, &quot;SIR … sherlock &lt;- sherlock_raw %&gt;% # Mutate story using a conditional statement mutate(story = ifelse(str_starts(text, &quot;ADVENTURE&quot;), text, NA)) %&gt;% # Fill in missing values with next value tidyr::fill(story, .direction = &quot;down&quot;) %&gt;% # Filter filter(story != &quot;THE ADVENTURES OF SHERLOCK HOLMES&quot;) %&gt;% # Factor mutate(story = factor(story, levels = unique(story))) sherlock &lt;- sherlock[,2:3] 7.7.2.3 Key ideas Topics as distributions of words Documents as distributions of topics What distributions? Probability Multinominal (e.g., Latent Dirichlet Distribution) Words lie on a lower dimensional space (dimension reduction) Co-occurrence of words (clustering) Bag of words (feature engineering) Upside: easy and fast (also quite working well) Downside: ignored grammatical structures and rich interactions among words (Alternative: word embeddings. Please check out text2vec) 7.7.2.4 Exploratory data analysis sherlock_n &lt;- sherlock %&gt;% unnest_tokens(output = word, input = text) %&gt;% count(story, word, sort = TRUE) sherlock_total_n &lt;- sherlock_n %&gt;% group_by(story) %&gt;% summarise(total = sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) sherlock_words &lt;- sherlock_n %&gt;% left_join(sherlock_total_n) ## Joining, by = &quot;story&quot; sherlock_words %&gt;% mutate(freq = n/total) %&gt;% group_by(story) %&gt;% top_n(10) %&gt;% ggplot(aes(x = fct_reorder(word, freq), y = freq, fill = story)) + geom_col() + coord_flip() + facet_wrap(~story, ncol = 2, scales = &quot;free_y&quot;) + scale_fill_viridis_d() + labs(x = &quot;&quot;) ## Selecting by freq 7.7.2.5 STM 7.7.2.5.1 Turn text into document-term matrix stm package has its own preprocessing function. dtm &lt;- textProcessor(documents = sherlock$text, metadata = sherlock, removestopwords = TRUE, verbose = FALSE) 7.7.2.5.2 Tuning K K is the number of topics. Let’s try K = 5, 10, 15. test_res &lt;- searchK(dtm$documents, dtm$vocab, K = c(5, 10, 15), prevalence =~ story, data = dtm$meta) ## Beginning Spectral Initialization ## Calculating the gram matrix... ## Finding anchor words... ## ..... ## Recovering initialization... ## ........................................................ ## Initialization complete. ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 1 (approx. per word bound = -7.570) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 2 (approx. per word bound = -7.481, relative change = 1.176e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 3 (approx. per word bound = -7.400, relative change = 1.090e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 4 (approx. per word bound = -7.381, relative change = 2.581e-03) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 5 (approx. per word bound = -7.375, relative change = 8.290e-04) ## Topic 1: come, time, might, know, must ## Topic 2: one, said, well, matter, came ## Topic 3: holm, upon, said, may, just ## Topic 4: littl, will, man, see, think ## Topic 5: case, yes, remark, noth, put ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 6 (approx. per word bound = -7.371, relative change = 4.271e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 7 (approx. per word bound = -7.370, relative change = 2.175e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 8 (approx. per word bound = -7.369, relative change = 7.176e-05) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Model Converged ## Beginning Spectral Initialization ## Calculating the gram matrix... ## Finding anchor words... ## .......... ## Recovering initialization... ## ........................................................ ## Initialization complete. ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 1 (approx. per word bound = -7.651) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 2 (approx. per word bound = -7.480, relative change = 2.234e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 3 (approx. per word bound = -7.378, relative change = 1.356e-02) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 4 (approx. per word bound = -7.354, relative change = 3.251e-03) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 5 (approx. per word bound = -7.347, relative change = 1.040e-03) ## Topic 1: holm, one, will, eye, across ## Topic 2: may, upon, case, shall, remark ## Topic 3: well, know, father, call, mind ## Topic 4: littl, see, like, yes, might ## Topic 5: said, man, see, watson, hard ## Topic 6: hous, room, hand, man, without ## Topic 7: come, came, even, back, tell ## Topic 8: upon, day, door, look, may ## Topic 9: said, ask, matter, noth, answer ## Topic 10: sherlock, hand, glanc, howev, anoth ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 6 (approx. per word bound = -7.344, relative change = 3.512e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 7 (approx. per word bound = -7.342, relative change = 2.888e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 8 (approx. per word bound = -7.340, relative change = 2.464e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 9 (approx. per word bound = -7.339, relative change = 1.991e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 10 (approx. per word bound = -7.338, relative change = 1.230e-04) ## Topic 1: holm, one, will, eye, chair ## Topic 2: may, case, shall, say, remark ## Topic 3: know, well, never, thought, cri ## Topic 4: littl, yes, like, might, see ## Topic 5: said, man, see, can, watson ## Topic 6: hous, room, young, ladi, without ## Topic 7: come, came, back, even, just ## Topic 8: upon, door, day, open, look ## Topic 9: ask, matter, must, take, noth ## Topic 10: hand, howev, sherlock, glanc, head ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 11 (approx. per word bound = -7.337, relative change = 5.576e-05) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Model Converged ## Beginning Spectral Initialization ## Calculating the gram matrix... ## Finding anchor words... ## ............... ## Recovering initialization... ## ........................................................ ## Initialization complete. ## .................................................................................................... ## Completed E-Step (3 seconds). ## Completed M-Step. ## Completing Iteration 1 (approx. per word bound = -7.737) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 2 (approx. per word bound = -7.500, relative change = 3.056e-02) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 3 (approx. per word bound = -7.388, relative change = 1.494e-02) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 4 (approx. per word bound = -7.356, relative change = 4.344e-03) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 5 (approx. per word bound = -7.345, relative change = 1.583e-03) ## Topic 1: holm, know, well, can, might ## Topic 2: small, lay, known, form, power ## Topic 3: time, seen, two, sherlock, anyth ## Topic 4: said, man, shall, miss, word ## Topic 5: look, face, man, first, eye ## Topic 6: room, man, must, open, took ## Topic 7: back, ask, came, way, come ## Topic 8: littl, said, noth, last, sir ## Topic 9: now, see, yes, like, think ## Topic 10: street, hand, found, sudden, pass ## Topic 11: someth, sure, went, mind, live ## Topic 12: come, call, busi, gentleman, may ## Topic 13: will, may, case, littl, find ## Topic 14: one, side, two, year, famili ## Topic 15: upon, tabl, finger, depend, wrist ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 6 (approx. per word bound = -7.340, relative change = 5.712e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 7 (approx. per word bound = -7.338, relative change = 2.704e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 8 (approx. per word bound = -7.336, relative change = 2.640e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 9 (approx. per word bound = -7.335, relative change = 1.952e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 10 (approx. per word bound = -7.333, relative change = 2.106e-04) ## Topic 1: holm, know, well, can, might ## Topic 2: small, work, told, lay, fire ## Topic 3: time, howev, day, sherlock, turn ## Topic 4: said, man, miss, young, word ## Topic 5: look, face, eye, tell, first ## Topic 6: room, must, open, father, made ## Topic 7: back, came, door, ask, way ## Topic 8: littl, noth, last, paper, read ## Topic 9: now, see, think, yes, like ## Topic 10: found, street, hand, sudden, light ## Topic 11: went, place, someth, mind, son ## Topic 12: come, away, busi, call, reason ## Topic 13: will, may, case, find, interest ## Topic 14: one, side, year, thing, two ## Topic 15: upon, tabl, besid, finger, bear ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 11 (approx. per word bound = -7.332, relative change = 2.243e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 12 (approx. per word bound = -7.331, relative change = 1.713e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 13 (approx. per word bound = -7.330, relative change = 1.282e-04) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Completing Iteration 14 (approx. per word bound = -7.329, relative change = 1.167e-04) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 15 (approx. per word bound = -7.328, relative change = 6.326e-05) ## Topic 1: holm, know, well, can, might ## Topic 2: small, work, told, lay, fire ## Topic 3: time, day, much, howev, turn ## Topic 4: said, man, miss, say, right ## Topic 5: look, face, eye, tell, first ## Topic 6: must, room, open, made, morn ## Topic 7: door, came, back, ask, way ## Topic 8: littl, noth, last, paper, sir ## Topic 9: now, see, think, yes, like ## Topic 10: found, street, hand, long, light ## Topic 11: place, went, someth, mind, son ## Topic 12: come, away, busi, call, certain ## Topic 13: will, may, case, find, take ## Topic 14: one, side, year, thing, two ## Topic 15: upon, tabl, besid, sever, finger ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 16 (approx. per word bound = -7.328, relative change = 5.296e-05) ## .................................................................................................... ## Completed E-Step (1 seconds). ## Completed M-Step. ## Completing Iteration 17 (approx. per word bound = -7.328, relative change = 3.663e-05) ## .................................................................................................... ## Completed E-Step (2 seconds). ## Completed M-Step. ## Model Converged 7.7.2.5.3 Evaludating models There are several metrics to assess the performance of topic models: the held-out likelihood, residuals, semantic coherence, and exclusivity. In this course, we examine the relationship between semantic coherence and exclusivity to understand the trade-off involved in selecting K. test_res$results %&gt;% unnest(K, exclus, semcoh) %&gt;% dplyr::select(K, exclus, semcoh) %&gt;% mutate(K = as.factor(K)) %&gt;% ggplot(aes(x = exclus, y = semcoh)) + geom_text(label = glue(&quot;K = {test_res$results$K}&quot;), size = 5, color = &quot;red&quot;) ## Warning: unnest() has a new interface. See ?unnest for details. ## Try `df %&gt;% unnest(c(K, exclus, semcoh))`, with `mutate()` if needed 7.7.2.5.4 Finalize final_stm &lt;- stm(dtm$documents, dtm$vocab, K = 10, prevalence =~ story, max.em.its = 75, data = dtm$meta, init.type=&quot;Spectral&quot;, seed = 1234567, verbose = FALSE) 7.7.2.5.5 Explore the results Using the stm pacakge. # plot plot(final_stm) Using ggplot2 # tidy tidy_stm &lt;- tidy(final_stm) # top terms tidy_stm %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(term, beta), beta, fill = as.factor(topic))) + geom_col(alpha = 0.8, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free_y&quot;) + coord_flip() + scale_y_continuous(labels = scales::percent) + scale_fill_viridis_d() 7.8 Bias and fairness in machine learning 7.9 Resources 7.9.1 Books An Introduction to Statistical Learning - with Applications in R (2013) by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. Amazon or free PDF. Hands-On Machine Learning with R (2020) by Bradley Boehmke &amp; Brandon Greenwell. CRC Press or Amazon Applied Predictive Modeling (2013) by Max Kuhn and Kjell Johnson. Springer: New York. Amazon Feature Engineering and Selection: A Practical Approach for Predictive Models (2019) by Kjell Johnson and Max Kuhn. Taylor &amp; Francis. Amazon or free HTML. Tidy Modeling with R (2020) by Max Kuhn and Julia Silge (work-in-progress) 7.9.2 Lecture slides An introduction to supervised and unsupervised learning (2015) by Susan Athey and Guido Imbens Introduction Machine Learning with the Tidyverse by Alison Hill 7.9.3 Blog posts “Using the recipes package for easy pre-processing” by Rebecca Barter "],
["big-data.html", "Chapter 8 Big data 8.1 Motivation 8.2 What is SQL? 8.3 Learning objectives 8.4 Quick overview 8.5 Setup 8.6 Packages 8.7 Datasets 8.8 Workflow 8.9 References", " Chapter 8 Big data 8.1 Motivation Big data problem: data is too big to fit into memory (=local environment). R reads data into random-access memory (RAM) at once and this object lives in memory entirely. So, object &gt; memory will crash R. So, the key to deal with big data in R is reducing the size of data you want to bring into it. Techniques to deal with big data Medium sized file (1-2 GB) Try to reduce the size of the file using slicing and dicing Tools: R:data.table::fread(file path, select = c(\"column 1\", \"column 2\")). This command imports data faster than read.csv() does. Command line: csvkit - a suite of command-line tools to and working with CSV Large file (&gt; 2-10 GB) Put the data into a database and ACCESS it Explore the data and pull the objects of interest Types of databases Relational database = a collection of tables (fixed columns and rows): SQL is a staple tool to define and query (focus of the workshop today) this type of database Non-relational database = a collection of documents (MongoDB), key-values (Redis and DyanoDB), wide-column stores (Cassandra and HBase), or graph (Neo4j and JanusGraph). This type of database does not preclude SQL. Note that NoSQL stands for “not only SQL.” Relational database Relational Database. Source: MySQL Tutorial 8.2 What is SQL? Structured Query Language. Called SEQUEL and developed by IBM Corporation in the 1970s. Remains the standard language for a relational database management system. It’s a DECLARATIVE language (what to do &gt; how to do) Database management systems figures optimal way to execute query (query optimization) SELECT COLUMN FROM TABLE 8.3 Learning objectives Embracing a new mindset: shifting from ownership (opening CSVs in your laptop) to access (accessing data stored in a database) Learning how to use R and SQL to access and query a database 8.4 Quick overview SQL and R SQL R SELECT select() for columns, mutate() for expressions, summarise() for aggregates FROM which data frame WHERE filter() GROUP BY group_by() HAVING filter() after group_by() ORDER BY arrange() LIMIT head() Challenge 1 1. Can you tell me the difference in the order in which the following R and SQL code were written to wrangle data? For instance, in R, what command comes first? In contrast, in SQL, what command comes first? R example data %&gt;% # Data select() %&gt;% # Column filter() %&gt;% # Row group_by() %&gt;% # Group by summarise(n = n()) %&gt;% # Aggregation filter() %&gt;% # Row order_by() # Arrange SQL example SELECT column, aggregation (count())` # Column FROM data # Data WHERE condition # Row GROUP BY column # Group by HAVING condition # Row ORDER BY column # Arrange SQL Zine by by Julia Evans 8.5 Setup Let’s get to work. 8.6 Packages pacman::p_load() reduces steps for installing and loading several packages simultaneously. # pacman if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman # The rest of pkgs pacman::p_load( tidyverse, # tidyverse packages DBI, # using SQL queries RSQLite, # SQLite dbplyr, # use database with dplyr glue, # glue to automate workflow nycflights13 # toy data ) 8.7 Datasets The flight on-time performance data from the Bureau of Transpiration Statistics of the U.S. government. The data goes back to 1987 and its size is more than 20 gigabytes. For practice, we only use a small subset of the original data (flight data departing NYC in 2013) provided by RStudio. From RStudio. 8.8 Workflow Create/connect to a database Note that server also can be your laptop (called localhost). Short answer: To do so, you need interfaces between R and a database. We use RSQLite in this tutorial because it’s easy to set up. Long answer: The DBI package in R provides a client-side interface that allows dplyr to work with databases. DBI is automatically installed when you installed dbplyr. However, you need to install a specific backend engine (a tool for communication between R and a database management system) for the database (e.g., RMariaDB, RPostgres, RSQLite). In this workshop, we use SQLite because it is the easiest to get started with. Personally, I love PostgreSQL because it’s an open-source and also powerful to do many amazing things (e.g., text mining, geospatial analysis). Copy a table to the database Option 1: You can create a table and insert rows manually. Table Collection of rows Collection of columns (fields or attributes) Each col has a type: String: VARCHAR(20) Integer: INTEGER Floating-point: FLOAT, DOUBLE Date/time: DATE, TIME, DATETIME Schema: the structure of the database The table name The names and types of its columns Various optional additional information (e.g., constraints) CREATE TABLE students ( id INT AUTO_INCREMENT, name VARCHAR(30), birth DATE, gpa FLOAT, grad INT, PRIMARY KEY(id)); INSERT INTO students(name, birth, gpa, grad) VALUES (&#39;Adam&#39;, &#39;2000-08-04&#39;, 4.0, 2020); Option 2: Copy a file (object) to a table in a database using copy_to). We take this option as it’s fast and we would like to focus on querying in this workshop. Query the table Main focus Pull the results of interests (data) using collect() Disconnect the database 8.8.1 Create a database # Define a backend engine drv &lt;- RSQLite::SQLite() # Create an empty in-memory database con &lt;- DBI::dbConnect(drv, dbname = &quot;:memory:&quot;) # Connect to an existing database #con &lt;- DBI::dbConnect(RMariaDB::MariaDB(), # host = &quot;database.rstudio.com&quot;, # user = &quot;hadley&quot;, # password = rstudioapi::askForPassword(&quot;Database password&quot;) #) dbListTables(con) ## character(0) Note that con is empty at this stage. 8.8.2 Copy an object as a table to the database (push) # Copy objects to the data copy_to(dest = con, df = flights) copy_to(dest = con, df = airports) copy_to(dest = con, df = planes) copy_to(dest = con, df = weather) # If you want you can also decide what columns you want to copy: # copy_to(dest = con, # df = flights, # name = &quot;flights&quot;, # indexes = list(c(&quot;year&quot;, &quot;tailnum&quot;, &quot;dest&quot;))) # Show two tables in the database dbListTables(con) ## [1] &quot;airports&quot; &quot;flights&quot; &quot;planes&quot; &quot;sqlite_stat1&quot; &quot;sqlite_stat4&quot; ## [6] &quot;weather&quot; # Show the columns/attributes/fields of a table dbListFields(con, &quot;flights&quot;) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; ## [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot; &quot;arr_time&quot; &quot;sched_arr_time&quot; ## [9] &quot;arr_delay&quot; &quot;carrier&quot; &quot;flight&quot; &quot;tailnum&quot; ## [13] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; ## [17] &quot;hour&quot; &quot;minute&quot; &quot;time_hour&quot; dbListFields(con, &quot;weather&quot;) ## [1] &quot;origin&quot; &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;hour&quot; ## [6] &quot;temp&quot; &quot;dewp&quot; &quot;humid&quot; &quot;wind_dir&quot; &quot;wind_speed&quot; ## [11] &quot;wind_gust&quot; &quot;precip&quot; &quot;pressure&quot; &quot;visib&quot; &quot;time_hour&quot; 8.8.3 Quick demonstrations: SELECT desired columns FROM tables Select all columns (*) from flights table and show the first ten rows Note that you can combine SQL and R commands thanks to dbplyr. Option 1 DBI::dbGetQuery(con, &quot;SELECT * FROM flights;&quot;) %&gt;% head(10) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 7 19 B6 507 N516JB EWR FLL 158 1065 6 0 ## 8 -14 EV 5708 N829AS LGA IAD 53 229 6 0 ## 9 -8 B6 79 N593JB JFK MCO 140 944 6 0 ## 10 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## time_hour ## 1 1357034400 ## 2 1357034400 ## 3 1357034400 ## 4 1357034400 ## 5 1357038000 ## 6 1357034400 ## 7 1357038000 ## 8 1357038000 ## 9 1357038000 ## 10 1357038000 Option 2 (works faster) SELECT * FROM flights LIMIT 5 Table 8.1: 5 records year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest air_time distance hour minute time_hour 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 1357034400 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 5 29 1357034400 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 5 40 1357034400 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN 183 1576 5 45 1357034400 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 6 0 1357038000 Option 3 (automating workflow) When local variables are updated, the SQL query is also automatically updated. # Local variables tbl &lt;- &quot;flights&quot; var &lt;- &quot;dep_delay&quot; num &lt;- 5 # Glue SQL query string # Note that to indicate a numeric value, you don&#39;t need `` sql_query &lt;- glue_sql(&quot; SELECT {`var`} FROM {`tbl`} LIMIT {num} &quot;, .con = con) # Run the query dbGetQuery(con, sql_query) ## dep_delay ## 1 2 ## 2 4 ## 3 2 ## 4 -1 ## 5 -6 Challenge 2 Can you rewrite the above code using LIMIT instead of head(10) You may notice that using only SQL code makes querying faster. Select dep_delay and arr_delay from flights table, show the first ten rows, then turn the result into a tibble. Challenge 3 Could you remind me how to see the list of attributes of a table? Let’s say you want to see the attributes of flights table. Collect the selected columns and filtered rows df &lt;- dbGetQuery(con, &quot;SELECT dep_delay, arr_delay FROM flights;&quot;) %&gt;% head(10) %&gt;% collect() Counting rows Count all (*) dbGetQuery(con, &quot;SELECT COUNT(*) FROM flights;&quot;) ## COUNT(*) ## 1 336776 dbGetQuery(con, &quot;SELECT COUNT(dep_delay) FROM flights;&quot;) ## COUNT(dep_delay) ## 1 328521 Count distinct values dbGetQuery(con, &quot;SELECT COUNT(DISTINCT dep_delay) FROM flights;&quot;) ## COUNT(DISTINCT dep_delay) ## 1 527 8.8.4 Tidy-way: dplyr -&gt; SQL Thanks to the dbplyr package you can use the dplyr syntax to query SQL. Note that pipe (%) works. # tbl select tables flights &lt;- con %&gt;% tbl(&quot;flights&quot;) airports &lt;- con %&gt;% tbl(&quot;airports&quot;) planes &lt;- con %&gt;% tbl(&quot;planes&quot;) weather &lt;- con %&gt;% tbl(&quot;weather&quot;) select = SELECT flights %&gt;% select(contains(&quot;delay&quot;)) ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.30.1 [:memory:] ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # … with more rows Challenge 4 Your turn: write the same code in SQL mutate = SELECT AS flights %&gt;% select(distance, air_time) %&gt;% mutate(speed = distance / (air_time / 60)) ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.30.1 [:memory:] ## distance air_time speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400 227 370. ## 2 1416 227 374. ## 3 1089 160 408. ## 4 1576 183 517. ## 5 762 116 394. ## 6 719 150 288. ## 7 1065 158 404. ## 8 229 53 259. ## 9 944 140 405. ## 10 733 138 319. ## # … with more rows Challenge 5 Your turn: write the same code in SQL (hint: mutate(new_var = var 1 * var2 = SELECT var1 * var2 AS near_var) filter = WHERE flights %&gt;% filter(month == 1, day == 1) ## # Source: lazy query [?? x 19] ## # Database: sqlite 3.30.1 [:memory:] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with more rows, and 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dbl&gt; Challenge 6 Your turn: write the same code in SQL (hint: filter(condition1, condition2) = WHERE condition1 and condition2) Note that R and SQL operators are not exactly alike. R uses != for Not equal to. SQL uses &lt;&gt; or !=. Furthermore, there are some cautions about using NULL (NA; unknown or missing): it should be IS NULL or IS NOT NULL not =NULL or !=NULL. arrange = ORDER BY flights %&gt;% arrange(carrier, desc(arr_delay)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT * ## FROM `flights` ## ORDER BY `carrier`, `arr_delay` DESC Challenge 7 Your turn: write the same code in SQL (hint: arrange(var1, desc(var2)) = ORDER BY var1, var2 DESC) summarise = SELECT AS and group by = GROUP BY flights %&gt;% group_by(month, day) %&gt;% summarise(delay = mean(dep_delay)) ## Warning: Missing values are always removed in SQL. ## Use `mean(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. ## # Source: lazy query [?? x 3] ## # Database: sqlite 3.30.1 [:memory:] ## # Groups: month ## month day delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 11.5 ## 2 1 2 13.9 ## 3 1 3 11.0 ## 4 1 4 8.95 ## 5 1 5 5.73 ## 6 1 6 7.15 ## 7 1 7 5.42 ## 8 1 8 2.55 ## 9 1 9 2.28 ## 10 1 10 2.84 ## # … with more rows Challenge 8 Your turn: write the same code in SQL (hint: in SQL the order should be SELECT group_var1, group_var2, AVG(old_var) AS new_var -&gt; FROM -&gt; GROUP BY) If you feel too much challenged, here’s a help. flights %&gt;% group_by(month, day) %&gt;% summarise(delay = mean(dep_delay)) %&gt;% show_query() # Show the SQL equivalent! ## &lt;SQL&gt; ## SELECT `month`, `day`, AVG(`dep_delay`) AS `delay` ## FROM `flights` ## GROUP BY `month`, `day` Joins Using joins is simpler in R than it is in SQL. However, more flexible joins exist in SQL and they are not available in R. Joins involving 3+ tables are not supported. Some advanced joins available in SQL are not supported. For more information, check out tidyquery to see the latest developments. flights %&gt;% left_join(weather, by = c(&quot;year&quot;, &quot;month&quot;)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT `LHS`.`year` AS `year`, `LHS`.`month` AS `month`, `LHS`.`day` AS `day.x`, `LHS`.`dep_time` AS `dep_time`, `LHS`.`sched_dep_time` AS `sched_dep_time`, `LHS`.`dep_delay` AS `dep_delay`, `LHS`.`arr_time` AS `arr_time`, `LHS`.`sched_arr_time` AS `sched_arr_time`, `LHS`.`arr_delay` AS `arr_delay`, `LHS`.`carrier` AS `carrier`, `LHS`.`flight` AS `flight`, `LHS`.`tailnum` AS `tailnum`, `LHS`.`origin` AS `origin.x`, `LHS`.`dest` AS `dest`, `LHS`.`air_time` AS `air_time`, `LHS`.`distance` AS `distance`, `LHS`.`hour` AS `hour.x`, `LHS`.`minute` AS `minute`, `LHS`.`time_hour` AS `time_hour.x`, `RHS`.`origin` AS `origin.y`, `RHS`.`day` AS `day.y`, `RHS`.`hour` AS `hour.y`, `RHS`.`temp` AS `temp`, `RHS`.`dewp` AS `dewp`, `RHS`.`humid` AS `humid`, `RHS`.`wind_dir` AS `wind_dir`, `RHS`.`wind_speed` AS `wind_speed`, `RHS`.`wind_gust` AS `wind_gust`, `RHS`.`precip` AS `precip`, `RHS`.`pressure` AS `pressure`, `RHS`.`visib` AS `visib`, `RHS`.`time_hour` AS `time_hour.y` ## FROM `flights` AS `LHS` ## LEFT JOIN `weather` AS `RHS` ## ON (`LHS`.`year` = `RHS`.`year` AND `LHS`.`month` = `RHS`.`month`) 8.8.5 Collect (pull) collect() is used to pull the data. Depending on the data size, it may take a long time to run. The following code won’t work. Error in UseMethod(“collect”) : no applicable method for ‘collect’ applied to an object of class “c(‘LayerInstance’, ‘Layer’, ‘ggproto’, ‘gg’)” origin_flights_plot &lt;- flights %&gt;% group_by(origin) %&gt;% tally() %&gt;% ggplot() + geom_col(aes(x = origin, y = n)) %&gt;% collect() This works. df &lt;- flights %&gt;% group_by(origin) %&gt;% tally() %&gt;% collect() origin_flights_plot &lt;- ggplot(df) + geom_col(aes(x = origin, y = n)) origin_flights_plot 8.8.6 Disconnect DBI::dbDisconnect(con) 8.9 References csv2db - for loading large CSV files in to a database R Studio, Database using R Ian Cook, “Bridging the Gap between SQL and R” rstudio::conf 2020 slides Video recording Data Carpentry contributors, SQL database and R, Data Carpentry, September 10, 2019. Introduction to dbplyr Josh Erickson, SQL in R, STAT 701, University of Michigan SQL zine by Julia Evans q - a command line tool that allows direct execution of SQL-like queries on CSVs/TSVs (and any other tabular text files) "]
]
