---
title: "Machine Learning in R"
author: "Chris Kennedy and Evan Muzzall"
date: "4/17/2018"
output:
  html_document:
    toc: yes
    toc_float: yes
---

# Introduction 
Be sure to check out the D-Lab [Machine Learning Working Group](http://dlab.berkeley.edu/working-groups/machine-learning-working-group-0) to learn more about machine learning in both R and Python, give a lightning talk, lead a coding walkthrough, and participate in other projects.  

### 1. Package installation
```{r, eval = F}
if (FALSE) {
  # Run this line manually to install the necessary packages.
install.packages(c("car", "caret","chemometrics", "class", "devtools", "doParallel", "gbm", "ggplot2", "gmodels", "pROC", "randomForest", "RhpcBLASctl", "rpart", "rpart.plot", "SuperLearner"))
}
  
library(car)
library(caret)
library(chemometrics)
library(class)
library(doParallel)
library(gbm)
library(ggplot2)
library(gmodels)
library(parallel)
library(pROC)
library(randomForest)
library(RhpcBLASctl)
library(rpart)
library(rpart.plot)
library(SuperLearner)
```

### 1. Machine learning
Machine learning evolved from scientific pursuits in statistics, computational/information theory, artificial intelligence, and pattern recognition.  

How to define machine learning?  

 * Algorithms, computers, and other machines that can "learn" without direct input from a programmer.  
 * A suite of tools for investigating/modeling/understanding complex datasets.  

### 2.  Supervised machine learning

Selecting a machine learning algorithm depends on the characteristics of the problem being investigated. Machine learning is generally divided into three broad classes of learning: [supervised](https://en.wikipedia.org/wiki/Supervised_learning), [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning), and [reinforcement](https://en.wikipedia.org/wiki/Reinforcement_learning). In this workshop we will focus on two main subtypes of supervised machine learning: classification and regression.  

Supervised machine learning algorithms learn a target function _f_ that best maps to a dependent/response/target/outcome variable `Y` based on independent/input/predictor variable(s) `X1, X2, X3,... Xn` from a set of [training data](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets). The algorithm then tries to predict `Y` using new test data for the `X` variables. It is then evaluated using some sort of performance metric. These predictions are influenced by bias, variance, reducible error, and irreducible [errors](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).  

More simply, supervised machine learning algorithms estimate an output using inputs by training a model on known data. We hope that this model is consistent when it tried to predict on new data. This takes the form $Y = f(X) + \epsilon$. Y is the dependent/response/outcome/target variable, X is the independent/input/predictor variable(s). The function _f_ relates X to Y but this is unknown and we must estimate it. Epsilon $\epsilon$ is the random error, is independent of X, and averages to zero.  

*Machine learning definition revisited: more specifically, we could think of machine learning as a bunch of methods to estimate $f$ Thus, we can predict Y using $\hat{y} = \hat{f}(X)$. See [An Introduction to Statistical Learning, Chapters 1 and 2](http://www-bcf.usc.edu/~gareth/ISL/) to learn more.  

##### 3.1 Classification or regression?
**Classification** is used when the `Y` variable is categorical/discrete. Binary examples generally refer to a yes/no situation where a `1` is prediction of the "yes" category and `0` as the "no". Classification models the probability that the outcome variable is 1 based on the covariates: $Pr(Y = 1 | X)$. This can be extended to multi-level classification as well.  

**Regression** is used when the target Y variable is continuous. Regression models the conditional expectation (conditional mean) of the outcome variable given the covariates: $E[Y \mid X]$.  

### 4.  Data preprocessing and performance metrics
##### 4.1 Data preprocessing
A longstanding first step is to split a dataset into **"training"** and **"test"** subsets. A training dataset usually consists of a majority portion of the original dataset so that an algorithm can learn the model. The remaining portion of the dataset is designated to the test dataset to evaluate model performance on data the model has not yet seen.  

##### 4.2 Model performance
**Performance metrics** are used to see how well a model predicts a specified outcome on training and test datasets.  

A model that performs poorly on the training dataset is **underfit** because it is not able to discern relationships between `X` and `Y` variables.  

A model that performs well on the training dataset but poorly on the test dataset is saide to be **overfit** because the model performed poorly when given new data - the patterns found in the test data simply might not exist in the test data.  

##### 4.3 Common performance metrics

  * Accuracy  
  * Mean squared error  
  * True positives and true negatives (https://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity)
  * Area under the ROC curve (AUC)  

### 5.  Workshop goals

##### 5.1 General goals

  * Learn a few basics and practice coding individual machine learning algorithms in R  
  * Examine model performance  
  * Construct ensembles to simultaneously compare multiple algorithms  
  * Visualize outputs  

##### 5.2 Specific goals

We ask the following questions:
1) **Binary classification examples:** did a woman participate in the United States labor force in the year 1975? (yes = 1, no = 0)  
2) **Regression example:** what does a prediction for the continuous "expected log wage rate" variable look like?  

# Algorithm walkthroughs
Load the Mroz and iris datasets
```{r}
library(car)
# load the Mroz dataset
data(Mroz) 

# view basic info
str(Mroz) 

# read variable descriptions
?Mroz 

# also load iris dataset for challenge questions
data(iris)
str(iris)
?iris
```

# 1.  k-nearest neighbor (knn)
The k-nearest neighbor (knn) makes no assumptions about the underlying distribution of the data. To classify a data point, knn uses the characteristics of the points around it to determine how to classify it.  

[Euclidean distances]((https://en.wikipedia.org/wiki/Euclidean_distance)) between points are used in this example. 
`k` is the number of neighbors used to classify the point in question. Choosing a proper "k" is integral to finding the best-performing model. *Large k-values* could be bad because the class with the largest size might win regardless of influence of more proximate points. *Small k-values* might also be bad because neighboring points might become overly influential.  

##### 1.1 knn - data preprocessing
First, convert categorical factors to numeric indicators using the `model.matrix` function. This is called "one-hot encoding" and functinos like `lm` and `glm` do this internally. This makes it easier to code a variety of algorithms to a dataset as many algorithms handle factors poorly (decision trees are the main exception).  
```{r}
# View Mroz variable names
names(Mroz)

# Assign outcome to its own vector.
Y <- Mroz$lfp

# Remove Y from the Mroz dataframe and name this new dataframe "data"
data <- subset(Mroz, select = -lfp)
colnames(data)

# Convert factors to indicators; this will add an unneeded X.Intercept variable:
data <- data.frame(model.matrix( ~ ., data)) 
names(data)

# Remove the extra intercept column.
data <- data[, -1]
names(data)

# view variable distributions
summary(data)
```

> Remember! 
- `Y` contains the `Mroz$lfp` variable.  
- `data` contains the predictor variables but not `Mroz$lfp`.  

##### 1.2 knn - split the dataset
Now we can split the dataset into training and test sets. Assign 70% of the data to training dataset and the remaining 30% to the test dataset.  
```{r}
library(caret)

# set seed for reproducibility
set.seed(1)

# Create a stratified random split
split <- createDataPartition(Y, p=0.70, list=FALSE) 
split

train_X <- data[split, ] # partition training dataset
test_X <- data[-split, ] # partition test dataset

train_label <- Y[split] # partition training Y vector labels
test_label <- Y[-split] # partition test Y vector labels
```

##### 1.3 knn - selecting a starting "k-value"
Different methods exist for choosing a start point for "k". Let's use the square root of the number of rows in the training datset: 
```{r}
round(sqrt(nrow(train_X)), digits=2) # 22.98
```

Fit the model! Our goal is to predict the classification accuracy of our `Y` variable (yes/no whether or not a woman participated in the workforce) based on the other `X` predictor variables:
```{r}
library(class)
set.seed(1)
data_predicted <- knn(train = train_X, test = test_X,
                     cl = train_label, k = 23, prob = TRUE)
```

Using accuracy as our performance metric, compute a contingency table to see how well the model predicted yes and no:
```{r}
library(gmodels)
CrossTable(x = test_label, y = data_predicted, 
           prop.chisq = F,
           prop.r = F,
           prop.c = F,
           prop.t = F)

# Compute accuracy
mean(test_label == data_predicted) # ~ 0.52
```
How did it do?  

##### 1.4 knn - improving performance metrics
Two common ways of improving model performance are to 1) standardize the data so that scale does not unduly influence classification, and 2) change the k-value. 

##### 1.4.1 knn - improving performance metrics - scale the data
Scaling the data is useful so that variables with large values do not bias the prediction. Create a new copy of the "data" dataset called "data_scaled", which will contain scaled values with means equal to 0 and standard deviations equal to 1. Our `Y` variable remains unchanged:
```{r}
data_scaled <- scale(data, center = TRUE, scale = TRUE)

# Look at the new distribution.
summary(data_scaled)
```

Repeat the process:  
1) split the scaled data  
```{r}
# Use "split" to create the training partition.
set.seed(1)
train_scaled <- data_scaled[split, ] 

# Create the test partition.
test_scaled <- data_scaled[-split, ] 

# Extract outcome data for training set.
train_label_scaled <- Y[split] 

# Extract outcome data for test set.
test_label_scaled <- Y[-split] 
```

2) fit the model on the scaled data  
```{r}
library(class)
set.seed(1)
data_predicted_scaled <- knn(train = train_scaled,
                            test = test_scaled, 
                            cl = train_label_scaled,
                            k = 23, prob = TRUE)
```

3) examine the accuracy of predictions on the scaled data:
```{r}
library(gmodels)
CrossTable(x = test_label_scaled, y = data_predicted_scaled, 
           prop.chisq = F,
           prop.r = F,
           prop.c = F,
           prop.t = F)

# Compute accuracy
mean(test_label_scaled == data_predicted_scaled) # 0.68
```
What happened?  

##### 1.4.2 knn - improving performance metrics - change k-value
It also helps to investigate the cross-validated errors for multiple k-values at once to see which is ideal. Plot the cross-validated errors:
```{r}
library(chemometrics)
set.seed(1)
knn_k <- knnEval(data_scaled, Y, split, 
                 knnvec=seq(1, 51, by = 1), 
                 legpo="bottomright", las = 3)
```
A combination of scaling the data and searching for the best cross-validated "k" is ideal.  

### Challenge 1  
1. What happens to accuracy when k = 2, 10, or 20?  
2. What happens when you set k to 17, the optimal value from the cross-validation plot? What happens to the predictive accuracy when you switch k to this value?  
3. Using what you learned above, classify predictive accuracies of the `Species` variable in the  "iris" dataset. 

# 2.  Linear regression
- Ordinary least squares regression (OLS) can be used when the target Y variable is continuous.  
- Remember that under the hood, `lm` and `glm` are one hot encoding factors to indicators. Write it out again for good practice: 
```{r}
# View structure of Mroz.
str(Mroz)

# Define our continuous Y response variable, "lwg". Call it "Y2".
Y2 <- Mroz$lwg

# Expand our predictor X variables out into indicators.
data2 <- data.frame(model.matrix( ~ ., subset(Mroz, select = -lwg)))
names(data2)

# Remove intercept.
data2 <- data2[, -1]

# View structure of "data2".
str(data2)
```

- Fit a model that predicts the `Y` variable log wage gap (`lwg`) using the other variables as predictors.  

- Mean squared error (MSE) will be our performance metric. MSE measures the difference between observed and expected values, with smaller values tending to reflect greater predictive accuracy. 
```{r}
# Fit the regression model.
data2_lm <- lm(Y2 ~ ., data = data2)

# View the regression results.
summary(data2_lm) 

# Predict the outcome back onto the training data.
data2_predicted <- predict(data2_lm, data2) 

# Calculate mean-squared error. 
MSE <- mean((Y2 - data2_predicted)^2)   

MSE
sqrt(MSE)
```
See the [yhat blog for Fitting & Interpreting Linear Models in R](http://blog.yhat.com/posts/r-lm-summary.html) to learn more about `lm` output.  

### Challenge 2
1) What happens to MSE when you try to predict "age" and "lfp"?  
2) Set up a regression model that predicts one of the numeric variables from the "iris" dataset. 

# 3.  Decision trees 

Decision trees are recursive partitioning methods that divides the predictor spaces into simpler regions and can be visualized in a tree-like structure. Decesion trees attempt to classify data by dividing it into subsets according to a `Y` output variable and based on some predictors.  

Let's see how a tree-based method classifies women's participation in the workforce.  

Note that we do not have to use `model.matrix` for our decision tree algorithm here:
```{r}
library(rpart)
# Choose method="anova" for a regression tree
# Instead of gini coefficient, select "information" inside the "parms" parameter
dt <- rpart(Mroz$lfp ~ ., data = subset(Mroz, select = -lfp),
            method = "class",
            parms = list(split = 'information'))

# Here is the text-based display of the decision tree. Yikes! 
print(dt)

# The plot is much easier to interpret.
library(rpart.plot)
rpart.plot(dt, cex = 1, type = 4, extra = 101)
           # shadow.col = "lightgray", 
           
# Variable importance is also informative:
dt$variable.importance
```

In decision trees the main hyperparameter (configuration setting) is the *complexity parameter* (CP). A high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits. So, the name is a little counterintuitive.  

rpart uses cross-validation internally to estimate the accuracy at various complexity parameter (CP) settings. We can review those to see what setting seems best.
```{r}
# Print the results for various CP settings.
# Here we want the setting with the lowest "xerror".
printcp(dt)

# Plot the performance estimates for different CP settings.
# We see that a larger tree (lower CP) seems more accurate.
plotcp(dt)

# We can "prune" a tree to a higher CP setting if we want.
# Note: due to rounding we make the cp value slightly higher than
# was shown in the table.
tree_pruned <- prune(dt, cp = 0.033847)

# Print detailed results, variable importance, and summary of splits.
summary(tree_pruned) 
rpart.plot(tree_pruned)

# Also see the "control" argument inside the rpart function: 
?rpart
```

Here, each node shows the cutoff for each of the predictors and how it contributed to the split. View a helpful decision tree tutorial at [GormAnalysis](https://gormanalysis.com/decision-trees-in-r-using-rpart/).  

### Challenge 3
1. Make a decision tree that tries to classify something other than "lfp".  
2. What are the "minsplit" and "cp" hyperparameters within the "control" parameter? How might you go about figuring out what these are?  
HINT: the syntax might look like this: `dt2_control <- rpart.control(minsplit = 20, cp = 0.001)`  

# 4. Random forests
Random forest is an algorithm that seeks to improve on the performance of a single decision tree by taking the average of many trees. Thus, a random forest is an **ensemble** method, or model averaging approach. The algorithm was invented by Berkeley's own Leo Breiman in 2001, who was also a co-creator of decision trees (see 1984 CART book).

Random forests are an extension of **bagging**, in which multiple samples of the original data are drawn with replacement (aka "bootstrap samples"), an algorithm is fit separately to each sample, then the average of those estimates is used for prediction. While bagging can be used any algorithm, RF uses decision trees as its base learner. Random Forests add another level of randomness by also randomly sampling the features (or covariates) at each split in each decision tree. This makes the decision trees use different covariates and therefore be more unique. As a result, the average of these trees tends to be more accurate overall.

```{r}
# View "Y" and "data"
table(Y)
str(data)

# split this data, similar to before
str(train_X)
str(test_X)
```

Now, let's fit a model that tries to predict the number of women who participated in the labor force in 1975 again using the other variables as our X predictors: 
```{r}
library(randomForest)
set.seed(1)
rf1 <- randomForest(train_label ~ ., 
                    data = train_X, 
                    # Number of trees
                    ntree = 500, 
                    # Number of variables randomly sampled as candidates at each split.
                    mtry = 2, 
                    # We want the importance of predictors to be assessed.
                    importance = TRUE) 

rf1

```
The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$.  OOB stands for "out of bag" - and bag is short for "bootstrap aggregation". So OOB estimates performance by comparing the predicted outcome value to the actual value across all trees using only the observations that were not part of the training data for that tree.

We can examine the relative variable importance in table and graph form. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled (permuted). The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never even chosen as a split in any of the decision trees.
```{r}
# As the function name suggests, this creates a variable importance plot.
varImpPlot(rf1)

# Raw data.
rf1$importance
```
You can read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) if interested. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable cells.  

Now, the goal is to see how the model performs on the test dataset:
```{r}
rf_pred <- predict(rf1, newdata = test_X)
table(rf_pred, test_label)
```

Check the accuracy of the test set:
```{r}
mean(rf_pred == test_label) # 0.7377778
# devtools::install_github("ck37/ck37r")
summary(ck37r::rf_count_terminal_nodes(rf1))
```
How did it do? Are the accuracies for the training and test sets similar?  

### Challenge 4

1. Try a few other values of mtry - can you find one that has improved performance?
2. Maxnodes is another tuning parameter for randomForest - does changing it improve your performance?
3. Select another Y variable to predict and evaluate performance on the train_rf and test_rf. How is the fit of the model for this variable?  

# 5. Boosting

"Boosting is a general method for improving the accuracy of any given learning algorithm" and evolved from AdaBoost and PAC learning (p. 1-2). Gradient boosted machines are ensembles decision tree methods of "weak" trees that are just slightly more accurate than random guessing. These are then "boosted" into "strong" learners. That is, the models don't have to be accurate over the entire feature space.  

The model first tries to predict each value in a dataset - the cases that can be predicted easily are _downweighted_ so that the algorithm does not try as hard to predict them.  

However, the cases that the model has difficulty predicting are _upweighted_ so that the model tries more assertively to predict them. This continues for multiple "boosting iterations", with a training-based performance measure produced at each iteration. This method can drive down generalization error (p. 5). (from [Freund Y, Schapire RE. 1999. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence 14:771-780](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf).  

Remember, our data are still split from before:
```{r}
# view "Y" and "data"
table(Y)
str(data)
nrow(train_X) + nrow(test_X) == nrow(data) # sanity check
```

Rather than testing only a single model at a time, it is useful to compare tuning parameters within that single model. Bootstrap is often a default, but we want cross-validation. We also want to specify different tuning parameters.  

Let us first create two objects - `control` and `grid`. `control` will allow us to tune the cross-validated performance metric, while `grid` lets us evaluate the model with different characteristics: 
```{r}
# Choose 10-fold repeated measure cross-validation as our performance metric
# (instead of the default "bootstrap").
control <- trainControl(method = "repeatedcv", 
	repeats = 2,
	# Calculate class probabilities.
	classProbs = TRUE,
	# Indicate that our response varaible is binary.
	summaryFunction = twoClassSummary) 

grid <- expand.grid(
  # Number of trees to fit, aka boosting iterations.
  n.trees = seq(500L, 1500L, by = 500L),
  # Depth of the decision tree.
	interaction.depth = c(1L, 3L, 5L), 
  # Learning rate: lower means the ensemble will adapt more slowly.
	shrinkage = c(0.001, 0.01, 0.1),
  # Stop splitting a tree if we only have this many obs in a tree node.
	n.minobsinnode = 10L)
grid
```

Fit the model. Note that we are now using the area under the ROC curve as our performance metric. This can be graphed for quick visualization: 
```{r}
library(caret)
library(pROC)
set.seed(1)
label_numeric = as.numeric(train_label == "yes")
table(label_numeric, train_label)

# cbind: caret expects the Y response and X predictors to be part of the same dataframe
gbm1 <- train(train_label ~ ., data = cbind(train_label, train_X), 
	method = "gbm",
	# Use AUC as our performance metric, which caret incorrectly calls "ROC".
	metric = "ROC",
	trControl = control,
	tuneGrid = grid,
	# Keep output more concise.
	verbose = TRUE)

# See how long this algorithm took to complete.
gbm1$times 

# Review model summary table.
gbm1 

# Plot variable importance.
summary(gbm1, las = 2)

# Generate predicted values.
gbm.pred <- predict(gbm1, test_X)

# Generate class probabilities.
gbm.prob <- predict(gbm1, test_X, type = "prob")

# Define ROC characteristics.
rocCurve <- roc(response = test_label,
	predictor = gbm.prob[, "yes"],
	levels = rev(levels(test_label)),
	auc=TRUE, ci=TRUE)

# Plot ROC.
plot(rocCurve, print.thres = "best", main = "GBM", col = "blue") 
# ggsave("gbm AUC.png")

# Plot the cross-validated AUC of the different configurations.
ggplot(gbm1) + theme_bw() + ggtitle("GBM model comparisons") 
# ggsave("gbm tuning comparison.png")
```

Also check out the ["xgboost" R package](https://cran.r-project.org/web/packages/xgboost/index.html) for extreme gradient boosting.  

### Challenge 5

1. Using your Mroz dataset, fit a GBM model where "wc" is your Y variable. What happens to variable relative importance? What happens to AUC?

# 6. Ensemble methods

The ["SuperLearner" R package](https://cran.r-project.org/web/packages/SuperLearner/index.html) is a method that simplifies ensemble learning by allowing you to simultaneously evaluate the cross-validated performance of multiple algorithms and/or a single algorithm with differently tuned hyperparameters.  

Let's see how the five algorithms you learned in this workshop (kNN, linear regression, decision trees, random forest, and gradient boosted machines) compare to each other and to the mean of Y as a benchmark algorithm, in terms of their cross-validated error!

A "wrapper" is a short function that adapts an algorithm for the SuperLearner package. Check out the different algorithm wrappers offered by SuperLearner:
```{r}
library(SuperLearner)
listWrappers()
```

Instead of splitting the data like before, since we are using cross-validation we actually want to use the entire `Mroz` dataset:
```{r}
str(Mroz)

# convert our Y variable to numeric type
Mroz$lfp <- ifelse(Mroz$lfp == "yes", 1, 0)

# Now, tell SuperLearner which algorithms to urn. In the interest of time we are not including GBM, but "SL.gbm" is the wrapper.
SL_library <- c("SL.mean", "SL.knn", "SL.glm", "SL.rpart", "SL.randomForest")

```

Fit the ensemble:

```{r}
library(SuperLearner)
# This is a seed that is compatible with multicore parallel processing.
# See ?set.seed for more information.
set.seed(1, "L'Ecuyer-CMRG") 

# This may take a minute to execute.
cv_sl <- CV.SuperLearner(Y = Mroz$lfp, X = data,
                         SL.library = SL_library,
                         family = binomial(),
                         # Fit only 3 folds for demo purposes only.
                         # For a real analysis we would do 20 folds.
                         cvControl = list(V = 3L),
                         # Set to T to show details.
                         verbose = F)
summary(cv_sl)
```

Risk is a performance estimate - it's the average loss, and loss is how far off the prediction was for an individual observation. The lower the risk, the fewer errors the model makes in its prediction. SuperLearner's default loss metric is squared error $(y_{actual} - y_{predicted})^2$, so the risk is the mean-squared error (just like in ordinary least _squares_ regression). View and plot results:
```{r}
summary(cv_sl)

plot(cv_sl) + theme_bw()

# ggsave("SuperLearner.pdf")
```
"Discrete SL" is when the SuperLearner chooses the single algorithm with the lowest risk. "SuperLearner" is a weighted average of multiple algorithms, or an "ensemble". In theory the weighted-average should have a little better performance, although they often tie. In this case we only have a few algorithms so the difference is minor.

### Challenge 6. 
1. If you set the seed again and re-run `CV.SuperLearner` do you get exactly the same results?
2. Choose your favorite 2 algorithms and re-run the SuperLearner with just those algorithms in the library. Does the performance change?
3. What are the elements of the CV_SL object? Take a look at 1 or 2 of them. Hint: use the `names()` function to list the elements of an object, then `$` to access them (just like a dataframe).

A longer tutorial on SuperLearner is available here: (https://github.com/ck37/superlearner-guide)
