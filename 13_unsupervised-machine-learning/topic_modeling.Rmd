---
title: "Document term frequency in R"
author: "Jae Yeon Kim"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

The following material is adapted from [Text Mining with R](https://proquest.safaribooksonline.com/9781491981641) (2017) by Julia Silge and David Robinson as well as [stm package manual](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) by Margaret E. Roberts, Brandon M. Stewart, and Dustin Tingley, and [stm tutorial](https://juliasilge.com/blog/evaluating-stm/) by again Julia Silge.

We touched on this concept briefly when we discussed tf-idf in the earlier session. It's time for us now to dive into the analysis of word and document frequency more deeply. Then, we move onto the topic analysis.

# Setup
```{r}

rm(list=ls())

# load packages
require(tm) # for text mining
library(tidyverse) # for tidyverse
library(tidytext) # for tidy text analysis
library(quanteda) # for text analysis and examples
library(wordcloud) # for wordcloud
library(stringr) # for string manipulation
library(gutenbergr) #  for text data
library(stm) # for topic modeling
library(drlib)

# load data 
inaug <- tidy(data_corpus_inaugural)
```

# Document-term frequency 

We have already seen this long tail distribution from Zipf's law.

```{r}
# tokenize and count
inaug_words <- inaug %>%
  filter(Year > 1945) %>%
  unnest_tokens(word, text) %>%
  count(President, word, sort = TRUE)

# total nunber of tokens 
inaug_total_words <- inaug_words %>%
  group_by(President) %>%
  summarize(total = sum(n))

# merge
inaug_words <- left_join(inaug_words, inaug_total_words)

# plot
ggplot(inaug_words, aes(x = n/total, fill = President)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~President, ncol = 2, scales = "free_y")
```

# Topic modeling

## Turn text into dfm 

stm package has its own preprocessing tool called textProcessor.

```{r}

inaug_dfm <- textProcessor(documents = inaug$text, metadata = inaug,
                            removestopwords = TRUE,
                            verbose = TRUE)

```

## Find the right number of topics 

- K is the number of topics. 
- Let's try K = 5, 10, 15 this time.
- But also mind that finding K is computationally expensive. If your machine is too slow, then consider using parallel processing.

```{r}

# using searchK function
storage <- searchK(inaug_dfm$documents, inaug_dfm$vocab, K = c(5, 10, 15),
        prevalence =~ Year + President, data = inaug_dfm$meta)

```

## Model metrics 

There are several metrics to assess the performance of topic models: the held-out likelihood, residuals, semantic coherence, and exclusivity. In this unit, we look at the relationship between semantic coherence and exclusivity to understand the trade-off involved in selecting K.

```{r}

# plot the relationship between exclusivity and coherence
storage$results %>%
  select(K, exclus, semcoh) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(x= semcoh, y = exclus, col = K)) +
  geom_point(size = 5, alpha = 0.7) %>%
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence")

# look at the other metrics as well
storage$results %>%
  gather(metrics, value, heldout, residual, semcoh, exclus) %>%
  ggplot(aes(x = K, y = value)) +
  geom_point(size = 3, alpha = 0.3) +
  geom_line(size = 1) +
  facet_wrap(~metrics, scales = "free_y") +
  labs(title = "Diagnostic Values by Number of Topics",
       x = "Number of Topics (K)",
       y = "Value") +
  guides(col=guide_legend(ncol=2))

# let's stick to 10
topic_model <- stm(inaug_dfm$documents, inaug_dfm$vocab, K = 5, prevalence =~ Year+ President,
                        max.em.its = 75, data = inaug_dfm$meta, init.type="Spectral",
                        seed = 1234567,
                        verbose  =TRUE)

```


## Explore the results 
```{r}

# plot
plot(topic_model)

# find relevant articles 
findThoughts(topic_model, texts = inaug$text, topics = 3)$docs[[1]]
```

## Explore the results with ggplot2
```{r}

# tidy  
tidy_text <- tidy(topic_model)

tidy_text

# top terms
tidy_text %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0(topic),
           term = drlib::reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = "Expected topic proportion",
         title = "Top 10 topics by prevalence",
         subtitle = "Highest word probabilities for each topic")+
    scale_y_continuous(labels = scales::percent)
```


