# High-dimensional data {#machine_learning}

## Overview 

- The rise of high-dimensional data. The new data frontiers in social sciences---text ([Gentzkow et al. 2019](https://web.stanford.edu/~gentzkow/research/text-as-data.pdf); [Grimmer and Stewart 2013](https://www.jstor.org/stable/pdf/24572662.pdf?casa_token=SQdSI4R_VdwAAAAA:4QiVLhCXqr9f0qNMM9U75EL5JbDxxnXxUxyIfDf0U8ZzQx9szc0xVqaU6DXG4nHyZiNkvcwGlgD6H0Lxj3y0ULHwgkf1MZt8-9TPVtkEH9I4AHgbTg)) and and image ([Joo and Steinert-Threlkeld 2018](https://arxiv.org/pdf/1810.01544))---are all high-dimensional data. 

    - Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. ["High-dimensional methods and inference on structural and treatment effects."](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.2.29) *Journal of Economic Perspectives 28*, no. 2 (2014): 29-50.

- The rise of new approach: statistics + computer science = machine learning 

- Statistical inference 

    - $y$ <- some probability models (e.g., linear regression, logistic regression) <- $x$
       
    - $y$ = $X\beta$ + $\epsilon$
        
    - The goal is to estimate $\beta$

- Machine learning 

    - $y$ <- unknown <- $x$ 
    
    - $y$ <-> decision trees, neutral nets <-> $x$
        
    - For the main idea behind prediction modeling, see Breiman, Leo (Berkeley stat faculty who passed away in 2005). ["Statistical modeling: The two cultures (with comments and a rejoinder by the author)."](https://projecteuclid.org/euclid.ss/1009213726) *Statistical science* 16, no. 3 (2001): 199-231.
    
    - "The problem is to find an algorithm $f(x)$ such that for future $x$ in a test set, $f(x)$ will be a good predictor of $y$."
    
    - "There are **two cultures** in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a **given** **stochastic data model**. The other uses **algorithmic models** and treats the data mechanism as **unknown**."

> Algorithmic models, both in theory and practice, has developed rapidly in fields of outside statistics. It can be used on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. - Leo Breiman

- How ML differs from econometrics? 

- A review by Athey, Susan, and Guido W. Imbens. ["Machine learning methods that economists should know about."](https://www.annualreviews.org/doi/full/10.1146/annurev-economics-080217-053433) *Annual Review of Economics* 11 (2019): 685-725.
        
- Stat:
  
    - Specifying a target (i.e., an estimand)

    - Fitting a model to data using an objective function (e.g., the sum of squared errors)

    - Reporting point estimates (effect size) and standard errors (uncertainty)

    - Validation by yes-no using goodness-of-fit tests and residual examination

- ML: 

    - Developing algorithms (estimating *f(x)*)

    - Prediction power not structural/causal parameters

    - Basically, high-dimensional data statistics (N < P)

    - The major problem is to avoid ["the curse of dimensionality"](https://en.wikipedia.org/wiki/Curse_of_dimensionality) ([too many features - > overfitting](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e))

    - Validation: out-of-sample comparisons (cross-validation) not in-sample goodness-of-fit measures

    - So, it's curve-fitting but the primary focus is unseen (test data) not seen data (training data)

- A quick review on ML lingos for those trained in econometrics 

    - Sample to estimate parameters = Training sample
  
    - Estimating the model = Being trained 
    - Regressors, covariates, or predictors = Features 
  
    - Regression parameters = weights 
  
    - Prediction problems = Supervised (some $y$ are known) + Unsupervised ($y$ unknown)
    
![How to teach machines. Based on [vas3k blog](https://vas3k.com/blog/machine_learning/). Many images in this chapter come from vas3k blog.](https://i.vas3k.ru/7w9.jpg)

![The main types of machine learning. Based on [vas3k blog](https://vas3k.com/blog/machine_learning/)](https://i.vas3k.ru/7vz.jpg)

![The map of the machine learning universe. Based on [vas3k blog](https://vas3k.com/blog/machine_learning/)](https://i.vas3k.ru/7vx.jpg)

![Classical machine learning. Based on [vas3k blog](https://vas3k.com/blog/machine_learning/)](https://i.vas3k.ru/7w1.jpg)

## Dataset  

```{r}
pacman::p_load(tidymodels)

# Ames, Iowa housing data 
data(ames)

# Glimpse 
ames %>%
  glimpse()
```

- For more information on the Iowa housing data, read [Cook (2011)](http://jse.amstat.org/v19n3/decock.pdf). This is one of the famous datastets used in many prediction modeling competitions.

## Workflow 

```{r echo=FALSE, results='asis'}
pacman::p_load(glue)

workflow_list <- c("Data splitting", 
                   "Preprocessing",
                   "Model building",
                   "Model fitting",
                   "Model evaluation",
                   "Model tuning",
                   "Prediction")

glue("- {seq(workflow_list)}. {workflow_list}")
```
![Workflow. Based on RStudio.](https://rviews.rstudio.com/post/2019-06-14-a-gentle-intro-to-tidymodels_files/figure-html/tidymodels.png)

### Tidymodels 

- Like `tidyverse`, `tidymodels` is a collection of packages.

![Tidymodels. From RStudio.](https://rviews.rstudio.com/post/2019-06-14-a-gentle-intro-to-tidymodels_files/figure-html/ds.png)

> tidymodels are an **integrated, modular, extensible** set of packages that implement a framework that facilitates creating predicative stochastic models. - Joseph Rickert@RStudio

- Currently, 238 models are [available](https://topepo.github.io/caret/available-models.html) 

### Data split 

 - [`rsample`](https://rsample.tidymodels.org/): for data splitting 
 
#### Random sampling 

```{r eval = FALSE}
# data split 
set.seed(1234)

df_split  <- rsample::initial_split(df, prop = 0.7)
train <- rsample::training(df_split)
test <- rsample::testing(df_split)

```

#### Stratified random sampling 

#### Cross-validation 

### Pre-process 

- [`recipes`](https://recipes.tidymodels.org/index.html): for pre-processing

- [`textrecipes`](https://github.com/tidymodels/textrecipes) for text pre-processing

```{r eval = FALSE}

# preprocess 
df_recipe <- df %>% recipe(resonse ~.) %>%
  # Centering: x - mean(x)
  step_center(all_predictors(), -all_outcomes()) %>%
  # Scaling: x * k 
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep()

# preprocessed training and testsets 
processed_train <- df_recipe %>% bake(train)
processed_test <- df_recipe %>% bake(test)
  
```

### Model building 

- [`parsnip`](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/): for model building 

#### Choose model 

```{r eval = FALSE}

# Fit model 

  ## Choose model 
df_ranger <- rand_forest(trees = 1000, 
                         ## Declare mode 
                         mode = "classification") %>%
  ## Choose engine 
  set_engine("ranger") %>%
  ## Fit 
  fit(response ~ ., data = processed_train)

# Make predictions 

df_pred <- predict(df_ranger, processed_test)

```

#### Choose engine 

#### Declare mode  

### Model evaluation 

- [`yardstick`](https://github.com/tidymodels/yardstick): for model evaluations 
    
```{r eval = FALSE}

# validate 

df_pred %>%
  bind_cols(processed_test) %>%
  # You can also easily change metrics you want to use 
  metrics(truth = response, estimate = .pred_class)

## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   <chr>    <chr>          <dbl>
## 1 accuracy multiclass     0.9
## 2 kap      multiclass     0.8

```

### Tuning 

- [`tune`](https://github.com/tidymodels/tune): parameter tuning 

#### Grid search 

#### Iterative search

## Supervised learning

x -> f - > y (defined)

### Regularization

#### Regression (OLS)

```{r}
# Build a linear regression model 
out <- lm(mpg ~ cyl, data = mtcars)

# Predict the first five rows 
predict(out)[1:5]
```
![Based on [vas3k blog](https://vas3k.com/blog/machine_learning/) ](https://i.vas3k.ru/7qy.jpg)

#### Lasso, ridge, and elastic net 

- Tibshirani, Robert. ["The lasso method for variable selection in the Cox model."](https://onlinelibrary.wiley.com/doi/pdf/10.1002/(SICI)1097-0258(19970228)16:4%3C385::AID-SIM380%3E3.0.CO;2-3?casa_token=-JOiqeKwpSkAAAAA:NXo0URXwmVbZtK31uN990xESg-sPNDGs0SyMJN5FaiwZfbIStEgUHm2xHyLvdCC_EGlV9g07DZvp-g) *Statistics in medicine* 16, no. 4 (1997): 385-395.

- [R `glmnet` package](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf)

### Decision tree and ensemble models

#### Decision tree 

- Partitioning feature space sequentially. 
#### Random forest 

#### XGboost 

- Repeatedly using weak learners. 

#### SuperLearners

#### Neural networks / Deep learning 

#### Applications 

##### Bandit algorithm (optimizing an experiment)

##### Causal forest (estimating heterogeneous treatment effect)

## Unsupervised learning

x -> f - > y (not defined)

### Dimension reduction

![Projecting 2D-data to a line (PCA). From vas3k.com](https://i.stack.imgur.com/Q7HIP.gif)

### Clustering

![Agglomerative Hierarchical Clustering. From [George Seif's medium post](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68).](https://miro.medium.com/max/770/1*ET8kCcPpr893vNZFs8j4xg.gif)

### Applications 

#### Imputation 

#### Topic modeling 

## Bias and fairness in machine learning 

## Resources

### Books 

- *An Introduction to Statistical Learning - with Applications in R (2013)* by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Springer: New York. [Amazon](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370) or [free PDF](http://www-bcf.usc.edu/~gareth/ISL/). 

- *Hands-On Machine Learning with R (2020)* by Bradley Boehmke & Brandon Greenwell. [CRC Press](https://www.routledge.com/Hands-On-Machine-Learning-with-R/Boehmke-Greenwell/p/book/9781138495685) or [Amazon](https://www.amazon.com/gp/product/1138495689?pf_rd_p=ab873d20-a0ca-439b-ac45-cd78f07a84d8&pf_rd_r=JBRX0ZJ1WFSR9T3JPTQE)

- *Applied Predictive Modeling (2013)* by Max Kuhn and Kjell Johnson. Springer: New York. [Amazon](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485?SubscriptionId=0ENGV10E9K9QDNSJ5C82&tag=apm0a-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=1461468485) 

- *Feature Engineering and Selection: A Practical Approach for Predictive Models (2019)* by Kjell Johnson and Max Kuhn. Taylor & Francis. [Amazon](http://www.feat.engineering/) or [free HTML](http://www.feat.engineering/). 
- *[Tidy Modeling with R](https://www.tmwr.org/) (2020)* by Max Kuhn and Julia Silge (work-in-progress)

### Lecture slides 

- [An introduction to supervised and unsupervised learning (2015)](https://www.nber.org/econometrics_minicourse_2015/nber_slides11.pdf) by Susan Athey and Guido Imbens 
