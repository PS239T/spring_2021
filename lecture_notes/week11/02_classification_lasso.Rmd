---
title: "Text Classification - Lasso"
author: "Jae Yeon Kim"
institute: "UC Berkeley"
date: "`r Sys.Date()`"
---

# Load packages

```{r}
## CRAN packages 
pacman::p_load(here,
               tidyverse, 
               tidymodels, # tidymodels framework 
               janitor, # cleaning data 
               textrecipes, # text recipe 
               themis, # extra recipe 
               doParallel, # parallel processing 
               patchwork, # arranging ggplots
               glue, 
               scales)
```

# Load data

```{r}
load(here("lecture_notes", "week11", "stuff.Rdata"))
```

# Supervised learning

x -\> f - \> y (defined)

# Modeling building

-   Build models (`parsnip`)

1.  Specify a model
2.  Specify an engine
3.  Specify a mode

```{r}
# Lasso spec 
lasso_spec <- logistic_reg(penalty = tune(), # tuning hyperparameter 
                         mixture = 1) %>% # 1 = lasso, 0 = ridge 
  set_engine("glmnet") %>%
  set_mode("classification") 
```

# Model tuning

## Setup

-   Parallel processing

```{r}
all_cores <- parallel::detectCores(logical = FALSE)

cl <- makeCluster(all_cores[1] - 1)

registerDoParallel(cl)
```

**Hyper**parameters are parameters that control the learning process.

-   Search space for hyperparameters

1. Grid search: a grid of hyperparameters 

2. Random search: random sample points from a bounded domain

![](https://www.programmersought.com/images/523/7e44435f20fe514c11ca0d930af8547b.png)

```{r}
lambda_grid <- grid_regular(penalty(), levels = 50)
```

-   10 fold cross-validation

![Source: Kaggle](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4788946%2F82b5a41b6693a313b246f02d79e972d5%2FK%20FOLD.png?generation=1608195745131795&alt=media)

```{r}
set.seed(1234) # for reproducibility 

rec_folds <- vfold_cv(train_x_class %>% bind_cols(tibble(category = train_y_class)),
                      strata = category)
```

## Add setup to workflow

```{r}
# Lasso 
lasso_wf <- workflow() %>%
  add_model(lasso_spec) %>%
  add_formula(category ~ . )
```

## Tuning results

1.  Confusion matrix

A confusion matrix is often used to describe the performance of a classification model. The below example is based on a binary classification model.

|                 | Predicted: YES      | Predicted: NO       |
|-----------------|---------------------|---------------------|
| **Actual: YES** | True positive (TP)  | False negative (FN) |
| **Actual: NO**  | False positive (FP) | True negative (TN)  |

2.  Metrics

-   `accuracy`: The proportion of the data predicted correctly ($\frac{TP + TN}{total}$). 1 - accuracy = misclassification rate.

-   `precision`: Positive predictive value. *When the model predicts yes, how correct is it?* ($\frac{TP}{TP + FP}$)

-   `recall` (sensitivity): True positive rate (e.g., healthy people healthy). *When the actual value is yes, how often does the model predict yes?* ($\frac{TP}{TP + FN}$)

-   `F-score`: A weighted average between precision and recall. 

-   `ROC Curve` (receiver operating characteristic curve): a plot that shows the relationship between true and false positive rates at different classification thresholds. y-axis indicates the true positive rate and x-axis indicates the false positive rate. What matters is the AUC (Area under the ROC Curve), which is a cumulative probability function of ranking a random "positive" - "negative" pair (for the probability of AUC, see [this blog post](https://www.alexejgossmann.com/auc/)).

![Source: Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/images/ROCCurve.svg)

-   To learn more about other metrics, check out the yardstick package [references](https://yardstick.tidymodels.org/reference/index.html).

```{r eval = FALSE}
# Define performance metrics 
metrics <- yardstick::metric_set(accuracy, precision, recall, f_meas)

# Lasso
lasso_res <- lasso_wf %>%
  tune_grid(
    resamples = rec_folds, 
    grid = lambda_grid, 
    metrics = metrics 
  )

#save(lasso_res, file = here(here("lecture_notes", "week11", "res.Rdata")))
load(file = here(here("lecture_notes", "week11", "res.Rdata")))
```

## Visualize the results

```{r}
lasso_res %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, col = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.3
  ) +
  geom_line(size = 2) +
  scale_x_log10() +
  labs(x = "log(lambda)") +
  facet_wrap(~glue("{toupper(.metric)}"), 
             scales = "free",
             nrow = 2) +
  theme(legend.position = "none") 
```

## Selecting the best performing model

```{r}
best_lasso <- select_best(lasso_res, metric = "f_meas")
```

-   Finalize your workflow and visualize [variable importance](https://koalaverse.github.io/vip/articles/vip.html)

```{r}
finalize_lasso <- lasso_wf %>%
  finalize_workflow(best_lasso)

final_fit <- finalize_lasso %>%
  fit(train_x_class %>% bind_cols(tibble(category = train_y_class)))

final_fit %>%
  pull_workflow_fit() %>%
  vip::vip()
```

## Test fit

-   Apply the tuned model to the test dataset

```{r}
test_fit <- finalize_lasso %>% 
  fit(test_x_class %>% bind_cols(tibble(category = test_y_class)))

evaluate_class(test_fit)
```
