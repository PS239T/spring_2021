---
title: "Text Classification - Lasso"
author: "Jae Yeon Kim"
institute: "UC Berkeley"
date: "`r Sys.Date()`"
---

# Load packages 

```{r}

## CRAN packages 
pacman::p_load(here,
               tidyverse, 
               tidymodels, # tidymodels framework 
               janitor, # cleaning data 
               textrecipes, # text recipe 
               themis, # extra recipe 
               doParallel, # parallel processing 
               patchwork, # arranging ggplots
               glue)
```

# Load data

```{r}
load(here("lecture_notes", "week11", "stuff.Rdata"))
```

# Supervised learning

x -> f - > y (defined)

# Modeling building 

- Build models (`parsnip`)

1. Specify a model 
2. Specify an engine 
3. Specify a mode 

```{r}
# Lasso spec 
lasso_spec <- logistic_reg(penalty = tune(), # tuning hyperparameter 
                         mixture = 1) %>% # 1 = lasso, 0 = ridge 
  set_engine("glmnet") %>%
  set_mode("classification") 
```

# Model tuning 

## Setup 

- Parallel processing 

```{r}
all_cores <- parallel::detectCores(logical = FALSE)

cl <- makeCluster(all_cores[1] - 1)

registerDoParallel(cl)
```

- Search space 

```{r}
lambda_grid <- grid_regular(penalty(), levels = 50)
```

- 10 fold cross-validation 

```{r}
set.seed(1234) # for reproducibility 

rec_folds <- vfold_cv(train_x_class %>% bind_cols(tibble(category = train_y_class)),
                      strata = category)
```

## Add setup to workflow 

```{r}
# Lasso 
lasso_wf <- workflow() %>%
  add_model(lasso_spec) %>%
  add_formula(category ~ . )
```

## Tuning results 

- `accuracy`: The proportion of the data predicted correctly 

- `precision`: Positive predictive value

- `recall` (specificity): True positive rate (e.g., healthy people healthy)

![From wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png)

- To learn more about other metrics, check out the yardstick package [references](https://yardstick.tidymodels.org/reference/index.html).

```{r eval = FALSE}
# Define performance metrics 
metrics <- yardstick::metric_set(accuracy, precision, recall, f_meas)

# Lasso
lasso_res <- lasso_wf %>%
  tune_grid(
    resamples = rec_folds, 
    grid = lambda_grid, 
    metrics = metrics 
  )

save(lasso_res, file = here(here("lecture_notes", "week11", "res.Rdata")))
```

## Visualize the results 

```{r}
lasso_res %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, col = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.3
  ) +
  geom_line(size = 2) +
  scale_x_log10() +
  labs(x = "log(lambda)") +
  facet_wrap(~glue("{toupper(.metric)}"), 
             scales = "free",
             nrow = 2) +
  theme(legend.position = "none")
```

## Selecting the best performing model 

```{r}
best_lasso <- select_best(lasso_res, metric = "f_meas")
```

- Finalize your workflow and visualize [variable importance](https://koalaverse.github.io/vip/articles/vip.html)

```{r}
finalize_lasso <- lasso_wf %>%
  finalize_workflow(best_lasso)

final_fit <- finalize_lasso %>%
  fit(train_x_class %>% bind_cols(tibble(category = train_y_class)))

final_fit %>%
  pull_workflow_fit() %>%
  vip::vip()
```

## Test fit 

- Apply the tuned model to the test dataset 

```{r}
test_fit <- finalize_lasso %>% 
  fit(test_x_class %>% bind_cols(tibble(category = test_y_class)))

evaluate_class(test_fit)
```
