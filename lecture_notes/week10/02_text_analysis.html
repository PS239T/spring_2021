<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Automated Text Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jae Yeon Kim" />
    <meta name="date" content="2021-04-09" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"x00ef00f6d7043dc8c76a188a3bb613e","expires":1}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <script src="libs/js-cookie/js.cookie.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automated Text Analysis
### Jae Yeon Kim
### UC Berkeley
### 2021-04-09

---






# High-dimensional Data 

- The rise of high-dimensional data. The new data frontiers in social sciences---text ([Gentzkow et al. 2019](https://web.stanford.edu/~gentzkow/research/text-as-data.pdf); [Grimmer and Stewart 2013](https://www.jstor.org/stable/pdf/24572662.pdf?casa_token=SQdSI4R_VdwAAAAA:4QiVLhCXqr9f0qNMM9U75EL5JbDxxnXxUxyIfDf0U8ZzQx9szc0xVqaU6DXG4nHyZiNkvcwGlgD6H0Lxj3y0ULHwgkf1MZt8-9TPVtkEH9I4AHgbTg)) and and image ([Joo and Steinert-Threlkeld 2018](https://arxiv.org/pdf/1810.01544))---are all high-dimensional data. 

- The definition of high-dimensional data: n &lt; p

    - 1000 common English words for 30-word tweets: `\(1000^{30}\)` similar to N of atoms in the universe ([Gentzkow et al. 2019](https://web.stanford.edu/~gentzkow/research/text-as-data.pdf))
    
- Another way you can think about machine learning is it's all about [high-dimensional statistics](https://en.wikipedia.org/wiki/High-dimensional_statistics). We'll talk about this more next week. This week, we focus on how machine understands text as data (=automated text analysis). 

---

# Text as Data

- Computers count numbers, not reading texts. 

- The first step of automated text analysis is turning texts into a form of numerical representation (i.e., matrices).

- In this course, we first focused on dataframes (wrangling, modeling, and visualization), then lists (scraping), and are now moving onto matrices (processing). 

- In this session, we will learn how to count words using document-term matrices. 

- In the following two weeks, We will learn how to process text as data using machine learning techniques. 

---

# Tweets as example 


```r
pacman::p_load(rtweet, tidyverse)

rt &lt;- rtweet::search_tweets(q = "#vaccine", n = 50)

head(rt$text)
```

```
## [1] "@LeahCMa @ECHR_CEDH @ECHRPublication @COE_Execution @CoE_fr @coe @ECLJ_Official https://t.co/gyrmdhmCf7 #CEDH #vaccine #vaccini #Vaccins #COVID19"                                                                                                                            
## [2] "% of Publix COVID Vaccine appts. remaining in Central East Florida:\n\nSumter - 92%\nCitrus - 78%\nMarion - 77%\nHernando - 64%\nPasco - 57%\nPinellas - 55%\nPolk - 31%\nHillsborough - 20%\n\n#COVID19 #vaccine #Florida"                                                   
## [3] "% of Publix COVID Vaccine appts. remaining in Central West Florida:\n\nIndian River - 69%\nFlagler - 61%\nOkeechobee - 57%\nBrevard - 51%\nVolusia - 50%\nLake - 48%\nMartin - 41%\nSt. Lucie - 36%\nOsceola - 30%\nSeminole - 10%\nOrange - 4%\n\n#COVID19 #vaccine #Florida"
## [4] "% of Publix COVID Vaccine appts. remaining in South Florida:\n\nDesoto - 86%\nHighlands - 80%\nCharlotte - 68%\nSarasota - 65%\nManatee - 59%\nLee - 36%\nCollier - 14%\nPalm Beach - 1%\n\nBooked: Monroe, Miami-Dade, Broward\n#COVID19 #vaccine #Florida"                  
## [5] "% of Publix COVID Vaccine appts. remaining in North Florida:\n\nNassau - 90%\nColumbia - 90%\nSuwannee - 81%\nSt. Johns - 78%\nDuval - 78%\nAlachua - 75%\nClay - 74%\n\n#COVID19 #vaccine #Florida"                                                                          
## [6] "% of Publix COVID Vaccine appts. remaining in Panhandle Florida:\n\nWalton - 84%\nLeon - 78%\nSanta Rosa - 76%\nBay - 75%\nOkaloosa - 71%\n\n#COVID19 #vaccine #Florida"
```

---

# Preprocessing text 

What method is most appropriate for preprocessing text depends on applications.


```r
pacman::p_load(textclean)

rt$text &lt;- rt$text %&gt;%
  # Remove URL
  replace_url() %&gt;%
  # Remove emoji
  replace_emoji() %&gt;%
  # Remove HTML
  replace_html() %&gt;%
  # Remove extra whitespace
  replace_white() 
```

---

# Tidy approach to text analysis 

![](https://www.tidytextmining.com/images/tmwr_0101.png)

Automated text analysis using tidy data principles. Source: [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/tidytext.html) by Julia Silge and David Robinson 

---

# Document-term frequencies

* Units: documents 
* Observations: The frequencies of words appearing in these documents 


```r
pacman::p_load(tidytext)

wide_rt &lt;- rt %&gt;%
  unnest_tokens(word, text)

wide_rt$word[1:5]
```

```
## [1] "leahcma"         "echr_cedh"       "echrpublication" "coe_execution"  
## [5] "coe_fr"
```

---

# Counting words 


```r
wide_rt %&gt;%
  count(word) %&gt;%
  arrange(desc(n))
```

```
## # A tibble: 696 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 vaccine    64
##  2 the        48
##  3 of         28
##  4 covid19    23
##  5 to         22
##  6 in         21
##  7 a          19
##  8 and        18
##  9 covid      14
## 10 florida    12
## # … with 686 more rows
```

---

# Removing stopwords 

Here the unstated assumption is the length of each document would be equal. Otherwise, we cannot directly compare term frequencies across documents. This assumption is often violated in applications. 


```r
wide_rt %&gt;%
  count(word) %&gt;%
  # Removing stopwords 
  anti_join(stop_words) %&gt;%
  arrange(desc(n))
```

```
## Joining, by = "word"
```

```
## # A tibble: 532 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 vaccine      64
##  2 covid19      23
##  3 covid        14
##  4 florida      12
##  5 appts         8
##  6 people        6
##  7 publix        6
##  8 remaining     6
##  9 dose          5
## 10 syringe       5
## # … with 522 more rows
```

---

# Visualizing top 10 words 

Does it look like what kind of distribution? 


```r
wide_rt %&gt;%
  count(word) %&gt;%
  # Removing stopwords 
  anti_join(stop_words) %&gt;%
  slice_max(n, n = 10) %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n)) +
    geom_col() +
    coord_flip() +
    labs(x = "Words", 
         y = "Term frequency")
```

```
## Joining, by = "word"
```

&lt;img src="02_text_analysis_files/figure-html/unnamed-chunk-6-1.png" width="300px" /&gt;

---

# TF-IDF

TF-IDF helps to normalize the text length. 

* TF: Term frequency 

* IDF: Inverse document frequency

$$ 
\textrm{idf(term)} = ln(\frac{\textrm{n documents}}{\textrm{n documents containing term}})
$$
---

# Zipf's law: 

&gt; Zipf's law was originally formulated in terms of quantitative linguistics, stating that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.

This explains why text data are often very sparse matrices. 


```r
tfidf_table &lt;- wide_rt %&gt;%
  count(word) %&gt;%
  # Removing stopwords 
  anti_join(stop_words) %&gt;%
  arrange(desc(n)) %&gt;%
  mutate(rank = row_number(), 
         total = sum(n), 
         tf_idf = n/total)
```

```
## Joining, by = "word"
```

---


```r
tfidf_table %&gt;%
  ggplot(aes(x = rank, y = tf_idf)) +
  geom_line(size = 1, alpha = 0.8, 
            show.legend = FALSE) + 
  scale_y_log10() +
  coord_flip()
```

&lt;img src="02_text_analysis_files/figure-html/unnamed-chunk-8-1.png" width="400px" /&gt;

---

# Sentiment analysis 

**Sentiment dictionaries**

The following example draws on [the tidytext book.](https://www.tidytextmining.com/sentiment.html)

- `AFINN` from Finn [Årup Nielsen](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html)
- `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)
- `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)


```r
get_sentiments("bing") %&gt;% head()
```

```
## # A tibble: 6 x 2
##   word       sentiment
##   &lt;chr&gt;      &lt;chr&gt;    
## 1 2-faces    negative 
## 2 abnormal   negative 
## 3 abolish    negative 
## 4 abominable negative 
## 5 abominably negative 
## 6 abominate  negative
```

```r
rt_words &lt;- wide_rt %&gt;%
  count(word) %&gt;%
  mutate(rank = row_number()) %&gt;%
  select(word, rank, n)

rt_words %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  head()
```

```
## Joining, by = "word"
```

```
## # A tibble: 6 x 4
##   word       rank     n sentiment
##   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;    
## 1 available   100     2 positive 
## 2 bad         104     1 negative 
## 3 better      114     1 positive 
## 4 blame       120     1 negative 
## 5 bomb        123     1 negative 
## 6 cancer      140     1 negative
```

---


```r
rt_words %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment)
```

```
## Joining, by = "word"
```

```
## # A tibble: 2 x 2
##   sentiment     n
##   &lt;chr&gt;     &lt;int&gt;
## 1 negative     27
## 2 positive     27
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
