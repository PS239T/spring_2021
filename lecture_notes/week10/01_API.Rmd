---
title: "The Wonder and Pain of API"
subtitle: "Getting Started in Social Media Analysis"
author: "Jae Yeon Kim"
institute: "UC Berkeley"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r xaringan-editable, echo=FALSE}
xaringanExtra::use_editable(expires = 1)
```

# Review 

* Over the past two weeks, we've learned how to scrape websites and PDFs. 

* The key objective here is learning how to automatically turn semi-structured data (input) into structured data (output). 

* What is semi-structured data?

> Semi-structured data is a form of structured data that does not obey the *tabular* structure of data models associated with relational databases or other forms of data tables, but nonetheless contains **tags** or **other markers** to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. - [Wikipedia](https://en.wikipedia.org/wiki/Semi-structured_data#:~:text=Semi%2Dstructured%20data%20is%20a,and%20fields%20within%20the%20data.)

- Examples: HTML (e.g., websites), XML (e.g., government data), JSON (e.g., social media API)

---

# JSON (Tweet) example 

- A tree-like structure 

- Keys and values (key:value) 

```{json}
{
  "created_at": "Thu Apr 06 15:24:15 +0000 2017",
  "id_str": "850006245121695744",
  "text": "1\/ Today we\u2019re sharing our vision for the future of the Twitter API platform!\nhttps:\/\/t.co\/XweGngmxlP",
  "user": {
    "id": 2244994945,
    "name": "Twitter Dev",
    "screen_name": "TwitterDev",
    "location": "Internet",
    "url": "https:\/\/dev.twitter.com\/",
    "description": "Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\/\/twittercommunity.com\/ \u2328\ufe0f #TapIntoTwitter"
  }
}
```

---


# Learning objectives

- Learning the concept of Application Programming Interface (API)

- Learning the landscape of social media API

- Learning how to access social media API

- Learning how to parse social media data (JSON)

---

# What is an API?

- What is an API?: An interface (you can think of it as something akin to a restaurant menu. API parameters are APImenu items.)

  - [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) (Representational state transfer) API: static information (e.g., user profiles, list of followers and friends)

  - [Streaming](https://blog.axway.com/amplify/api-management/streaming-apis#:~:text=Streaming%20APIs%20are%20used%20to,a%20subset%20of%20Streaming%20APIS.) API: dynamic information (e.g, new tweets)

---

# Why should we care?

- API is the new data frontier. [ProgrammableWeb](https://www.programmableweb.com/apis/directory) shows that there are more than 24,046 APIs as of April 1, 2021.

  - Big and streaming (real-time) data 
  
  - High-dimensional data (e.g., text, image, video, etc.)
  
  - Lots of analytic opportunities (e.g., time-series, network, spatial analysis)
  
- Also, this type of data has many limitations (external validity, algorithmic bias, etc).

- Think about taking the API + approach (i.e., API not replacing but augmenting traditional data collection)
  
---

# How API works 

Request <-> Response 

![](https://mk0appinventiv4394ey.kinstacdn.com/wp-content/uploads/sites/1/2018/05/What-are-APIs-Learn-How-API-Works.jpg)

Source: https://appinventiv.com/blog/complete-guide-to-api-development/

---

# Twitter API

- [Twitter API](https://developer.twitter.com/en/docs) is widely accessible to researchers. 

  - In January 2021, Twitter introduced the [academic Twitter API](https://developer.twitter.com/en/solutions/academic-research) that allows generous access to Twitter's historical data for academic researchers 

    - Many R packages exist for the Twitter API: [rtweet](https://cran.r-project.org/web/packages/rtweet/rtweet.pdf) (REST + streaming), [tweetscores](https://github.com/pablobarbera/twitter_ideology/tree/master/pkg/tweetscores) (REST), [streamR](https://github.com/pablobarbera/streamR) (streaming)
    
    - Some notable limitations. If Twitter users don't share their tweets' locations (e.g., GPS), you can't collect them. 
    
> Twitter data is unique from data shared by most other social platforms because it reflects information that users *choose* to share publicly. Our API platform provides broad access to public Twitter data that users have chosen to share with the world. - Twitter Help Center

---

## Other social media API

* The following comments draw on Alexandra Siegel's talk on "Collecting and Analyzing Social Media Data" given at Montr√©al Methods Workshops. 

- [Facebook API](https://developers.facebook.com/) access has become constrained since the 2016 U.S. election.  

  - Exception: [Social Science One](https://socialscience.one/blog/unprecedented-facebook-urls-dataset-now-available-research-through-social-science-one).

  - Also, check out [Crowdtangle](https://www.crowdtangle.com/) for collecting publig FB page data 
  
  - Using FB ads is still a popular method, especially among scholars studying developing countries. 

- [YouTube API](https://developers.google.com/youtube/v3): generous access + (computer-generated) transcript in many languages 

  - Documentation on [captions](https://developers.google.com/youtube/v3/docs/captions) from YouTube
  
- [Instragram API](https://www.instagram.com/developer/): Data from public accounts are available. 

- [Reddit API](https://www.reddit.com/dev/api/): Well-annotated text data suitable for machine learning 

---

## Pros and Cons of using API

.pull-left[
**Upside**

-   Legal and well-documented.

-   Access to rich and big data (e.g., text, image, and video). 

-   Well-organized, managed and curated data. 
]

.pull-right[
**Downside**

1.  Rate-limited.

2.  If you want to access more and various data than those available, you need to pay for premium access.
]

---

# Two approaches 

1. Getting API data using 
plug in play packages 

  a) Using RStudio (`rtweet`)

  b) Using the terminal (`twarc` and `tidytweetjson`)

2. Getting API data from scratch (`httr`, `jsonlite`)

---

# rtweet package: setup 

The rtweet examples draw from [Chris Bail's tutorial](https://cbail.github.io/SICSS_APIs_markdown.html). 

The first thing you need to do is setup.

Assuming that you already signed up for a Twitter developer account 

```{r eval = FALSE}
app_name <- "YOUR APP NAME"
consumer_key <- "YOUR CONSUMER KEY"
consumer_secret <- "YOUR CONSUMER SECRET"

rtweet::create_token(app = app_name, 
                     consumer_key = consumer_key, 
                     consumer_secret = consumer_secret)
```
---

# rtweet pacakge: search API

Using **search API**; This API returns a collection of Tweets mentioning a particular query.

```{r cache=TRUE}
# Install and load rtweet 
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load(rtweet)

# The past 6-9 days 
rt <- search_tweets("#stopasianhate", n = 1000, include_rts = FALSE)

# The longer term 
# search_fullarchive() premium service

# Do you remember my comment on geocode?

head(rt$text)
```

---

# rtweet package 

Can you guess what would be the class type of rt?

```{r eval = FALSE}
class(rt)
```

What would be the number of rows?

```{r eval = FALSE}
nrow(rt)
```

---

# rtweet package: time series analysis 

```{r}
pacman::p_load(ggplot2, ggthemes, rtweet)

ts_plot(rt, "3 hours") +
  ggthemes::theme_fivethirtyeight() +
  labs(title = "Frequency of Tweets about StopAsianHate from the Past Day",
       subtitle = "Tweet counts aggregated using three-hour intervals",
       source = "Twitter's Search API via rtweet")
```

---

# rtweet package: geographical analysis

```{r}
pacman::p_load(maps)

geocoded <- lat_lng(rt)

maps::map("state", lwd = .25) # lwd = line type 
with(geocoded, points(lng, lat))
```

---

# Hydrating: An Alternative Way to Collect Historical Twitter Data 

You can collect Twitter data using Twitter's API, or you can hydrate Tweet IDs collected by other researchers. This is an excellent resource to collect historical Twitter data.

- [Covid-19 Twitter chatter dataset for scientic use](http://www.panacealab.org/covid19/) by Panacealab

- [Women's March Dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5ZVMOR) by Littman and Park

- Harvard Dataverse has many dehydrated Tweet IDs that could be of interest to social scientists.

---

# Twarc

- Twarc: A command-line tool and Python library that works for almost every Twitter API-related problem.

- I assume that you have already installed [Python 3](https://www.python.org/download/releases/3.0/).

```{bash eval = FALSE}
pip3 install twarc
```

- Configuration 

```{bash eval = FALSE}
twarc configure
```

---

# Twarc: Hydrating 

```{bash eval=FALSE}
twarc hydrate tweet_ids.txt > tweets.jsonl 
```

You can then use [tidytweetjson](https://jaeyk.github.io/tidytweetjson/) I developed to turn the Tweet JSON file into a dataframe. 

```{r eval = FALSE}
## Install the current development version from GitHub

devtools::install_github("jaeyk/tidytweetjson",
                         dependencies = TRUE)
```
---

# rtweet and twarc 

- The main difference is using RStudio vs. the terminal. 

- The difference matters when your data size is large. Suppose the size of the Twitter data you downloaded is 10 GB. R/RStudio might have a hard time dealing with this size of data. Then, how can you wrangle this size of data in a complex way using R?

---

# Dealing with big data in R

- Social media data are usually big.

- Working with big data in R ([blog post](https://www.r-bloggers.com/2019/07/three-strategies-for-working-with-big-data-in-r/#:~:text=Strategy%202%3A%20Chunk%20and%20Pull,similar%20to%20the%20MapReduce%20algorithm.))
  
  **1. Sample and model**
  
    - Pros: Easy to work with 
    
    - Cons: Sampling 

  **2. Chunk and pull** 
  
    - Pros: Full data + complex operations 
    
    - Cons: Need to split data and pulling process is intensive. 

  **3. Push compute to data** 
    
    - Pros: Using database (little constrained by size) and data summary is quick 
    
    - Cons: What you can do depend on database operations and speed 

---

# twarc + tidytweetjson

- twarc + tidytweetjson allows using the chunk and pull strategy. 

![Chunk and Pull. From Studio.](https://rviews.rstudio.com/post/2019-07-01-3-big-data-paradigms-for-r_files/chunk_pull.png)

---

# Getting API data from scratch

Load packages. For the connection interface, don't use `RCurl`, but strongly recommend using `httr`.

```{r}
pacman::p_load(httr, jsonlite, purrr, glue)
```

Form REQUEST 

```{r eval = FALSE}
baseurl <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"

term <- "muslim+muslims"

begin_date <- "19960911"

end_date <- "20060911"

# URL request 

r <- GET(baseurl, 
         query = list('q' = term, 
                      'begin_date' = begin_date, 
                      'end_date' = end_date,
                      'api-key' = key))
```

---

# Extract data 

```{r eval = FALSE}
extract_nyt_data <- function(i){
  
  # JSON object 
  out <- fromJSON(httr::content(r, "text"),                   
                  simplifyDataFrame = TRUE, 
                  flatten = TRUE) 
    
  # Page numbering 
  out$page <- i
  
  message(glue("Scraping {i} page"))
  
  return(out)
  
}
```

---

# Automating iterations

```{r eval = FALSE}
# 6 seconds sleep between calls, max 4000 requests per day 
rate <- rate_delay(pause = 6, max_times = 4000)

slowly_extract <- slowly(extract_nyt_data, rate = rate)
  
extract_all <- function(page_list) {

  df <- map_dfr(page_list, slowly_extract) 
  
  return(df)
  
}

max_pages <- round((fromJSON(httr::content(r, "text", 
                                           encoding = "UTF-8"), simplifyDataFrame = TRUE, flatten = TRUE)$response$meta$hits[1] / 10) - 1)

max_pages
```
