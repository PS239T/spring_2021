---
title: "Unsupervised ML"
author: "Jae Yeon Kim"
institute: "UC Berkeley"
date: "`r Sys.Date()`"
---

# Load packages 

```{r}
## CRAN packages 
pacman::p_load(here,
               tidyverse, 
               tidymodels, # tidymodels framework 
               janitor, # cleaning data 
               textrecipes, # text recipe 
               themis, # extra recipe 
               doParallel, # parallel processing 
               patchwork) # arranging ggplots
```

# Unsupervised learning

x -\> f - \> y (not defined)

## Dimension reduction

![Projecting 2D-data to a line (PCA). From vas3k.com](https://i.stack.imgur.com/Q7HIP.gif)

# Dataset 

[Heart disease data from UCI](https://archive.ics.uci.edu/ml/datasets/heart+Disease)

One of the popular datasets used in machine learning competitions

```{r message = FALSE}

df <- read_csv(here("lecture_notes", "week12", "heart.csv"))

```

# Correlation analysis

This dataset is a good problem for PCA as some features are highly correlated. 

The following data dictionary comes from [this site](http://rstudio-pubs-static.s3.amazonaws.com/24341_184a58191486470cab97acdbbfe78ed5.html).

* age - age in years
* sex - sex (1 = male; 0 = female)
* cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)
* trestbps - resting blood pressure (in mm Hg on admission to the hospital)
* chol - serum cholestoral in mg/dl
* fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
* restecg - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy)
* thalach - maximum heart rate achieved
* exang - exercise induced angina (1 = yes; 0 = no)
* oldpeak - ST depression induced by exercise relative to rest
slope - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)
* ca - number of major vessels (0-3) colored by flourosopy
* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect
* num - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < 50% diameter narrowing; Value 1 = > 50% diameter narrowing)

```{r}
df %>%
  select(-target) %>%
  corrr::correlate()
```


# Descriptive statistics 

Notice the scaling issues? PCA is not scale-invariant. So, we need to fix this problem.

```{r}
min_max <- list(
  min = ~min(.x, na.rm = TRUE), 
  max = ~max(.x, na.rm = TRUE)
)

df %>%
  select(-target) %>%
  summarise(across(where(is.numeric), min_max))
```

#### Preprocessing

`recipe` is essential for preprocessing multiple features at once.

```{r}

pca_recipe <- recipe(~., data = df) %>%
  # Normalize some numeric variables 
  step_normalize(c("age", "trestbps", "chol", "thalach", "oldpeak")) 

```

#### PCA analysis

```{r}

pca_res <- pca_recipe %>% 
  step_pca(all_predictors(), 
           id = "pca") %>% # id argument identifies each PCA step 
  prep()

pca_res %>%
  tidy(id = "pca") 

```

##### Screeplot

```{r}
pca_recipe %>%
  step_pca(all_predictors(), 
           id = "pca") %>% # id argument identifies each PCA step 
  prep() %>%
  tidy(id = "pca", type = "variance") %>%
  filter(terms == "percent variance") %>% 
  ggplot(aes(x = component, y = value)) +
    geom_col() +
    labs(x = "PCAs of heart disease",
         y = "% of variance",
         title = "Scree plot")
```

##### View factor loadings

Loadings are the covariances between the features and the principal components (=eigenvectors).

```{r}
pca_recipe %>%
  step_pca(all_predictors(), 
           id = "pca") %>% # id argument identifies each PCA step 
  prep() %>%
  tidy(id = "pca") %>%
  filter(component %in% c("PC1", "PC2")) %>%
  ggplot(aes(x = fct_reorder(terms, value), y = value)) +
    geom_col(position = "dodge") +
    coord_flip() +
    labs(x = "Terms",
         y = "Contribtutions",
         fill = "PCAs") 
```

**The key lesson**

You can use these low-dimensional data to solve the curse of dimensionality problem. Compressing feature space via dimension reduction techniques is called feature extraction. PCA is one way of doing this.

# Topic modeling

# Setup

```{r}
pacman::p_load(tidytext, # tidy text analysis
               glue, # paste string and objects                
               stm, # structural topic modeling
               gutenbergr) # toy datasets 
```

# Dataset

The data munging process draws on [Julia Silge's blog post](https://juliasilge.com/blog/sherlock-holmes-stm/).

```{r}

sherlock_raw <- gutenberg_download(1661)

glimpse(sherlock_raw)

sherlock <- sherlock_raw %>%
  # Mutate story using a conditional statement 
  mutate(story = ifelse(str_starts(text, "ADVENTURE"), 
                                   text, NA)) %>%
  # Fill in missing values with next value  
  tidyr::fill(story, .direction = "down") %>%
  # Filter 
  filter(story != "THE ADVENTURES OF SHERLOCK HOLMES") %>%
  # Factor 
  mutate(story = factor(story, levels = unique(story)))

sherlock <- sherlock[,2:3]

```

# Key ideas behind Latent Dirichlet Allocation (LDA)

![Source: paperswithcode.com](https://paperswithcode.com/media/thumbnails/task/task-0000000179-fd3a1d11_fGQkZCJ.jpg)

* Main papers: See [Latent Dirichlet Allocation](https://proceedings.neurips.cc/paper/2001/file/296472c9542ad4d4788d543508116cbc-Paper.pdf) by David M. Blei, Andrew Y. Ng and Michael I. Jordan (then all Berkeley) and this [follow-up paper](http://www.cse.cuhk.edu.hk/irwin.king/_media/presentations/latent_dirichlet_allocation.pdf) with the same title.

-   Topics as **distributions** of words ($\beta$ distribution)

-   Documents as **distributions** of topics ($\alpha$ distribution)

-   What distributions?

    -   Probability

    -   Multinominal 

-   Words lie on a lower-dimensional space (dimension reduction akin to PCA)

-   Co-occurrence of words (clustering)

-   Bag of words (feature engineering)

    -   Upside: easy and fast (also working quite well)
    -   Downside: ignored grammatical structures and rich interactions among words (Alternative: word embeddings. Please check out [text2vec](http://text2vec.org/))

-   Documents are exchangeable (sequencing won't matter).

-   Topics are independent (uncorrelated). If you don't think this assumption holds, use Correlated Topics Models by [Blei and Lafferty (2007)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.958.2484&rep=rep1&type=pdf).

# Exploratory data analysis

```{r}

sherlock_n <- sherlock %>%
  unnest_tokens(output = word,
                input = text) %>%
  count(story, word, sort = TRUE)

sherlock_total_n <- sherlock_n %>%
  group_by(story) %>%
  summarise(total = sum(n))

sherlock_words <- sherlock_n %>% left_join(sherlock_total_n)

sherlock_words %>%
  mutate(freq = n/total) %>%
  group_by(story) %>%
  top_n(10) %>%
  ggplot(aes(x = fct_reorder(word, freq), 
             y = freq, 
             fill = story)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~story, 
             ncol = 2, 
             scales = "free_y") +
  scale_fill_viridis_d() +
  labs(x = "")

```

# STM

[Structural Topic Modeling](https://www.structuraltopicmodel.com/) by Roberts, Stewart, and Tingley helps estimating how the proportions of topics vary by covariates. If you don't use covaraites, this approach is close to CTM. The other useful (and very recent) topic modeling package is Keyword Assited Topic Models ([keyATM](https://keyatm.github.io/keyATM/)) by Shusei, Imai, and Sasaki. Also, note that we didn't cover other important techniques in topic modeling such as dynamic and hierarchical topic modeling.

![](https://warin.ca/shiny/stm/images/fig02.png)

## Turn text into document-term matrix

`stm` package has its preprocessing function.

```{r}

dtm <- textProcessor(documents = sherlock$text,
                     metadata = sherlock, 
                     removestopwords = TRUE,
                     verbose = FALSE)

```

## Tuning K

-   K is the number of topics.
-   Let's try K = 5, 10, 15.

```{r}

test_res <- searchK(dtm$documents, dtm$vocab, 
                   K = c(5, 10, 15),
                   prevalence = ~ story, 
                   data = dtm$meta)

# save(test_res, file = here("lecture_notes", "week11", "test_res.Rdata"))

load(here("lecture_notes", "week11", "test_res.Rdata"))
```

## Evaludating models

Several metrics assess topic models' performance: the held-out likelihood, residuals, semantic coherence, and exclusivity. Here we examine the relationship between semantic coherence and exclusivity to understand the trade-off involved in selecting K.

* Semantic coherence: high probability words for a topic co-occur in documents 

* Exclusivity: key words of one topic are not likely to appear as key words in other topics.

> In Roberts et al 2014 we proposed using the Mimno et al 2011 semantic coherence metric for helping with topic model selection. We found that semantic coherence alone is relatively easy to achieve by having only a couple of topics which all are dominated by the most common words. Thus we also proposed an exclusivity measure.

> Our exclusivity measure includes some information on word frequency as well. It is based on the FREX labeling metric (calcfrex) with the weight set to .7 in favor of exclusivity by default.

```{r}

test_res$results %>%
  unnest(c(K, exclus, semcoh)) %>%
  select(K, exclus, semcoh) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(x = exclus, y = semcoh)) +
    geom_point() +
    geom_text(label = glue("K = {test_res$results$K}"),
              size = 5,
              color = "red",
              position = position_jitter(width = 0.05,
                                         height = 0.05)) +
    labs(x = "Exclusivity",
         y = "Semantic coherence", 
         title = "Exclusivity and semantic coherence")
```

## Finalize

```{r}
final_stm <- stm(dtm$documents, 
                 dtm$vocab, 
                 K = 10, prevalence = ~ story,
                 max.em.its = 75, 
                 data = dtm$meta, 
                 init.type = "Spectral",
                 seed = 1234567,
                 verbose = FALSE)
```

## Explore the results

-   Using the `stm` package.

```{r}

# plot
plot(final_stm)

```

-   Using ggplot2

In LDA distribution, $\alpha$ represents document-topic density and $\beta$ represents topic-word density. 

```{r}
# tidy  
tidy_stm <- tidy(final_stm)

# top terms
tidy_stm %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    ggplot(aes(fct_reorder(term, beta), beta, 
               fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_viridis_d()
```
