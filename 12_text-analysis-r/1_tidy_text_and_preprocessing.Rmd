---
title: "Tidy text and preprocessing in R"
author: "Jae Yeon Kim"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

The technical part of the material is adapted from [Text Mining with R](https://proquest.safaribooksonline.com/9781491981641) (2017) by Julia Silge and David Robinson and the theoretical part from Foundations of Statistical Natural Language Processing (1999) by Christopher D. Manning and Heinrich Schütze and lecture notes by David Bamman[http://people.ischool.berkeley.edu/~dbamman/]. Silge and Robinson's book is an accessible introduction to text analysis in R. I like the book a lot since it uses tidyverse framework so you can apply the skills you developed earlier in this course to text analysis. Manning and Schütze's book is still the best theoretical introduction to natural language processing. 

**Learning objectives**

    1. Understanding basic theoretical ideas of natural language processing (NLP)
    2. Understanding the steps students need to take to conduct a text analysis 

This is part of the analysis part of the course. We start from text analysis fundamentals then move to dictionary-based methods (non-model approach), to topic modeling (unsupervised learning), and to text classification (supervised learning).  

Before digging into technical details, let's take a step back and think about this question for a movement: "What Is a Language?"

- **Rationalist approach**: Chomsky's "the poverty of the stimulus" argument -> language acquisition device and universal grammar (language is something hardwired in our brain and has a very specific design.)

- **Empiricist approach**: They also agree that language is part of our brain. But they believe that our brain doesn't have a very specific design, but has general operational rules like association, pattern recognition, and generalization. It is an old idea that is now re-surging with the new computational methods. 
    
- **Statistical NLP** is a child of the empiricist approach. The belief is we can understand the structure of language by specifying an appropriate general language model, and then inducing the values of parameters by applying statistical, pattern recognition, and machine learning methods to a large amount of language use (mostly big texts called a *corpus* -- Latin word for "body"). 

**Language processing** is getting the meaning out of the structure of language (e.g., words, parts of speech, syntax). Statistical NLP does that by treating language and cognition as **probabilistic** phenomenon. Cognition has some kind of quantitative framework that can handle uncertainty and incomplete information that surround humans in the world. If cognition is probabilistic, then its outcome (language) is also probabilistic. (For this reason, the deeper theoretical understanding of NLP requires some knowledge of probability and information theory.) This idea is the foundation for everything we do in computational text analysis. In **natural language processing**, we process language with computers. 

Processing natural language is hard since language is so creative (and thus complex and ambiguous). To tackle this challenge collectively, the field has been evolved as an interdisciplinary field.

- Linguistics (representation of language) -- structure of language 

- Computer scientists (AI and ML) -- dealing with high-dimensional data (n of n samples < p of p features = curse of the curse of dimensionality)
> "As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially." - Charles Isbell (Georgia Tech)

- Social sciences/humanities (computational social science, digital humanities, and digital journalism) -- providing domain knowledge

# Setup
```{r}

rm(list=ls())

require(tm) # for text mining
library(tidyverse) # for tidyverse
library(tidytext) # for tidy text analysis
library(quanteda) # for text analysis and examples
```

# Corpus

A corpus is a collection of texts (raw strings, often called "documents") annotated with meta data (e.g., author, date, source)

For this exercise, we play with Martin Luther King Jr's "I have a dream" speech.

# Tidytext 

## Create a corpus 

```{r}
# load MLK speech

MLK <- readLines("http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt")

MLK[1:10]

```

## Tidy text

- tibble is a class of data frame in the dplyr and tibble packages.
- It's better than built-in data frame since it does not convert strings to factors.

```{r}

# tidy text
MLK_df <- data_frame(line = 1:length(MLK), text = MLK) 

# see data
MLK_df

# see class
class(MLK_df) 

```

# Preprocessing 

- In the end, you need to figure out how to represent text data in a numerical way.
- And how to reduce dimensions of such text data by 
    - 1) tokenizing and using n-grams, 
    - 2) removing noise (e.g., html tags), and 
    - 3) [normalizing](https://en.wikipedia.org/wiki/Text_normalization) (e.g., stop words, [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)). 
- In this unit, we focus on 1) and 3). For noise removing, you might need to learn how to do string manipulations by using [regular expressions](https://www.regular-expressions.info/) in R.

## Tokenizing

[Tokenizing](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) splits longer strings into smaller pieces (called 'tokens'). 

From now on, we use the US presidential inaugural address texts from quanteda package to have experience with complicated data.

- For unnest_tokens, word is a token output and text is a text input.
- The function also strips punctuation, and converts the tokens to lower case. (Check to_lower argument.). This is nice because we want words not uninteresting characters like punctuation.
- Also, note that tokenizing is language specific. We only deal with English text data here.

```{r}
# load data 
inaug <- tidy(data_corpus_inaugural)

inaug %>%
  group_by(President) %>%
  unnest_tokens(word, text)
```

## Using N-grmas
N-gram is a consecutive sequence of words.

```{r}
# no filtered
inaug %>%
  filter(Year > 1945) %>%
  group_by(President) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)

# filtered
inaug_filtered <- inaug %>%
  filter(Year > 1945) %>%
  group_by(President) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

## count
inaug_filtered %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

## count with filter
inaug_filtered %>%
  filter(word2 == "government") %>%
  count(President, word2, sort = TRUE)

```

## Normalizing
### Stop words 

- Remove common words (something like "the") not useful for the analysis
- These words are technically called [stop words](https://en.wikipedia.org/wiki/Stop_words) in natural language processing.
- Also, remember that removing stop words is not always a good idea. Think about "to be" or "not to be". If you delete "not", there's no difference between the two words. 
- If you want to cut out (assumed unnecessary) parts of the text, you can try either [stemming](https://en.wikipedia.org/wiki/Stemming) (no "ed", "ing", or "ly" in the end) or [lemminization](https://en.wikipedia.org/wiki/Lemmatisation)  (better -> goo).

```{r}
MLK_df %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)
```

```{r}
# load stopwords dataset
data(stopwords) 

# apply stopwords
MLK_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%  
  count(word, sort = TRUE)
```

- Now, visualize the results.

```{r}
MLK_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%  
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word,  y = n)) +
  geom_col() +
  coord_flip()
```

### tf-idf

- TF indicates a term frequency.
- IDF indicates a term's inverse document frequency.
- tf-idf multiples the term frequency and the inverse document frequency.
- IDF decreases the weight for common words and increases the weight for less common words. 

We are going to see how IDF works with an actual example.

```{r}
# words 
inaug_words <- inaug %>%
  filter(Year > 1945) %>%
  unnest_tokens(word, text) %>%
  count(President, word, sort = TRUE) %>%
  ungroup() # to return to nongrouped data

# total words
inaug_total <- inaug_words %>%
  group_by(President) %>%
  summarize(total = sum(n))

# merge
inaug_words <- left_join(inaug_words, inaug_total)

inaug_words
```

Note that the plot below proves Zipf's law: the frequency that a word appears is inversely proportional to its rank. As we will learn later, this insight is key to the understanding of how topic modeling works. In most cases, we need only a few number of topics because a few words will have lots of examples.


```{r}

# plot the distribution of n/total
ggplot(data = inaug_words, aes(n/total, fill = President)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~President, ncol = 5, scales = "free_y")

```

```{r}
inaug_words %>% 
  group_by(President) %>%
  mutate(rank = row_number(), 
         term_frequency = n/total) %>%
  ggplot(aes(x = rank, y = term_frequency, color = President)) +
    geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
    scale_x_log10() +
    scale_y_log10() +
    labs(x = "Rank",
         y = "Term Frequency")
```

```{r}
# tf_idf
inaug_words <- inaug_words %>%
  bind_tf_idf(word, President, n)

# visualize 
inaug_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(President) %>%
  top_n(10) %>%
  ungroup %>% 
  ggplot(aes(reorder(word, tf_idf), tf_idf, fill = President)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~President, ncol = 3, scales = "free") +
    coord_flip()

```