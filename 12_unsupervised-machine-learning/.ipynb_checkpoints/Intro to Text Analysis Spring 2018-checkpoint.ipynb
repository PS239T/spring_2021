{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a string is a sequence of characters\n",
      "['these', 'strings', 'are', 'stored', 'as', 'a', 'list', '!']\n",
      "['a', 'string', 'is', 'a', 'sequence', 'of', 'characters']\n"
     ]
    }
   ],
   "source": [
    "# Text as data: what are strings?\n",
    "\n",
    "sentence = 'a string is a sequence of characters'\n",
    "\n",
    "words = ['these', 'strings', 'are', 'stored', 'as', 'a', 'list']\n",
    "words.append('!')\n",
    "\n",
    "print(sentence) #print is a function, hence parentheses ()\n",
    "print(words)\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words[0]) # first item in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence)) # length of characters\n",
    "print(len(sentence.split())) # length of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does the Mueller team have 13 hardened Democrats, \n",
      " some big Crooked Hillary supporters, and Zero Republicans? \n",
      " Another Dem recently added. does anyone think this is fair? \n",
      " And yet, there is NO COLLUSION!\n"
     ]
    }
   ],
   "source": [
    "# let's use a random tweet from no one in particular\n",
    "\n",
    "tweet = \"\"\"Why does the Mueller team have 13 hardened Democrats, \n",
    " some big Crooked Hillary supporters, and Zero Republicans? \n",
    " Another Dem recently added. does anyone think this is fair? \n",
    " And yet, there is NO COLLUSION!\"\"\"\n",
    "\n",
    "# Triple quotes are used for strings that break across multiple lines\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string functions\n",
    "\n",
    "tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string functions\n",
    "\n",
    "tweet.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string functions\n",
    "\n",
    "tweet.replace('Hillary','Clinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can combine functions\n",
    "\n",
    "tweet.replace('Hillary','Clinton').lower().split() # note the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet.lower().replace('Hillary','Clinton').split() # hillary doesn't get replaced in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lists of filter words\n",
    "\n",
    "stop_words = ['the', 'it', 'is', 'a', 'was', 'and', \n",
    "             'why', 'what', 'how', 'has', 'have', 'this', 'that']\n",
    "\n",
    "punctuation = [\".\", \",\" , \"?\", \"!\", \"#\", \"$\", '\\n']\n",
    "\n",
    "positive_words = ['fair','good', 'nice', 'super', 'fun', 'delightful', 'supporters']\n",
    "negative_words = ['no', 'crooked', 'collusion', 'bad', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's do some simple pre-processing of our tweet\n",
    "\n",
    "tweet_processed=tweet.lower() #convert to lowercase\n",
    "\n",
    "for p in punctuation:\n",
    "    tweet_processed=tweet_processed.replace(p,'') # erase punctuation\n",
    "\n",
    "words = tweet_processed.split() # store the processed tweet in an object called 'word'\n",
    "\n",
    "# remove stopwords and store remaining words in a new list\n",
    "results = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        results.append(word)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list comprehension achieves same result as above in a single line\n",
    "\n",
    "list_comp = [word for word in words if word not in punctuation and word not in stop_words]\n",
    "\n",
    "list_comp == results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#which words are positive or negative?\n",
    "\n",
    "for word in words:\n",
    "    if word in positive_words:\n",
    "        print(word + ' is a positive word')\n",
    "        \n",
    "for word in words:\n",
    "    if word in negative_words:\n",
    "        print(word + ' is a negative word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK: The Natural Language Tool Kit\n",
    "\n",
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids() # these are the book corpora included in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamlet_words = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt') #list of words\n",
    "hamlet_sents = nltk.corpus.gutenberg.sents('shakespeare-hamlet.txt') # list of sentences, each sentence is list\n",
    "hamlet_paras = nltk.corpus.gutenberg.paras('shakespeare-hamlet.txt') # list of pararaphs, each paragraph is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(hamlet_words[0:5])\n",
    "print(hamlet_sents[0:5])\n",
    "print(hamlet_paras[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:20] #the brackets are to index the first 20 items in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[word for word in hamlet_words if word.lower() not in stopwords.words('english')][:10] # first 10 items in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[word.lower() for word in hamlet_words if word.isalpha()][:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "print([word for word in hamlet_words[80:110]])\n",
    "print([porter.stem(word) for word in hamlet_words[80:110]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"I'm gonna tokenize this sentence into a list of words\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CORPUS_PATH = \"/Users/Fiona_Shen_Bayh/nltk_data/corpora/state_union/\"\n",
    "\n",
    "filenames = sorted([os.path.join(CORPUS_PATH, fn) for fn in os.listdir(CORPUS_PATH)])\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "\"\"\" CountVectorizer class gathers word frequencies (or term frequencies) associated with texts into a document-term matrix.\n",
    "    min_df: discard words appearing in less than n documents, in large corpus this may be set to 15 or higher to eliminate very rare words\n",
    "    max_df: discard words appearing in more than n documents\n",
    "    decode_error: if a byte sequence contains characters that aren't part of the given encoding, a UnicodeDecodeError will be raised. To prevent this, specify ‘ignore’ or ‘replace’.\n",
    "\"\"\"\n",
    "\n",
    "vectorizer = text.CountVectorizer(input='filename', min_df=10, max_df=.95, decode_error='replace', stop_words='english')\n",
    "\n",
    "#Document term matrix    \n",
    "\"\"\" all_the_docs is a dataframe, but vectorizer needs string as input\n",
    "    so we convert the dataframe to a unicode string object\"\"\"\n",
    "\n",
    "dtm = vectorizer.fit_transform(filenames).toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "\"\"\" Now we have a document-term matrix (dtm) and a vocabulary list (vocab).\"\"\"\n",
    "\n",
    "\"\"\" Convert vocab, a list storing the vocabulary words, into a NumPy array, because an array supports a greater variety of operations than a list.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# TOPIC MODELING #\n",
    "##################\n",
    "\n",
    "\"\"\" A “non-negative matrix” is a matrix containing non-negative values (i.e. zero or positive word frequencies).\n",
    "    Non-negative matrix factorization (NMF) is often characterized as a machine learning algorithm\n",
    "    It strongly resembles Latent Dirichlet Allocation (LDA), which is a probabilistic model of the corpus\n",
    "    Whereas LDA expresses uncertainty about placement of topics across texts and assignment of words to topics,\n",
    "        NMF is a deterministic algorithm that arrives at a single representation of the corpus\n",
    "        Both NMF and LDA take a corpus and uncover “latent topics” \n",
    "        \n",
    "        In what follows, let's start with an NMF topic model\"\"\"\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "\"\"\" Here we will use NMF to get a document-topic matrix (topics here will also be referred to as “components”) \n",
    "    and a list of top words for each topic.\"\"\"\n",
    "\n",
    "num_topics = 10 #number of latent topics\n",
    "\n",
    "num_top_words = 20 #number of words per topic\n",
    "\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1) #plug these components into the algorithm\n",
    "\n",
    "doctopic = clf.fit_transform(dtm) #plug the dtm into the algorithm\n",
    "\n",
    "topic_words = [] #create an empty list for our topic words\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])\n",
    "    \n",
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True) # scale the document-component matrix such that the component values associated with each document sum to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "presidents = []\n",
    "\n",
    "for fn in filenames:\n",
    "    basename = os.path.basename(fn)\n",
    "    name, ext = os.path.splitext(basename)\n",
    "    name = name.lstrip('0123456789-')\n",
    "    name = name.rstrip('-123')\n",
    "    presidents.append(name) \n",
    "\n",
    "# turn this into an array so we can use NumPy functions\n",
    "presidents = np.asarray(presidents)\n",
    "\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# preprocess\n",
    "num_groups = len(set(presidents))\n",
    "\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "for i, name in enumerate(sorted(set(presidents))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[presidents == name, :], axis=0)\n",
    "\n",
    "doctopic = doctopic_grouped\n",
    "\n",
    "group = sorted(set(presidents)) \n",
    "\n",
    "print(\"Top NMF topics in...\")\n",
    "\n",
    "\"\"\" Topic shares associated with a set of documents can be interpreted in terms of word frequencies, \n",
    "    i.e. how many times a given word appears in a given topic\n",
    "    Python uses 0-based indexing, so the first topic is topic 0.\"\"\"\n",
    "\n",
    "for i in range(len(doctopic)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(group[i], top_topics_str)) # the numbers represent the top 3 topics for each search document\n",
    "    \n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:20]))) #prints top 15 words associated with each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, K = doctopic.shape  # N documents, K topics\n",
    "\n",
    "ind = np.arange(N)  # the x-axis locations for the courts\n",
    "\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "plots = []\n",
    "\n",
    "height_cumulative = np.zeros(N)\n",
    "\n",
    "for k in range(K):\n",
    "    color = plt.cm.coolwarm(k/K, 1) #colormap\n",
    "    if k == 0:\n",
    "        p = plt.bar(ind, doctopic[:, k], width, color=color)\n",
    "    else:\n",
    "        p = plt.bar(ind, doctopic[:, k], width, bottom=height_cumulative, color=color)\n",
    "    height_cumulative += doctopic[:, k]\n",
    "    plots.append(p)\n",
    "\n",
    "plt.ylim((0, 1))  # proportions sum to 1, so the height of the stacked bars is 1\n",
    "\n",
    "plt.ylabel('') #y-axis label\n",
    "\n",
    "plt.title('Topics') #plot title\n",
    "\n",
    "plt.xticks(ind+width/2, group) #placement and naming of x-axis ticks\n",
    "\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(K)] # {} will become topic 1:K\n",
    "\n",
    "\n",
    "# see http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend for more details on making a legend in matplotlib\n",
    "\n",
    "plt.legend([p[0] for p in plots], topic_labels, bbox_to_anchor=(1.4,0.8), title=\"Topics\")\n",
    "\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, K = doctopic.shape # N documents, K topics\n",
    "\n",
    "ind = np.arange(N)  # points on the x-axis\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "plt.bar(ind, doctopic[:,2], width=width)\n",
    "\n",
    "xlabels = set(presidents)\n",
    "\n",
    "plt.xticks(ind + width/2, group, rotation=45)  # put labels in the center\n",
    "\n",
    "plt.title('Share of Topic #2')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
